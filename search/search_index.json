{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"","title":"Home"},{"location":"community/","text":"Work with us \u00b6 The OSSVerse community is the driving force behind our mission to create a trustworthy and secure open-source ecosystem. Join us today and be part of the movement that is shaping the future of open-source software. Whether you're an OSS producer, consumer, OASP, or simply passionate about open-source security and quality, your contributions are invaluable. Together, we can build a world where open-source software thrives, empowering innovation while upholding the highest standards of security, compliance, and responsible practices.","title":"Overview"},{"location":"community/#work-with-us","text":"The OSSVerse community is the driving force behind our mission to create a trustworthy and secure open-source ecosystem. Join us today and be part of the movement that is shaping the future of open-source software. Whether you're an OSS producer, consumer, OASP, or simply passionate about open-source security and quality, your contributions are invaluable. Together, we can build a world where open-source software thrives, empowering innovation while upholding the highest standards of security, compliance, and responsible practices.","title":"Work with us"},{"location":"community/contact/","text":"Contact Us \u00b6 Contact points \ud83d\udce7 Email | info@binbash.co \ud83c\udf0e Web Site | https://www.binbash.co \ud83c\udfe2 LinkedIn | https://www.linkedin.com/company/binbash \ud83d\udcde Phone | +1 786 2244551 \ud83d\udcf1 ** WhatsApp / Telegram |** +54 9351 5510132 || +54 93543 516289 \ud83d\udcac ** Slack |** Join Leverage channel Contact Us","title":"Contact Us"},{"location":"community/contact/#contact-us","text":"Contact points \ud83d\udce7 Email | info@binbash.co \ud83c\udf0e Web Site | https://www.binbash.co \ud83c\udfe2 LinkedIn | https://www.linkedin.com/company/binbash \ud83d\udcde Phone | +1 786 2244551 \ud83d\udcf1 ** WhatsApp / Telegram |** +54 9351 5510132 || +54 93543 516289 \ud83d\udcac ** Slack |** Join Leverage channel Contact Us","title":"Contact Us"},{"location":"community/contribute/","text":"Contribute and Developing binbash Leverage \u00b6 This document explains how to get started with developing for Leverage Reference Architecture . It includes how to build, test, and release new versions. Quick Start \u00b6 Getting the code \u00b6 The code must be checked out from this same github.com repo inside the binbash Leverage Github Organization . git clone git@github.com:binbashar/le-tf-infra-aws.git cd le-tf-infra-aws cd .. git clone git@github.com:binbashar/le-ansible-infra.git cd le-ansible-infra cd .. Initial developer environment build \u00b6 TODO Dependencies \u00b6 This guide requires you to install X v0.1 or newer. Deploying \u00b6 To deploy the Leverage Reference Architecture onto AWS. Please check the deployment guide Testing \u00b6 To run tests, just run... Releasing \u00b6 CircleCi PR auto-release job \u00b6 https://circleci.com/gh/binbashar/bb-devops-tf-infra-aws NOTE: Will only run after merged PR.","title":"Contribute and Developing binbash Leverage"},{"location":"community/contribute/#contribute-and-developing-binbash-leverage","text":"This document explains how to get started with developing for Leverage Reference Architecture . It includes how to build, test, and release new versions.","title":"Contribute and Developing binbash Leverage"},{"location":"community/contribute/#quick-start","text":"","title":"Quick Start"},{"location":"community/contribute/#getting-the-code","text":"The code must be checked out from this same github.com repo inside the binbash Leverage Github Organization . git clone git@github.com:binbashar/le-tf-infra-aws.git cd le-tf-infra-aws cd .. git clone git@github.com:binbashar/le-ansible-infra.git cd le-ansible-infra cd ..","title":"Getting the code"},{"location":"community/contribute/#initial-developer-environment-build","text":"TODO","title":"Initial developer environment build"},{"location":"community/contribute/#dependencies","text":"This guide requires you to install X v0.1 or newer.","title":"Dependencies"},{"location":"community/contribute/#deploying","text":"To deploy the Leverage Reference Architecture onto AWS. Please check the deployment guide","title":"Deploying"},{"location":"community/contribute/#testing","text":"To run tests, just run...","title":"Testing"},{"location":"community/contribute/#releasing","text":"","title":"Releasing"},{"location":"community/contribute/#circleci-pr-auto-release-job","text":"https://circleci.com/gh/binbashar/bb-devops-tf-infra-aws NOTE: Will only run after merged PR.","title":"CircleCi PR auto-release job"},{"location":"community/faqs/","text":"Frequently Asked Questions (FAQs) \u00b6 Target audience \u00b6 Who is Leverage's target audience? Leverage is mainly oriented to Latam, North America and European startup's CTOs, VPEs, Engineering Managers and/or team leads (Software Architects / DevOps Engineers / Cloud Solutions Architects) looking to rapidly set and host their modern web and mobile applications and systems in Amazon Web Services (\u2705 typically in just a few weeks!). Oriented to Development leads or teams looking to solve their current AWS infrastructure and software delivery business needs in a securely and reliably manner, under the most modern best practices. Your Entire AWS Cloud solutions based on DevOps practices will be achieved: Containerization Infrastructure as Code Container Orchestration (K8s) & Application Services CI / CD Security, Compliance & Reliability Cost Optimization & Performance Efficiency Observability & Monitoring Moreover, if you are looking to have the complete control of the source code, and of course be able to run it without us, such as building new Development environments and supporting your Production Cloud environments, you're a great fit for the Leverage AWS Cloud Solutions Reference Architecture model. And remember you could implement yourself or we could implement it for you! \ud83d\udcaa Agreement and statement of work \u00b6 Project Kick-Off \u00b6 Project Kick-Off Once the agreement contract and NDA are signed we estimate 15 days to have the team ready to start the project following the proposed Roadmap (\u201cStatement of work\u201d) that describes at length exactly what you'll receive. Assignments and Delivery \u00b6 Assignments and Delivery After gathering all the customer project requirements and specifications we'll adjust the Reference Architecture based on your needs. As a result we'll develop and present the Leverage Reference Architecture for AWS implementation Roadmap. A typical Roadmap (\u201cStatement of Work\u201d) includes a set number of Iterations (sprints). We try to keep a narrow scope for each Iteration so that we can tightly control how hours get spent to avoid overruns. We typically avoid adding tasks to a running Iteration so that the scope does not grow. That's also why we have an allocation for to specific long lived tasks: General-Task-1: DevOps and Solutions Architecture challenge, definitions, tasks (PM), reviews, issues and audit. General-Task-2: WEEKLY FOLLOW-UP Meeting, Which is work that falls outside of the current Iteration specific tasks. This is for special requests, meetings, pair programming sessions, extra documentation, etc. binbash will participate and review the planned tasks along the customer: planned roadmap features bug fixes Implementation support Using the relevant ticketing system ( Jira ) to prioritize and plan the corresponding work plan. Reports and Invoicing \u00b6 Reports and Invoicing Weekly task reports and tasks management agile metrics. We use Toggl to track all our time by client, project, sprint, and developer. We then import these hours into Quickbooks for invoicing. Rates and Billing \u00b6 Rates and pricing plans Pre-paid package subscriptions: A number of prepaid hours is agreed according to the needs of the project. It could be a \"Basic Plan\" of 40 hours per month. Or a \"Premium Plan\" of 80 hours per month (if more hours are needed it could be reviewed). When buying in bulk there is a discount on the value of the hour. When you pay for the package you start discounting the hours from the total as they are used, and if there are unused hours left, consider that maximum 20% could be transferred for the next month. On-demand Business Subscription : There are a certain number of hours tracked each month, as planned tasks are demanded. The total spent hours will be reported each month. There is a monthly minimum of 40 hours per month. Support tasks maximum estimated effort should be between 80 and 120 hs / month. Billing The Customer will be billed every month. Invoices are due within 15 days of issue. We accept payments via US Bank ACH, Bill.com , and Payoneer . Rates include all applicable taxes and duties as required by law.","title":"Frequently Asked Questions (FAQs)"},{"location":"community/faqs/#frequently-asked-questions-faqs","text":"","title":"Frequently Asked Questions (FAQs)"},{"location":"community/faqs/#target-audience","text":"Who is Leverage's target audience? Leverage is mainly oriented to Latam, North America and European startup's CTOs, VPEs, Engineering Managers and/or team leads (Software Architects / DevOps Engineers / Cloud Solutions Architects) looking to rapidly set and host their modern web and mobile applications and systems in Amazon Web Services (\u2705 typically in just a few weeks!). Oriented to Development leads or teams looking to solve their current AWS infrastructure and software delivery business needs in a securely and reliably manner, under the most modern best practices. Your Entire AWS Cloud solutions based on DevOps practices will be achieved: Containerization Infrastructure as Code Container Orchestration (K8s) & Application Services CI / CD Security, Compliance & Reliability Cost Optimization & Performance Efficiency Observability & Monitoring Moreover, if you are looking to have the complete control of the source code, and of course be able to run it without us, such as building new Development environments and supporting your Production Cloud environments, you're a great fit for the Leverage AWS Cloud Solutions Reference Architecture model. And remember you could implement yourself or we could implement it for you! \ud83d\udcaa","title":"Target audience"},{"location":"community/faqs/#agreement-and-statement-of-work","text":"","title":"Agreement and statement of work"},{"location":"community/faqs/#project-kick-off","text":"Project Kick-Off Once the agreement contract and NDA are signed we estimate 15 days to have the team ready to start the project following the proposed Roadmap (\u201cStatement of work\u201d) that describes at length exactly what you'll receive.","title":"Project Kick-Off"},{"location":"community/faqs/#assignments-and-delivery","text":"Assignments and Delivery After gathering all the customer project requirements and specifications we'll adjust the Reference Architecture based on your needs. As a result we'll develop and present the Leverage Reference Architecture for AWS implementation Roadmap. A typical Roadmap (\u201cStatement of Work\u201d) includes a set number of Iterations (sprints). We try to keep a narrow scope for each Iteration so that we can tightly control how hours get spent to avoid overruns. We typically avoid adding tasks to a running Iteration so that the scope does not grow. That's also why we have an allocation for to specific long lived tasks: General-Task-1: DevOps and Solutions Architecture challenge, definitions, tasks (PM), reviews, issues and audit. General-Task-2: WEEKLY FOLLOW-UP Meeting, Which is work that falls outside of the current Iteration specific tasks. This is for special requests, meetings, pair programming sessions, extra documentation, etc. binbash will participate and review the planned tasks along the customer: planned roadmap features bug fixes Implementation support Using the relevant ticketing system ( Jira ) to prioritize and plan the corresponding work plan.","title":"Assignments and Delivery"},{"location":"community/faqs/#reports-and-invoicing","text":"Reports and Invoicing Weekly task reports and tasks management agile metrics. We use Toggl to track all our time by client, project, sprint, and developer. We then import these hours into Quickbooks for invoicing.","title":"Reports and Invoicing"},{"location":"community/faqs/#rates-and-billing","text":"Rates and pricing plans Pre-paid package subscriptions: A number of prepaid hours is agreed according to the needs of the project. It could be a \"Basic Plan\" of 40 hours per month. Or a \"Premium Plan\" of 80 hours per month (if more hours are needed it could be reviewed). When buying in bulk there is a discount on the value of the hour. When you pay for the package you start discounting the hours from the total as they are used, and if there are unused hours left, consider that maximum 20% could be transferred for the next month. On-demand Business Subscription : There are a certain number of hours tracked each month, as planned tasks are demanded. The total spent hours will be reported each month. There is a monthly minimum of 40 hours per month. Support tasks maximum estimated effort should be between 80 and 120 hs / month. Billing The Customer will be billed every month. Invoices are due within 15 days of issue. We accept payments via US Bank ACH, Bill.com , and Payoneer . Rates include all applicable taxes and duties as required by law.","title":"Rates and Billing"},{"location":"community/support/","text":"Support \u00b6 Leverage Reference Architecture \u00b6 Please create a Github Issue to get immediate support from the binbash Leverage Team Our Engineering & Support Team \u00b6 AWS Well Architected Review \u00b6 Feel free to contact us for an AWS Well Architected Framework Review Well Architected Framework Review Reference Study Case Operational Excellence Security Cost Optimization Reliability Performance Efficiency WAF Exta Material DevSecOps Security Audit - v0.1 WAF Cost Optimization Checklist - v0.1 Read More \u00b6 How AWS Well-Architected Reviews Can Drive a Customer-First Culture","title":"Support"},{"location":"community/support/#support","text":"","title":"Support"},{"location":"community/support/#leverage-reference-architecture","text":"Please create a Github Issue to get immediate support from the binbash Leverage Team","title":"Leverage Reference Architecture"},{"location":"community/support/#our-engineering-support-team","text":"","title":"Our Engineering &amp; Support Team"},{"location":"community/support/#aws-well-architected-review","text":"Feel free to contact us for an AWS Well Architected Framework Review Well Architected Framework Review Reference Study Case Operational Excellence Security Cost Optimization Reliability Performance Efficiency WAF Exta Material DevSecOps Security Audit - v0.1 WAF Cost Optimization Checklist - v0.1","title":"AWS Well Architected Review"},{"location":"community/support/#read-more","text":"How AWS Well-Architected Reviews Can Drive a Customer-First Culture","title":"Read More"},{"location":"community/testimonials/","text":"Testimonials (by industry) \u00b6 Helthcare (Clinics) & Real State & Travel \u00b6 Yury Yakubchyk | Founder, Investor, Board Member & Advisor @ Multiple US Industries \ud83c\udf0e Company website: lifehousehotels.com \ud83c\udf0e Company website: joinsprouttherapy.com \"We utilized BinBash to help get our DevOps infrastructure launched and manage all aspects of our AWS-based setup. Their security and compliance focus turned out to be a great fit for our HIPAA regulation needs. We\u2019ve been very pleased with their responsiveness and thoroughness with regards to our technology needs and would be happy to work with them again in the future.\" EdTech (Education) \u00b6 Alejandro Parise | Founder & CEO @ Latam & North America EdTech Industry \ud83c\udf0e Company website: e-valuados.com \"binbash provided us with cloud architecture consulting at a critical time for our company. Prior to our production go-live they reviewed in-depth our application architecture. Resulting in several optimization points, with focus on response time, security, costs, monitoring, DB data sets, backups & restore. They truly exceeded our expectations and provided support when e-valuados greatly needed it.\" Banking, Fintech and Insurtech \u00b6 Martin Vago | IT & CloudOps Manager @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\" Felipe Lerena | Software Architect & Dev Lead @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"binbash helped us build a resilient, future-proof infrastructure in record time. They will not only do things for you, they will transfer all the knowledge and make you part of the decision. They are always thinking about re-usability, security and scale. The binbash team is not only technically proficient but they are really nice human beings.\" Juan Manuel Rodrigo | CTO @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"We found the right AWS and DevOps technology partner at binbash, who quickly interpreted our core business, and while working together we identified quick gains that helped us to significantly differentiate our cloud native solution. Moreover, their product, Leverage resulted in an AWS automation framework that accelerated our business roadmap, highlighting the technical talent of the binbash team for its rapid and solid implementation.\" Alejandro Creta | Infrastructure Architecture Lead @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"Our experience with binbash Leverage has been overwhelmingly positive. It allowed us to adopt AWS Well-Architected Framework and Infrastructure as Code in an accelerated and efficient way without losing compatibility with the tool's stardards we're using such as Terraform, Ansible and Helm. The project planning and the work methodology helped us to strengthen our DevOps culture in the areas involved such as loosely couple cloud architecture, development & CI/CD, monitoring, shift left on security and audit, adding important cultural values such as collaboration, continuous improvement and knowledge transfer.\" Media & Entertainment (Streaming) \u00b6 Max Ivanov | Software Architect @ US Media Entertainment Industry \"binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\" Cyber Security & Risk Management \u00b6 Franco Gauchat | DevSecOps Engineer @ Cyber Security Industry \ud83c\udf0e Company website: btrconsulting.com \"binbash team is highly experienced and skilled in DevOps practices and AWS cloud solutions. I have worked very closely with them on a large scale Fintech project and have demonstrated a deep understanding of our deployed AWS infrastructure. They collaborate with us to design and implement the best possible course of action for each of our applications based on short and long term business and security goals. Including cloud architecture, security, audit, monitoring, centralized logs, deployments, infra as code, scalability and performance. Hope our paths cross again very soon.\" Horacio G. de Oro | Head of DevOps @ Risk Management US Industry \ud83c\udf0e Company website: thirdpartytrust.com \"binbash redesigned and helped us to develop our entire AWS Cloud Solutions Architecture under a IaC (InfraAsCode) best practice approach, and always taking care of the environment security. Including our new AWS Organizations Multi-Account and Kubernetes deployment infrastructure. The binbash team managed the entire engagement effectively and within budget. The team members are knowledgeable, proactive, work collaboratively, offer solid and creative solutions, communicate well, and deliver timely and high quality work products, such as Leverage. We highly recommend binbash for your AWS cloud infrastructure implementation.\" Sports and Events \u00b6 Leandro Basso | Co-Founder & BizDev Manager @ Latam Sports and Events Industry \ud83c\udf0e Company website: hayturno.com \"When our company The Ideas Factory was in search of a new DevOps team, we looked for a partnership that would not only help direct us into the modern world, but also one that understood our business. binbash DevOps Cloud Services performed over and above our expectations, and the initial response from visitors of our remodeled WebApp has been overwhelmingly positive.\" Digital Advertising / Marketing \u00b6 Alina Fermo | Project Manager @ US Digital Marketing Industry \ud83c\udf0e Company website: grey.com \"I\u00b4ve got the pleasure to work with binbash leaders in the recent past. I was managing client side DevOps project for a US Digital Marketing customer and I have only good things to say about them. Good professionals and also kind persons. They are those people you hope to have in all your projects. They are always there to give an efective response to the client, to suggest good practices, improvements with excellent communication skills. When I needed them, they were always there to solve our issues and attend our requests. Moreover, one of the binbash Tech architected several of our project applications and also work along with their team in achieving continuous integration implementation and kick-off.Their sense of urgency and his ability to discover potential risks are great, as well as finding the best way to solve problems when facing them. There is nothing bad I can say about these guys. I only wish someday to have the pleasure to work with them again.\"","title":"Testimonials (by industry)"},{"location":"community/testimonials/#testimonials-by-industry","text":"","title":"Testimonials (by industry)"},{"location":"community/testimonials/#helthcare-clinics-real-state-travel","text":"Yury Yakubchyk | Founder, Investor, Board Member & Advisor @ Multiple US Industries \ud83c\udf0e Company website: lifehousehotels.com \ud83c\udf0e Company website: joinsprouttherapy.com \"We utilized BinBash to help get our DevOps infrastructure launched and manage all aspects of our AWS-based setup. Their security and compliance focus turned out to be a great fit for our HIPAA regulation needs. We\u2019ve been very pleased with their responsiveness and thoroughness with regards to our technology needs and would be happy to work with them again in the future.\"","title":"Helthcare (Clinics) &amp; Real State &amp; Travel"},{"location":"community/testimonials/#edtech-education","text":"Alejandro Parise | Founder & CEO @ Latam & North America EdTech Industry \ud83c\udf0e Company website: e-valuados.com \"binbash provided us with cloud architecture consulting at a critical time for our company. Prior to our production go-live they reviewed in-depth our application architecture. Resulting in several optimization points, with focus on response time, security, costs, monitoring, DB data sets, backups & restore. They truly exceeded our expectations and provided support when e-valuados greatly needed it.\"","title":"EdTech (Education)"},{"location":"community/testimonials/#banking-fintech-and-insurtech","text":"Martin Vago | IT & CloudOps Manager @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\" Felipe Lerena | Software Architect & Dev Lead @ Latam Fintech Industry \ud83c\udf0e Company website: tunubi.com \"binbash helped us build a resilient, future-proof infrastructure in record time. They will not only do things for you, they will transfer all the knowledge and make you part of the decision. They are always thinking about re-usability, security and scale. The binbash team is not only technically proficient but they are really nice human beings.\" Juan Manuel Rodrigo | CTO @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"We found the right AWS and DevOps technology partner at binbash, who quickly interpreted our core business, and while working together we identified quick gains that helped us to significantly differentiate our cloud native solution. Moreover, their product, Leverage resulted in an AWS automation framework that accelerated our business roadmap, highlighting the technical talent of the binbash team for its rapid and solid implementation.\" Alejandro Creta | Infrastructure Architecture Lead @ Latam Fintech / Banking / Insurtech Industries \ud83c\udf0e Company website: flexibility.com.ar \"Our experience with binbash Leverage has been overwhelmingly positive. It allowed us to adopt AWS Well-Architected Framework and Infrastructure as Code in an accelerated and efficient way without losing compatibility with the tool's stardards we're using such as Terraform, Ansible and Helm. The project planning and the work methodology helped us to strengthen our DevOps culture in the areas involved such as loosely couple cloud architecture, development & CI/CD, monitoring, shift left on security and audit, adding important cultural values such as collaboration, continuous improvement and knowledge transfer.\"","title":"Banking, Fintech and Insurtech"},{"location":"community/testimonials/#media-entertainment-streaming","text":"Max Ivanov | Software Architect @ US Media Entertainment Industry \"binbash has a focused and highly productive professional team. They have exceptional tech skills and effectively transmit and implement their solutions, such as Leverage. Thanks to their collaboration we managed to have a superlative product hosted in AWS. It is undoubtedly a company with which I would like to work in any challenge that lies ahead\"","title":"Media &amp; Entertainment (Streaming)"},{"location":"community/testimonials/#cyber-security-risk-management","text":"Franco Gauchat | DevSecOps Engineer @ Cyber Security Industry \ud83c\udf0e Company website: btrconsulting.com \"binbash team is highly experienced and skilled in DevOps practices and AWS cloud solutions. I have worked very closely with them on a large scale Fintech project and have demonstrated a deep understanding of our deployed AWS infrastructure. They collaborate with us to design and implement the best possible course of action for each of our applications based on short and long term business and security goals. Including cloud architecture, security, audit, monitoring, centralized logs, deployments, infra as code, scalability and performance. Hope our paths cross again very soon.\" Horacio G. de Oro | Head of DevOps @ Risk Management US Industry \ud83c\udf0e Company website: thirdpartytrust.com \"binbash redesigned and helped us to develop our entire AWS Cloud Solutions Architecture under a IaC (InfraAsCode) best practice approach, and always taking care of the environment security. Including our new AWS Organizations Multi-Account and Kubernetes deployment infrastructure. The binbash team managed the entire engagement effectively and within budget. The team members are knowledgeable, proactive, work collaboratively, offer solid and creative solutions, communicate well, and deliver timely and high quality work products, such as Leverage. We highly recommend binbash for your AWS cloud infrastructure implementation.\"","title":"Cyber Security &amp; Risk Management"},{"location":"community/testimonials/#sports-and-events","text":"Leandro Basso | Co-Founder & BizDev Manager @ Latam Sports and Events Industry \ud83c\udf0e Company website: hayturno.com \"When our company The Ideas Factory was in search of a new DevOps team, we looked for a partnership that would not only help direct us into the modern world, but also one that understood our business. binbash DevOps Cloud Services performed over and above our expectations, and the initial response from visitors of our remodeled WebApp has been overwhelmingly positive.\"","title":"Sports and Events"},{"location":"community/testimonials/#digital-advertising-marketing","text":"Alina Fermo | Project Manager @ US Digital Marketing Industry \ud83c\udf0e Company website: grey.com \"I\u00b4ve got the pleasure to work with binbash leaders in the recent past. I was managing client side DevOps project for a US Digital Marketing customer and I have only good things to say about them. Good professionals and also kind persons. They are those people you hope to have in all your projects. They are always there to give an efective response to the client, to suggest good practices, improvements with excellent communication skills. When I needed them, they were always there to solve our issues and attend our requests. Moreover, one of the binbash Tech architected several of our project applications and also work along with their team in achieving continuous integration implementation and kick-off.Their sense of urgency and his ability to discover potential risks are great, as well as finding the best way to solve problems when facing them. There is nothing bad I can say about these guys. I only wish someday to have the pleasure to work with them again.\"","title":"Digital Advertising / Marketing"},{"location":"concepts/next-steps/","text":"Next Steps \u00b6 Now that you know the basic concepts about Leverage feel free to give it a try or check out the User Guide section to go deeper into the implementation details. Links down below: Learn More \u00b6 See Try Leverage to take the tutorial that will help you deploy a basic AWS Landing Zone via Leverage. See User Guide to take the comprehensive route to learn more about Leverage. See Work with us if you want to join us or know more about the team behind Leverage.","title":"Next Steps"},{"location":"concepts/next-steps/#next-steps","text":"Now that you know the basic concepts about Leverage feel free to give it a try or check out the User Guide section to go deeper into the implementation details. Links down below:","title":"Next Steps"},{"location":"concepts/next-steps/#learn-more","text":"See Try Leverage to take the tutorial that will help you deploy a basic AWS Landing Zone via Leverage. See User Guide to take the comprehensive route to learn more about Leverage. See Work with us if you want to join us or know more about the team behind Leverage.","title":"Learn More"},{"location":"concepts/oss-assurance-provider/","text":"Open Source Assurance Service Providers \u00b6 Open Source Software Assurance Service Providers (OASPs) are entities or individuals that offer a range of services aimed at ensuring the security, compliance, and quality of open source software (OSS) for consumers. These services are critical in identifying vulnerabilities, ensuring legal compliance, and maintaining the overall integrity and reliability of open source software. OASPs play a vital role in the open source ecosystem by providing expertise and services that help in mitigating the risks associated with using open source components in both commercial and non-commercial software projects. Role and Services Offered: Vulnerability Assessment and Management : OASPs conduct thorough assessments of open source software to identify security vulnerabilities. They provide detailed reports on identified vulnerabilities and offer recommendations or patches to mitigate these risks. Compliance and Licensing : They ensure that the use of open source software complies with various open source licenses. This includes identifying licensing obligations, preventing license conflicts, and ensuring that software distributions comply with the terms of open source licenses. Quality Assurance : Beyond security and compliance, OASPs also evaluate the quality of open source software. This can include code reviews, performance testing, and other quality assurance processes to ensure that the software meets certain standards before it is deployed. Security Auditing and Certification: Some OASPs offer formal security auditing services, where they scrutinize the open source software for security flaws and certify its security posture. This certification can be crucial for organizations that require a certain level of assurance before integrating open source components into their projects. Patch Management and Security Updates : They provide ongoing services to manage patches and security updates for open source software, ensuring that the software remains secure against newly discovered vulnerabilities over time. Community Engagement and Contribution : Many OASPs actively engage with and contribute to the open source community. This can include reporting vulnerabilities, contributing patches, and participating in open source projects to improve their security. Importance in the Ecosystem \u00b6 The role of OASPs is crucial in the open source ecosystem. They bridge the gap between the open nature of open source software and the security and compliance requirements of its users, especially in enterprise environments. By providing these specialized services, OASPs enable organizations to leverage the benefits of open source software while managing the associated risks effectively. This not only supports the sustainability and growth of open source projects but also ensures that organizations can maintain high standards of security and compliance in their software development practices.","title":"Assurance Provider"},{"location":"concepts/oss-assurance-provider/#open-source-assurance-service-providers","text":"Open Source Software Assurance Service Providers (OASPs) are entities or individuals that offer a range of services aimed at ensuring the security, compliance, and quality of open source software (OSS) for consumers. These services are critical in identifying vulnerabilities, ensuring legal compliance, and maintaining the overall integrity and reliability of open source software. OASPs play a vital role in the open source ecosystem by providing expertise and services that help in mitigating the risks associated with using open source components in both commercial and non-commercial software projects. Role and Services Offered: Vulnerability Assessment and Management : OASPs conduct thorough assessments of open source software to identify security vulnerabilities. They provide detailed reports on identified vulnerabilities and offer recommendations or patches to mitigate these risks. Compliance and Licensing : They ensure that the use of open source software complies with various open source licenses. This includes identifying licensing obligations, preventing license conflicts, and ensuring that software distributions comply with the terms of open source licenses. Quality Assurance : Beyond security and compliance, OASPs also evaluate the quality of open source software. This can include code reviews, performance testing, and other quality assurance processes to ensure that the software meets certain standards before it is deployed. Security Auditing and Certification: Some OASPs offer formal security auditing services, where they scrutinize the open source software for security flaws and certify its security posture. This certification can be crucial for organizations that require a certain level of assurance before integrating open source components into their projects. Patch Management and Security Updates : They provide ongoing services to manage patches and security updates for open source software, ensuring that the software remains secure against newly discovered vulnerabilities over time. Community Engagement and Contribution : Many OASPs actively engage with and contribute to the open source community. This can include reporting vulnerabilities, contributing patches, and participating in open source projects to improve their security.","title":"Open Source Assurance Service Providers"},{"location":"concepts/oss-assurance-provider/#importance-in-the-ecosystem","text":"The role of OASPs is crucial in the open source ecosystem. They bridge the gap between the open nature of open source software and the security and compliance requirements of its users, especially in enterprise environments. By providing these specialized services, OASPs enable organizations to leverage the benefits of open source software while managing the associated risks effectively. This not only supports the sustainability and growth of open source projects but also ensures that organizations can maintain high standards of security and compliance in their software development practices.","title":"Importance in the Ecosystem"},{"location":"concepts/oss-consumer/","text":"Open Source Software Consumers \u00b6 Open source software consumers encompass a broad and diverse group that ranges from individual hobbyists to large multinational corporations. The nature of open source software, being freely available for use, modification, and distribution, makes it appealing to a wide array of users for various purposes. Here are some of the main categories of open source software consumers: Individual Users : Many individuals use open source software for personal computing needs, such as operating systems (e.g., Linux distributions), office suites (e.g., LibreOffice), web browsers (e.g., Firefox), and multimedia editing (e.g., GIMP for image editing, Audacity for audio editing). The appeal often lies in the lack of licensing fees, the ability to customize the software, and privacy considerations. Developers and Programmers : Open source software is incredibly popular among developers for a multitude of reasons, including the availability of source code, the ability to modify and improve the software, and the vast community support. Developers use open source tools, libraries, frameworks, and development environments for creating a wide range of applications. Educational Institutions : Schools, colleges, and universities often utilize open source software for educational purposes, including teaching programming and computer science, running servers and computer labs, and conducting research. Open source software provides an affordable solution for educational institutions, allowing students to learn with real-world tools that they can also use outside of the institution without licensing restrictions. Government Agencies : Governments around the world use open source software for various applications, from running websites and managing data to more specialized uses like geographic information systems (GIS) and statistical analysis. Open source software offers governments transparency, potential cost savings, and the ability to customize software for their specific needs. Non-Profit Organizations : Non-profits often operate with limited budgets, making open source software an attractive option due to its cost-effectiveness. Additionally, the ethos of community and collaboration in the open source world aligns with the values of many non-profit organizations. Small and Medium Enterprises (SMEs) : SMEs frequently turn to open source software to manage their business operations, including customer relationship management (CRM) systems, enterprise resource planning (ERP) systems, and content management systems (CMS). Open source solutions can provide SMEs with powerful tools without the significant investment required for commercial software. Large Corporations : Many large corporations not only use but also contribute to open source software. They may use open source for running servers, databases, development tools, and even in their product offerings. Companies like Google, Facebook, and IBM are known for their contributions to and reliance on open source software. Cloud and Technology Service Providers : These companies use open source software to build and offer services to their customers, including web hosting, cloud computing resources, and platform-as-a-service (PaaS) offerings. Open source software enables them to provide scalable and flexible services at a competitive price. In summary, open source software consumers include virtually anyone who uses technology, from individuals seeking free and customizable software options to large organizations looking for reliable, cost-effective, and scalable solutions. The open nature of the software encourages a wide range of use cases and fosters a vibrant community of users and contributors.","title":"Consumer"},{"location":"concepts/oss-consumer/#open-source-software-consumers","text":"Open source software consumers encompass a broad and diverse group that ranges from individual hobbyists to large multinational corporations. The nature of open source software, being freely available for use, modification, and distribution, makes it appealing to a wide array of users for various purposes. Here are some of the main categories of open source software consumers: Individual Users : Many individuals use open source software for personal computing needs, such as operating systems (e.g., Linux distributions), office suites (e.g., LibreOffice), web browsers (e.g., Firefox), and multimedia editing (e.g., GIMP for image editing, Audacity for audio editing). The appeal often lies in the lack of licensing fees, the ability to customize the software, and privacy considerations. Developers and Programmers : Open source software is incredibly popular among developers for a multitude of reasons, including the availability of source code, the ability to modify and improve the software, and the vast community support. Developers use open source tools, libraries, frameworks, and development environments for creating a wide range of applications. Educational Institutions : Schools, colleges, and universities often utilize open source software for educational purposes, including teaching programming and computer science, running servers and computer labs, and conducting research. Open source software provides an affordable solution for educational institutions, allowing students to learn with real-world tools that they can also use outside of the institution without licensing restrictions. Government Agencies : Governments around the world use open source software for various applications, from running websites and managing data to more specialized uses like geographic information systems (GIS) and statistical analysis. Open source software offers governments transparency, potential cost savings, and the ability to customize software for their specific needs. Non-Profit Organizations : Non-profits often operate with limited budgets, making open source software an attractive option due to its cost-effectiveness. Additionally, the ethos of community and collaboration in the open source world aligns with the values of many non-profit organizations. Small and Medium Enterprises (SMEs) : SMEs frequently turn to open source software to manage their business operations, including customer relationship management (CRM) systems, enterprise resource planning (ERP) systems, and content management systems (CMS). Open source solutions can provide SMEs with powerful tools without the significant investment required for commercial software. Large Corporations : Many large corporations not only use but also contribute to open source software. They may use open source for running servers, databases, development tools, and even in their product offerings. Companies like Google, Facebook, and IBM are known for their contributions to and reliance on open source software. Cloud and Technology Service Providers : These companies use open source software to build and offer services to their customers, including web hosting, cloud computing resources, and platform-as-a-service (PaaS) offerings. Open source software enables them to provide scalable and flexible services at a competitive price. In summary, open source software consumers include virtually anyone who uses technology, from individuals seeking free and customizable software options to large organizations looking for reliable, cost-effective, and scalable solutions. The open nature of the software encourages a wide range of use cases and fosters a vibrant community of users and contributors.","title":"Open Source Software Consumers"},{"location":"concepts/oss-distributor/","text":"Open Source Software Distributors \u00b6 Open source software distributors are individuals, organizations, or companies that distribute software which is licensed in a way that allows its source code to be freely accessed, used, modified, and shared by anyone. These distributors play a crucial role in the open source ecosystem by making software easily accessible to both users and developers, and often contribute to the development and improvement of the software through community collaboration. There are several types of open source software distributors, including: Original Project Authors or Maintainers : These are the individuals or organizations that originally create an open source project. They often continue to distribute the software directly from their own websites or through source code repositories like GitHub, GitLab, or Bitbucket. Community Distributions : For some open source projects, especially larger ones, the community around the project may organize to distribute the software. This can include providing easy-to-install packages, documentation, and support forums. Examples include many Linux distributions, such as Debian, Fedora, and Ubuntu, which are community-driven projects. Commercial Companies : Some companies specialize in distributing open source software by providing additional services such as support, training, or custom development. These companies may offer a free, community version of the software alongside commercial versions that include extra features or services. Red Hat and SUSE are examples of companies that distribute open source software in this way. Software Repositories and Package Managers : Many programming languages and operating systems have centralized repositories or package managers that distribute open source software. Examples include the Python Package Index (PyPI) for Python libraries, npm for Node.js packages, and the various package managers used by Linux distributions (e.g., APT for Debian/Ubuntu, YUM/DNF for Fedora/RHEL). Third-Party Websites and Services : There are also many websites and services that aggregate and distribute open source software, making it easier for users to find and download. Examples include SourceForge, FossHub, and OSDN. These platforms often provide hosting for the software, forums for support, and sometimes even tools for managing project development. Cloud-Based Service Providers : Some cloud service providers offer open source software as part of their platform, either as standalone services or integrated into their offerings. This can include databases, web servers, development tools, and more. While the software itself remains open source, the cloud provider may charge for the infrastructure or additional services used to run it. Open source software distributors are vital for the dissemination and adoption of open source software, providing pathways for software to reach users and for users to contribute back to the projects they benefit from.","title":"Distributor"},{"location":"concepts/oss-distributor/#open-source-software-distributors","text":"Open source software distributors are individuals, organizations, or companies that distribute software which is licensed in a way that allows its source code to be freely accessed, used, modified, and shared by anyone. These distributors play a crucial role in the open source ecosystem by making software easily accessible to both users and developers, and often contribute to the development and improvement of the software through community collaboration. There are several types of open source software distributors, including: Original Project Authors or Maintainers : These are the individuals or organizations that originally create an open source project. They often continue to distribute the software directly from their own websites or through source code repositories like GitHub, GitLab, or Bitbucket. Community Distributions : For some open source projects, especially larger ones, the community around the project may organize to distribute the software. This can include providing easy-to-install packages, documentation, and support forums. Examples include many Linux distributions, such as Debian, Fedora, and Ubuntu, which are community-driven projects. Commercial Companies : Some companies specialize in distributing open source software by providing additional services such as support, training, or custom development. These companies may offer a free, community version of the software alongside commercial versions that include extra features or services. Red Hat and SUSE are examples of companies that distribute open source software in this way. Software Repositories and Package Managers : Many programming languages and operating systems have centralized repositories or package managers that distribute open source software. Examples include the Python Package Index (PyPI) for Python libraries, npm for Node.js packages, and the various package managers used by Linux distributions (e.g., APT for Debian/Ubuntu, YUM/DNF for Fedora/RHEL). Third-Party Websites and Services : There are also many websites and services that aggregate and distribute open source software, making it easier for users to find and download. Examples include SourceForge, FossHub, and OSDN. These platforms often provide hosting for the software, forums for support, and sometimes even tools for managing project development. Cloud-Based Service Providers : Some cloud service providers offer open source software as part of their platform, either as standalone services or integrated into their offerings. This can include databases, web servers, development tools, and more. While the software itself remains open source, the cloud provider may charge for the infrastructure or additional services used to run it. Open source software distributors are vital for the dissemination and adoption of open source software, providing pathways for software to reach users and for users to contribute back to the projects they benefit from.","title":"Open Source Software Distributors"},{"location":"concepts/oss-producer/","text":"Open Source Software Producers \u00b6 Open source software producers are individuals, groups, or organizations that create, develop, and maintain software under licenses that allow users to freely use, study, modify, and distribute the software and its source code. These producers play a foundational role in the open source ecosystem, contributing a wide range of software solutions from small utilities to large-scale enterprise systems. Here's a closer look at the various types of open source software producers: Individual Developers : Many open source projects start as personal initiatives by individual developers. These projects can range from simple scripts and tools to complex applications. Individual developers might work on open source projects for learning, for the challenge, to solve a personal need, or to contribute to the community. Open Source Communities : Some of the most successful and widely used open source projects are developed and maintained by communities of volunteers. These communities consist of developers, users, and enthusiasts who collaborate to improve the software. Examples include the Linux kernel, the Apache Software Foundation projects, and the Mozilla Foundation projects. Community-driven projects often have a governance model that includes meritocracy or a democratic approach to decision making. Companies : Many companies are significant producers of open source software. Some companies start open source projects to encourage the adoption of technologies that complement their business, to set industry standards, or to benefit from community contributions. Companies like Red Hat, Google, and Facebook have released and actively maintain numerous open source projects. Additionally, some companies are founded specifically to offer services, support, and development for open source software products. Academic and Research Institutions : Universities, colleges, and research organizations often produce open source software as part of academic research projects or educational initiatives. This software can be related to scientific research, educational tools, or technology development. Academic-produced open source software not only serves as a resource for learning and experimentation but also contributes to scientific and technological advancement. Government and Public Sector Organizations : In recent years, various government bodies and public sector organizations have started to embrace open source software, both as users and as producers. By developing software solutions as open source, governments can enhance transparency, reduce costs, and encourage collaboration and reuse within and between government agencies and the public. Non-Profit Organizations : Some non-profits focus on producing open source software that aligns with their mission, such as promoting privacy, accessibility, or social good. These organizations might develop software tools and platforms to support their causes, relying on community contributions and philanthropic support to sustain their development efforts. Collaborative Projects and Foundations : There are also collaborative projects and foundations that are specifically established to support the development of open source software. These entities provide a legal and organizational framework for projects, helping to manage contributions, licensing, and sometimes financial support. Examples include the Linux Foundation, the Free Software Foundation, and the Eclipse Foundation. Open source software producers are motivated by a variety of factors, including the desire to share knowledge, collaborate with others, drive innovation, and create software that is freely available for anyone to use and improve. The diversity of producers ensures a rich and vibrant ecosystem of open source software covering nearly every conceivable application and use case.","title":"Producer"},{"location":"concepts/oss-producer/#open-source-software-producers","text":"Open source software producers are individuals, groups, or organizations that create, develop, and maintain software under licenses that allow users to freely use, study, modify, and distribute the software and its source code. These producers play a foundational role in the open source ecosystem, contributing a wide range of software solutions from small utilities to large-scale enterprise systems. Here's a closer look at the various types of open source software producers: Individual Developers : Many open source projects start as personal initiatives by individual developers. These projects can range from simple scripts and tools to complex applications. Individual developers might work on open source projects for learning, for the challenge, to solve a personal need, or to contribute to the community. Open Source Communities : Some of the most successful and widely used open source projects are developed and maintained by communities of volunteers. These communities consist of developers, users, and enthusiasts who collaborate to improve the software. Examples include the Linux kernel, the Apache Software Foundation projects, and the Mozilla Foundation projects. Community-driven projects often have a governance model that includes meritocracy or a democratic approach to decision making. Companies : Many companies are significant producers of open source software. Some companies start open source projects to encourage the adoption of technologies that complement their business, to set industry standards, or to benefit from community contributions. Companies like Red Hat, Google, and Facebook have released and actively maintain numerous open source projects. Additionally, some companies are founded specifically to offer services, support, and development for open source software products. Academic and Research Institutions : Universities, colleges, and research organizations often produce open source software as part of academic research projects or educational initiatives. This software can be related to scientific research, educational tools, or technology development. Academic-produced open source software not only serves as a resource for learning and experimentation but also contributes to scientific and technological advancement. Government and Public Sector Organizations : In recent years, various government bodies and public sector organizations have started to embrace open source software, both as users and as producers. By developing software solutions as open source, governments can enhance transparency, reduce costs, and encourage collaboration and reuse within and between government agencies and the public. Non-Profit Organizations : Some non-profits focus on producing open source software that aligns with their mission, such as promoting privacy, accessibility, or social good. These organizations might develop software tools and platforms to support their causes, relying on community contributions and philanthropic support to sustain their development efforts. Collaborative Projects and Foundations : There are also collaborative projects and foundations that are specifically established to support the development of open source software. These entities provide a legal and organizational framework for projects, helping to manage contributions, licensing, and sometimes financial support. Examples include the Linux Foundation, the Free Software Foundation, and the Eclipse Foundation. Open source software producers are motivated by a variety of factors, including the desire to share knowledge, collaborate with others, drive innovation, and create software that is freely available for anyone to use and improve. The diversity of producers ensures a rich and vibrant ecosystem of open source software covering nearly every conceivable application and use case.","title":"Open Source Software Producers"},{"location":"concepts/ossverse-operator/","text":"OSSVerse Marketplace Operator \u00b6 The marketplace operator, in the context of OSSVerse that connects open source software consumers (buyers) with open source software assurance service providers (sellers), serves as a facilitator and intermediary to ensure the smooth functioning of transactions and interactions between these two groups. The role and responsibilities of such an operator can be detailed as follows: OSSVerse marketplace operator is the role responsible for the platform that provides a digital venue where open source software consumers can find, evaluate, and engage services that offer assurance, such as security audits, compliance checks, and quality assessments, for open source software. The operator does not own the services being offered but creates a space where service providers can list their offerings and consumers can browse through them to find the services that best match their needs. Role and Responsibilities: Facilitation of Transactions : The operator enables transactions between buyers and consumers, often handling aspects like listing services, processing payments, and possibly offering escrow services to ensure both parties are protected during the transaction. Quality Control and Assurance : They may establish standards or criteria for service providers to meet before they can offer their services on the platform. This ensures that consumers find reliable and high-quality services. Dispute Resolution : The operator often plays a role in resolving disputes between consumers and service providers, ensuring fair treatment and maintaining trust in the platform. Marketplace Integrity : Maintaining the integrity of the marketplace by implementing and enforcing policies to prevent fraud, abuse, and other activities that could harm users or the reputation of the marketplace. User Experience and Support : Providing a user-friendly interface that makes it easy for consumers to find and purchase services and for service providers to list and manage their offerings. Additionally, offering customer support to assist users with any issues that arise. Feedback and Ratings System : Implementing a system that allows buyers to rate and review services, which helps future consumers make informed decisions and encourages service providers to maintain high standards. Compliance and Legal : Ensuring that the marketplace operates in compliance with relevant laws and regulations, including data protection and privacy laws, intellectual property rights, and any specific regulations related to software assurance services. Community Building : Often, such platforms also work towards building a community around their marketplace, offering forums, resources, and events that help both consumers and service providers to network, share knowledge, and stay updated on industry trends. In summary, the role of marketplace operator in this context is multifaceted, focusing on facilitating transactions, ensuring quality and integrity, supporting users, and complying with legal requirements, all while fostering a community that promotes the growth and security of open source software.","title":"Operator"},{"location":"concepts/ossverse-operator/#ossverse-marketplace-operator","text":"The marketplace operator, in the context of OSSVerse that connects open source software consumers (buyers) with open source software assurance service providers (sellers), serves as a facilitator and intermediary to ensure the smooth functioning of transactions and interactions between these two groups. The role and responsibilities of such an operator can be detailed as follows: OSSVerse marketplace operator is the role responsible for the platform that provides a digital venue where open source software consumers can find, evaluate, and engage services that offer assurance, such as security audits, compliance checks, and quality assessments, for open source software. The operator does not own the services being offered but creates a space where service providers can list their offerings and consumers can browse through them to find the services that best match their needs. Role and Responsibilities: Facilitation of Transactions : The operator enables transactions between buyers and consumers, often handling aspects like listing services, processing payments, and possibly offering escrow services to ensure both parties are protected during the transaction. Quality Control and Assurance : They may establish standards or criteria for service providers to meet before they can offer their services on the platform. This ensures that consumers find reliable and high-quality services. Dispute Resolution : The operator often plays a role in resolving disputes between consumers and service providers, ensuring fair treatment and maintaining trust in the platform. Marketplace Integrity : Maintaining the integrity of the marketplace by implementing and enforcing policies to prevent fraud, abuse, and other activities that could harm users or the reputation of the marketplace. User Experience and Support : Providing a user-friendly interface that makes it easy for consumers to find and purchase services and for service providers to list and manage their offerings. Additionally, offering customer support to assist users with any issues that arise. Feedback and Ratings System : Implementing a system that allows buyers to rate and review services, which helps future consumers make informed decisions and encourages service providers to maintain high standards. Compliance and Legal : Ensuring that the marketplace operates in compliance with relevant laws and regulations, including data protection and privacy laws, intellectual property rights, and any specific regulations related to software assurance services. Community Building : Often, such platforms also work towards building a community around their marketplace, offering forums, resources, and events that help both consumers and service providers to network, share knowledge, and stay updated on industry trends. In summary, the role of marketplace operator in this context is multifaceted, focusing on facilitating transactions, ensuring quality and integrity, supporting users, and complying with legal requirements, all while fostering a community that promotes the growth and security of open source software.","title":"OSSVerse Marketplace Operator"},{"location":"concepts/overview/","text":"OSSVerse is an open source Marketplace. It is conceptualized as an eco system project comprising of multiple platforms. It is an adaptation of ONDC and Beckn protocol for open source software service delivery. OSSVerse leverages BeSecure(BeS) for delivering open source software security assurance services. Aims to establish an open network of OASPs for businesses that will offer trustworthy and reliable open source software assurance services. Video Presentation \u00b6 Check out this intro video that explains what OSSVerse is in less than 5 minutes: Key Stakeholders \u00b6 A Trustworthy Marketplace for Open-Source Assurance Service Providers (OASP) backed by a thriving open-source security community. Affordable and timely access to vendor neutral security services for organizations producing and consuming open-source. OASP led open source assurance service delivery would give full control over their open source components Open Source Assurance Service Provider (OASP) : An organization that provides security assurance support services like risk assessment, a hardened version of the OSS artifact, remediation of one or more specific vulnerabilities. Provide Validation, Verification, Attestation & Support Service. Set up dedicated remediated pipelines for organizations, Validate models, create model cards & ensure quality/security. Open Source Producers (Individuals & Organizations) : Create and share open source models. Open Source Consumers (Organizations) : Consume and request for assurance services of open source models. Open Source Distributors (Platforms & Organizations) : Distribute the sharing of open-source models along with the proof of attestation of models OASP. Security Experts & Freelancers : Offering open-source security assessments and support services. Open Source Marketplace operator/consortia : Deploys people resources to maintain and operate marketplace and responsible for the governance marketplace.","title":"OSSVerse"},{"location":"concepts/overview/#video-presentation","text":"Check out this intro video that explains what OSSVerse is in less than 5 minutes:","title":"Video Presentation"},{"location":"concepts/overview/#key-stakeholders","text":"A Trustworthy Marketplace for Open-Source Assurance Service Providers (OASP) backed by a thriving open-source security community. Affordable and timely access to vendor neutral security services for organizations producing and consuming open-source. OASP led open source assurance service delivery would give full control over their open source components Open Source Assurance Service Provider (OASP) : An organization that provides security assurance support services like risk assessment, a hardened version of the OSS artifact, remediation of one or more specific vulnerabilities. Provide Validation, Verification, Attestation & Support Service. Set up dedicated remediated pipelines for organizations, Validate models, create model cards & ensure quality/security. Open Source Producers (Individuals & Organizations) : Create and share open source models. Open Source Consumers (Organizations) : Consume and request for assurance services of open source models. Open Source Distributors (Platforms & Organizations) : Distribute the sharing of open-source models along with the proof of attestation of models OASP. Security Experts & Freelancers : Offering open-source security assessments and support services. Open Source Marketplace operator/consortia : Deploys people resources to maintain and operate marketplace and responsible for the governance marketplace.","title":"Key Stakeholders"},{"location":"concepts/roles-you-can-play/","text":"Read through the following sections which describe what OSSVerse can bring on the table depending on the type of participant in the decentralized marketplace. Check out this marketplace transaction video that explains how can different parties transact in OSSVerse marketplace: OSSVerse for Open Source Projects and ML Model Consumers \u00b6 Accelerate development and optimize costs Annual cost savings are a new standard and best practice. Profits are being targeted to business development, regulatory and compliance needs. Resulting in a reduction of pressure on IT and development budgets, granting the opportunity to focus in new features and boost innovation. Modernize applications architecture (loosely coupled and modular) Strategically decompose the monolith into a fine-grained, loosely coupled modular architecture to increase both development and business agility. When the system architecture is designed to allow teams to test, deploy and change systems without relying on other teams, they require little communication to get the job done. In other words, both the architecture and the teams are loosely coupled. Innovation - Rapidly adopt new technologies and reduce development time Use Leverage Reference Architecture and for AWS + our libraries to provide a collection of cloud application architecture components to build and deploy faster in the cloud. Building a cloud Landing Zone is complex, especially since most companies have little or no expertise in this area. And it can take a significant amount of time to get it right. Leverage a reference architecture to give you an AWS Landing Zone that provides a consistent and solid \"foundations\" to bootstrap your project in the cloud. The code solution implements the best AWS Well-Architected Framework practices as well as the battle-tested tech experience and years of knowledge of our contributors. Hours or days, not weeks or months Leverage implements infrastructure as code at all times. We have rolled this out using Terraform, and has been fully proven in AWS and other Terraform providers that are part of our reference architecture like Kubernetes, Helm and Hashicorp Vault. By using the Leverage CLI , our binary will help you to quickly bootstrap your AWS Landing Zone in a matter of hours (or at most a few days). OSSVerse for Open Source Projects and ML Model Producers \u00b6 It's not just a pile of scripts It's not just another layer of untested, one time and stand-alone developed scripts. The code is modularized and well designed under best practices, our Leverage CLI has both unit and integration tests. While our Terraform code has been extensively E2E tested. Moreover, 100% of the code is yours (to modify, extend, reuse, etc), with no vendor locking and vendor licensing fees. We use the MIT license, so you can take the code, modify it and use it as your private code. All we ask in return is a friendly greeting and that (if possible) consider contributing to binbash Leverage project. Implement Leverage yourself or we can deploy it for you! DevOps culture and methodologies Team agility and continuous improvements based on feedback loops are some of the main drivers of cloud adoption, and IAC's goal of reducing the frequency of deployment of both infrastructure and applications are some of the most important aspects of DevOps practices. We continue to apply these methodologies to achieve a DevOps first culture. We have experienced and demonstrated their potential and have practiced them in dozens of projects over the past 5 years. The Leverage reference architecture for AWS combines a set of application best practices, technology patterns and a common CI/CD deployment approach through Leverage CLI for all your application environments. As a result, we are pursuing a world-class software delivery performance through optimized collaboration, communication, reliability, stability, scalability and security at ever-decreasing cost and effort. Repeatable, composable and extensible immutable infrastructure The best high-performance development teams create and recreate their development and production environments using infrastructure as code (IaC) as part of their daily development processes. The Leverage CLI allows to build repeatable and immutable infrastructure. So your cloud development, staging and production environments will consistently be the same. OSSVerse for Open Source Assurance Service Providers \u00b6 Provisioning infrastructure as code (Iac) Instead of manually provisioning infrastructure, the real benefits of cloud adoption come from orchestrating infrastructure through code. However, this is really challenging to achieve, there are literally thousands of tiny things and configs to consider and they all seem to take forever. Our experience is that it can take teams up to 24 months to achieve a desired infra state in AWS. By using Leverage you could get your AWS Landing-Zone in few weeks, or your entire AWS Well-Architected based cloud solution within 1 to 3 months (depending on your project complexity needs). We've done it before (don't reinvent the wheel) Often, development teams have similar and recurring requests such as: iam, networking, security, storage, databases, compute and secret management, etc. binbash Leverage has been proven in dozen of project to create software-defined (IaC) AWS environments. Best practices baked in the code Leverage provides IaC reference architecture for AWS hosted applications infrastructure. This is baked into the code as a combination of the best AWS Well-Architected framework practices and the experience of having successfully orchestrated many customers to AWS cloud. On-demand infra deployment Leverage provides your DevOps, Cloud, SRE and Development teams with the ability to provision on-demand infrastructure, granting that it will meet the rigorous security requirements of modern cloud native best practices. It fully implements AWS Well-Architected Framework (WAF) and best DevOps practices, including practices, including collaboration, version control, CI/CD, continuous testing, cloud infrastructure and losely couple architectures. Easier to support and maintain Leverage IaC approach significantly reduce your AWS infra deployment, config and support burden and reduce risk. Our code backed provisioning has been rigorously tested many times, eliminating the possibility of manual errors. Because the entire infrastructure is deployed from the same proven code, the consistency your cloud environments will simplify your setup and maintenance. Use the versioned code to iterate and improve, extend or compose your internal processes as your cloud operating model evolves. There is no vendor lock-in. You own the solution With Leverage you own 100% of the code with no lock-in clauses. If you choose to leave Leverage , you will still have your entire AWS cloud infrastructure that you can access and manage. If you drop Leverage , you will still have your entire cloud native infrastructure code (Terraform, Helm, Ansible, Python). It\u2019s 100% Open Source on GitHub and is free to use with no strings attached under MIT license (no licensing fees), and you are free to commercially and privately use, distribute and modify.","title":"Roles you can play"},{"location":"concepts/roles-you-can-play/#ossverse-for-open-source-projects-and-ml-model-consumers","text":"Accelerate development and optimize costs Annual cost savings are a new standard and best practice. Profits are being targeted to business development, regulatory and compliance needs. Resulting in a reduction of pressure on IT and development budgets, granting the opportunity to focus in new features and boost innovation. Modernize applications architecture (loosely coupled and modular) Strategically decompose the monolith into a fine-grained, loosely coupled modular architecture to increase both development and business agility. When the system architecture is designed to allow teams to test, deploy and change systems without relying on other teams, they require little communication to get the job done. In other words, both the architecture and the teams are loosely coupled. Innovation - Rapidly adopt new technologies and reduce development time Use Leverage Reference Architecture and for AWS + our libraries to provide a collection of cloud application architecture components to build and deploy faster in the cloud. Building a cloud Landing Zone is complex, especially since most companies have little or no expertise in this area. And it can take a significant amount of time to get it right. Leverage a reference architecture to give you an AWS Landing Zone that provides a consistent and solid \"foundations\" to bootstrap your project in the cloud. The code solution implements the best AWS Well-Architected Framework practices as well as the battle-tested tech experience and years of knowledge of our contributors. Hours or days, not weeks or months Leverage implements infrastructure as code at all times. We have rolled this out using Terraform, and has been fully proven in AWS and other Terraform providers that are part of our reference architecture like Kubernetes, Helm and Hashicorp Vault. By using the Leverage CLI , our binary will help you to quickly bootstrap your AWS Landing Zone in a matter of hours (or at most a few days).","title":"OSSVerse for Open Source Projects and ML Model Consumers"},{"location":"concepts/roles-you-can-play/#ossverse-for-open-source-projects-and-ml-model-producers","text":"It's not just a pile of scripts It's not just another layer of untested, one time and stand-alone developed scripts. The code is modularized and well designed under best practices, our Leverage CLI has both unit and integration tests. While our Terraform code has been extensively E2E tested. Moreover, 100% of the code is yours (to modify, extend, reuse, etc), with no vendor locking and vendor licensing fees. We use the MIT license, so you can take the code, modify it and use it as your private code. All we ask in return is a friendly greeting and that (if possible) consider contributing to binbash Leverage project. Implement Leverage yourself or we can deploy it for you! DevOps culture and methodologies Team agility and continuous improvements based on feedback loops are some of the main drivers of cloud adoption, and IAC's goal of reducing the frequency of deployment of both infrastructure and applications are some of the most important aspects of DevOps practices. We continue to apply these methodologies to achieve a DevOps first culture. We have experienced and demonstrated their potential and have practiced them in dozens of projects over the past 5 years. The Leverage reference architecture for AWS combines a set of application best practices, technology patterns and a common CI/CD deployment approach through Leverage CLI for all your application environments. As a result, we are pursuing a world-class software delivery performance through optimized collaboration, communication, reliability, stability, scalability and security at ever-decreasing cost and effort. Repeatable, composable and extensible immutable infrastructure The best high-performance development teams create and recreate their development and production environments using infrastructure as code (IaC) as part of their daily development processes. The Leverage CLI allows to build repeatable and immutable infrastructure. So your cloud development, staging and production environments will consistently be the same.","title":"OSSVerse for Open Source Projects and ML Model Producers"},{"location":"concepts/roles-you-can-play/#ossverse-for-open-source-assurance-service-providers","text":"Provisioning infrastructure as code (Iac) Instead of manually provisioning infrastructure, the real benefits of cloud adoption come from orchestrating infrastructure through code. However, this is really challenging to achieve, there are literally thousands of tiny things and configs to consider and they all seem to take forever. Our experience is that it can take teams up to 24 months to achieve a desired infra state in AWS. By using Leverage you could get your AWS Landing-Zone in few weeks, or your entire AWS Well-Architected based cloud solution within 1 to 3 months (depending on your project complexity needs). We've done it before (don't reinvent the wheel) Often, development teams have similar and recurring requests such as: iam, networking, security, storage, databases, compute and secret management, etc. binbash Leverage has been proven in dozen of project to create software-defined (IaC) AWS environments. Best practices baked in the code Leverage provides IaC reference architecture for AWS hosted applications infrastructure. This is baked into the code as a combination of the best AWS Well-Architected framework practices and the experience of having successfully orchestrated many customers to AWS cloud. On-demand infra deployment Leverage provides your DevOps, Cloud, SRE and Development teams with the ability to provision on-demand infrastructure, granting that it will meet the rigorous security requirements of modern cloud native best practices. It fully implements AWS Well-Architected Framework (WAF) and best DevOps practices, including practices, including collaboration, version control, CI/CD, continuous testing, cloud infrastructure and losely couple architectures. Easier to support and maintain Leverage IaC approach significantly reduce your AWS infra deployment, config and support burden and reduce risk. Our code backed provisioning has been rigorously tested many times, eliminating the possibility of manual errors. Because the entire infrastructure is deployed from the same proven code, the consistency your cloud environments will simplify your setup and maintenance. Use the versioned code to iterate and improve, extend or compose your internal processes as your cloud operating model evolves. There is no vendor lock-in. You own the solution With Leverage you own 100% of the code with no lock-in clauses. If you choose to leave Leverage , you will still have your entire AWS cloud infrastructure that you can access and manage. If you drop Leverage , you will still have your entire cloud native infrastructure code (Terraform, Helm, Ansible, Python). It\u2019s 100% Open Source on GitHub and is free to use with no strings attached under MIT license (no licensing fees), and you are free to commercially and privately use, distribute and modify.","title":"OSSVerse for Open Source Assurance Service Providers"},{"location":"concepts/tech-stack/","text":"OSSVerse is built around the Beckn Protocol and it uses a stack that includes Beckn-Onix , BeSecure , BeSLab and other tools. We are also adopters and supporters of Open Source and the Cloud Native movement, which should become self-evident as you keep exploring our technology stack. Why did we choose our tech stack? \u00b6 Why Beckn\u2753 Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. Millions of customers\u2014including the fastest-growing startups, largest enterprises, and leading government agencies\u2014are using AWS to lower costs, become more agile, and innovate faster. Build, Deploy, and Manage Websites, Apps or Processes On AWS' Secure, Reliable Network. AWS is Secure, Reliable, Scalable Services. HIPAA Compliant. Easily Manage Clusters. Global Infrastructure. Highly Scalable. Read More: What is AWS Why BeSecure\u2753 AWS Well-Architected helps cloud architects to build secure, high-performing, resilient, and efficient infrastructure for their applications and workloads. Based on five pillars \u2014 operational excellence, security, reliability, performance efficiency, and cost optimization \u2014 AWS Well-Architected provides a consistent approach for customers and partners to evaluate architectures, and implement designs that can scale over time. Read More: AWS Well-architected Why Security as Code\u2753 Confidence: A change breaks the env? Just roll it back. Still not working? Build a whole new env with a few keystrokes. IaC enables this. Repeatability: Allows your infra to be automatically instantiated, making it easy to build multiple identical envs. Troubleshooting: Check source control and see exactly what changed in the env. As long as you are diligent and don\u2019t make manual envs changes, then IaC can be a game changer. DR: Require the ability to set up an alternate env in a different DC or Region. IaC makes this a much more manageable prospect. Auditability: You will need to be able to audit both changes and access to an env, IaC gives you this right out of the box. Visibility: As an env expands over time, is challenging to tell what has been provisioned. In the #cloud this can be a huge #cost issue. IaC allows tracking your resources. Portability: Some IaC techs are #multicloud. Also, translating #Terraform from one cloud provider to another is considerably more simple than recreating your entire envs in a cloud-specific tool. Security: See history of changes to your SG rules along with commit messages can do wonders for being confident about the security configs of your envs. Terraform allows to codify your application infrastructure, reduce human error and increase automation by provisioning infrastructure as code. With TF we can manage infrastructure across clouds and provision infrastructure across 300+ public clouds and services using a single workflow. Moreover it helps to create reproducible infrastructure and provision consistent testing, staging, and production environments with the same configuration. Terraform has everything we expect from a IaC framework: open source, cloud-agnostic provisioning tool that supported immutable infrastructure, a declarative language, and a client-only architecture. Read More Why Infrastructure as Code Why Terraform by Gruntwork Why BeSLab\u2753 AWS Organizations helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts. Read More How it works: AWS Organizations AWS Organizations Why Open Source Projects\u2753 AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. Integration and Fine-grained access control with almost every AWS service and its resources. Multi-factor authentication for highly privileged users. Analyze, monitor and audit access. Read More How it works: AWS IAM AWS Identity and Access Management (IAM) Why Open Source LLMs\u2753 Amazon Virtual Private Cloud (Amazon VPC) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 for most resources in your virtual private cloud, helping to ensure secure and easy access to resources and applications. Read More How it works: AWS Networking AWS Virtual Private Cloud Why S3\u2753 Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world. Read More How it works: AWS Storage AWS S3","title":"Tech stack"},{"location":"concepts/tech-stack/#why-did-we-choose-our-tech-stack","text":"Why Beckn\u2753 Amazon Web Services (AWS) is the world\u2019s most comprehensive and broadly adopted cloud platform, offering over 200 fully featured services from data centers globally. Millions of customers\u2014including the fastest-growing startups, largest enterprises, and leading government agencies\u2014are using AWS to lower costs, become more agile, and innovate faster. Build, Deploy, and Manage Websites, Apps or Processes On AWS' Secure, Reliable Network. AWS is Secure, Reliable, Scalable Services. HIPAA Compliant. Easily Manage Clusters. Global Infrastructure. Highly Scalable. Read More: What is AWS Why BeSecure\u2753 AWS Well-Architected helps cloud architects to build secure, high-performing, resilient, and efficient infrastructure for their applications and workloads. Based on five pillars \u2014 operational excellence, security, reliability, performance efficiency, and cost optimization \u2014 AWS Well-Architected provides a consistent approach for customers and partners to evaluate architectures, and implement designs that can scale over time. Read More: AWS Well-architected Why Security as Code\u2753 Confidence: A change breaks the env? Just roll it back. Still not working? Build a whole new env with a few keystrokes. IaC enables this. Repeatability: Allows your infra to be automatically instantiated, making it easy to build multiple identical envs. Troubleshooting: Check source control and see exactly what changed in the env. As long as you are diligent and don\u2019t make manual envs changes, then IaC can be a game changer. DR: Require the ability to set up an alternate env in a different DC or Region. IaC makes this a much more manageable prospect. Auditability: You will need to be able to audit both changes and access to an env, IaC gives you this right out of the box. Visibility: As an env expands over time, is challenging to tell what has been provisioned. In the #cloud this can be a huge #cost issue. IaC allows tracking your resources. Portability: Some IaC techs are #multicloud. Also, translating #Terraform from one cloud provider to another is considerably more simple than recreating your entire envs in a cloud-specific tool. Security: See history of changes to your SG rules along with commit messages can do wonders for being confident about the security configs of your envs. Terraform allows to codify your application infrastructure, reduce human error and increase automation by provisioning infrastructure as code. With TF we can manage infrastructure across clouds and provision infrastructure across 300+ public clouds and services using a single workflow. Moreover it helps to create reproducible infrastructure and provision consistent testing, staging, and production environments with the same configuration. Terraform has everything we expect from a IaC framework: open source, cloud-agnostic provisioning tool that supported immutable infrastructure, a declarative language, and a client-only architecture. Read More Why Infrastructure as Code Why Terraform by Gruntwork Why BeSLab\u2753 AWS Organizations helps you centrally manage and govern your environment as you grow and scale your AWS resources. Using AWS Organizations, you can programmatically create new AWS accounts and allocate resources, group accounts to organize your workflows, apply policies to accounts or groups for governance, and simplify billing by using a single payment method for all of your accounts. Read More How it works: AWS Organizations AWS Organizations Why Open Source Projects\u2753 AWS Identity and Access Management (IAM) enables you to manage access to AWS services and resources securely. Using IAM, you can create and manage AWS users and groups, and use permissions to allow and deny their access to AWS resources. Integration and Fine-grained access control with almost every AWS service and its resources. Multi-factor authentication for highly privileged users. Analyze, monitor and audit access. Read More How it works: AWS IAM AWS Identity and Access Management (IAM) Why Open Source LLMs\u2753 Amazon Virtual Private Cloud (Amazon VPC) is a service that lets you launch AWS resources in a logically isolated virtual network that you define. You have complete control over your virtual networking environment, including selection of your own IP address range, creation of subnets, and configuration of route tables and network gateways. You can use both IPv4 and IPv6 for most resources in your virtual private cloud, helping to ensure secure and easy access to resources and applications. Read More How it works: AWS Networking AWS Virtual Private Cloud Why S3\u2753 Amazon Simple Storage Service (Amazon S3) is an object storage service that offers industry-leading scalability, data availability, security, and performance. This means customers of all sizes and industries can use it to store and protect any amount of data for a range of use cases, such as data lakes, websites, mobile applications, backup and restore, archive, enterprise applications, IoT devices, and big data analytics. Amazon S3 provides easy-to-use management features so you can organize your data and configure finely-tuned access controls to meet your specific business, organizational, and compliance requirements. Amazon S3 is designed for 99.999999999% (11 9's) of durability, and stores data for millions of applications for companies all around the world. Read More How it works: AWS Storage AWS S3","title":"Why did we choose our tech stack?"},{"location":"concepts/why-ossverse/","text":"Many organizations lack internal expertise to effectively assure and remediate open-source models, creating a demand for external services such as Open-source Assurance Service Providers (OASP). Organizations require timely support for open-source models, datasets, and projects. Did you know? The widespread adoption of OSS (open source software) across industries has brought immense innovation but also unique security challenges. One of the barriers to OSS adoption is the lack of Trust.! The solution \u00b6 A Trustworthy Marketplace for Open-Source Assurance Service Providers (OASP) backed by a thriving open-source security community. Affordable and timely access to vendor neutral security services for organizations producing and consuming open-source. OASP led open source assurance service delivery would give full control over their open source components Considerations for an Open Source first strategy \u00b6 Supply Chain & Security Risk While open-source software can be more secure than proprietary software due to its transparency and the potential for widespread review, vulnerabilities can still exist. Organizations should have a strategy for monitoring and applying security patches and updates promptly. Assessing the security practices and history of the OSS project is crucial Limited Control and Support Unlike proprietary software, which typically comes with vendor support, open-source projects rely on community support or third-party vendors for troubleshooting and maintenance. Organizations must consider the availability and responsiveness of support options, including the possibility of contracting with external providers for critical applications Forking Cost While open-source software is often free to use, there can be costs associated with implementation, customization, support, and maintenance. A thorough cost-benefit analysis can help in understanding the total cost of ownership compared to proprietary alternatives Community Health and Longevity The health and sustainability of the open-source project's community are vital. A vibrant, active community suggests ongoing development and support, while a dwindling community may indicate a project at risk of becoming outdated or abandoned. Assessing the project's leadership, contribution activity, and roadmap can provide insights into its viability Cultural Fit The open-source philosophy emphasizes collaboration, transparency, and community contribution. Organizations should assess whether this culture aligns with their own values and practices, and be prepared to embrace these principles. Training and Skill Availability Adopting open-source software may require new skills or knowledge. Organizations need to consider the availability of skilled personnel, both within and outside the organization, and the potential need for training to ensure effective use and management of the OSS","title":"Why OSSVerse?"},{"location":"concepts/why-ossverse/#the-solution","text":"A Trustworthy Marketplace for Open-Source Assurance Service Providers (OASP) backed by a thriving open-source security community. Affordable and timely access to vendor neutral security services for organizations producing and consuming open-source. OASP led open source assurance service delivery would give full control over their open source components","title":"The solution"},{"location":"concepts/why-ossverse/#considerations-for-an-open-source-first-strategy","text":"Supply Chain & Security Risk While open-source software can be more secure than proprietary software due to its transparency and the potential for widespread review, vulnerabilities can still exist. Organizations should have a strategy for monitoring and applying security patches and updates promptly. Assessing the security practices and history of the OSS project is crucial Limited Control and Support Unlike proprietary software, which typically comes with vendor support, open-source projects rely on community support or third-party vendors for troubleshooting and maintenance. Organizations must consider the availability and responsiveness of support options, including the possibility of contracting with external providers for critical applications Forking Cost While open-source software is often free to use, there can be costs associated with implementation, customization, support, and maintenance. A thorough cost-benefit analysis can help in understanding the total cost of ownership compared to proprietary alternatives Community Health and Longevity The health and sustainability of the open-source project's community are vital. A vibrant, active community suggests ongoing development and support, while a dwindling community may indicate a project at risk of becoming outdated or abandoned. Assessing the project's leadership, contribution activity, and roadmap can provide insights into its viability Cultural Fit The open-source philosophy emphasizes collaboration, transparency, and community contribution. Organizations should assess whether this culture aligns with their own values and practices, and be prepared to embrace these principles. Training and Skill Availability Adopting open-source software may require new skills or knowledge. Organizations need to consider the availability of skilled personnel, both within and outside the organization, and the potential need for training to ensure effective use and management of the OSS","title":"Considerations for an Open Source first strategy"},{"location":"developer/","text":"User Guide \u00b6 Overview \u00b6 The pages in this section explore, with great detail, the architecture of the components that make up Leverage. Architecture Reference Architecture Overview","title":"User Guide"},{"location":"developer/#user-guide","text":"","title":"User Guide"},{"location":"developer/#overview","text":"The pages in this section explore, with great detail, the architecture of the components that make up Leverage. Architecture Reference Architecture Overview","title":"Overview"},{"location":"developer/architecture/configuration/","text":"Configuration \u00b6 Configuration Files \u00b6 Config files can be found under each config folders Global config file /config/common.tfvars contains global context TF variables that we inject to TF commands which are used by all sub-directories such as leverage terraform plan or leverage terraform apply and which cannot be stored in backend.tfvars due to TF. Account config files backend.tfvars contains TF variables that are mainly used to configure TF backend but since profile and region are defined there, we also use them to inject those values into other TF commands. account.tfvars contains TF variables that are specific to an AWS account. Global common-variables.tf file /config/common-variables.tfvars contains global context TF variables that we symlink to all terraform layers code e.g. shared/us-east-1/tools-vpn-server/common-variables.tf . build.env file By utilizing the build.env capability, you can easily change some default behaviors of the CLI. Read more in its dedicated \"Override defaults via build.env file\" section . Setting credentials for Terraform via AWS profiles \u00b6 File backend.tfvars will inject the profile name that TF will use to make changes on AWS. Such profile is usually one that relies on another profile to assume a role to get access to each corresponding account. Please read the credentials section to understand the alternatives supported by Leverage to authenticate with AWS. Read the following page leverage doc to understand how to set up a profile to assume a role","title":"Configuration"},{"location":"developer/architecture/configuration/#configuration","text":"","title":"Configuration"},{"location":"developer/architecture/configuration/#configuration-files","text":"Config files can be found under each config folders Global config file /config/common.tfvars contains global context TF variables that we inject to TF commands which are used by all sub-directories such as leverage terraform plan or leverage terraform apply and which cannot be stored in backend.tfvars due to TF. Account config files backend.tfvars contains TF variables that are mainly used to configure TF backend but since profile and region are defined there, we also use them to inject those values into other TF commands. account.tfvars contains TF variables that are specific to an AWS account. Global common-variables.tf file /config/common-variables.tfvars contains global context TF variables that we symlink to all terraform layers code e.g. shared/us-east-1/tools-vpn-server/common-variables.tf . build.env file By utilizing the build.env capability, you can easily change some default behaviors of the CLI. Read more in its dedicated \"Override defaults via build.env file\" section .","title":"Configuration Files"},{"location":"developer/architecture/configuration/#setting-credentials-for-terraform-via-aws-profiles","text":"File backend.tfvars will inject the profile name that TF will use to make changes on AWS. Such profile is usually one that relies on another profile to assume a role to get access to each corresponding account. Please read the credentials section to understand the alternatives supported by Leverage to authenticate with AWS. Read the following page leverage doc to understand how to set up a profile to assume a role","title":"Setting credentials for Terraform via AWS profiles"},{"location":"developer/architecture/dir-structure/","text":"Project Structure \u00b6 Files/Folders Organization \u00b6 The following block provides a brief explanation of the chosen files/folders layout, under every account ( management , shared , security , etc) folder you will see a service layer structure similar to the following: MyExample project file structure ... \u251c\u2500\u2500 \ud83d\udcc2 apps-devstg \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars | \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 backups | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-certificates | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 cdn-s3-frontend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 databases-aurora | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 databases-mysql | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 databases-pgsql | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks-demoapps | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-certs | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-firewall | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 storage | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 tools-cloud-nuke | \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2514\u2500\u2500 \ud83d\udcc2 security-keys \u251c\u2500\u2500 \ud83d\udcc2 apps-prd \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 backups | \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u251c\u2500\u2500 \ud83d\udcc2 cdn-s3-frontend | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks | \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc2 security-certs | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2514\u2500\u2500 \ud83d\udcc2 security-keys \u251c\u2500\u2500 \ud83d\udcc4 build.env \u251c\u2500\u2500 \ud83d\udcc4 build.py \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2514\u2500\u2500 \ud83d\udcc4 common.tfvars \u251c\u2500\u2500 \ud83d\udcc2 management \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-identities | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 cost-mgmt | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 organizations | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 sso \u2502 \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 backups | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-keys \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u2514\u2500\u2500 \ud83d\udcc2 security-monitoring \u251c\u2500\u2500 \ud83d\udcc2 network \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u2502 \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 network-firewall | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-keys | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 transit-gateway \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u251c\u2500\u2500 \ud83d\udcc2 network-firewall | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u251c\u2500\u2500 \ud83d\udcc2 security-keys | \u2514\u2500\u2500 \ud83d\udcc2 transit-gateway \u251c\u2500\u2500 \ud83d\udcc2 security \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u2502 \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 firewall-manager | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-keys | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 security-monitoring \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2514\u2500\u2500 \ud83d\udcc2 security-monitoring \u2514\u2500\u2500 \ud83d\udcc2 shared \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u251c\u2500\u2500 \ud83d\udcc2 global | \u251c\u2500\u2500 \ud83d\udcc2 base-dns | \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 backups | \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u251c\u2500\u2500 \ud83d\udcc2 container-registry | \u251c\u2500\u2500 \ud83d\udcc2 ec2-fleet | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks-demoapps | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks-prd | \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u251c\u2500\u2500 \ud83d\udcc2 storage | \u251c\u2500\u2500 \ud83d\udcc2 tools-cloud-scheduler-stop-start | \u251c\u2500\u2500 \ud83d\udcc2 tools-eskibana | \u251c\u2500\u2500 \ud83d\udcc2 tools-github-selfhosted-runners | \u251c\u2500\u2500 \ud83d\udcc2 tools-jenkins | \u251c\u2500\u2500 \ud83d\udcc2 tools-managedeskibana | \u251c\u2500\u2500 \ud83d\udcc2 tools-prometheus | \u251c\u2500\u2500 \ud83d\udcc2 tools-vault | \u251c\u2500\u2500 \ud83d\udcc2 tools-vpn-server | \u2514\u2500\u2500 \ud83d\udcc2 tools-webhooks \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 \u251c\u2500\u2500 \ud83d\udcc2 base-network \u251c\u2500\u2500 \ud83d\udcc2 container-registry \u251c\u2500\u2500 \ud83d\udcc2 security-compliance \u251c\u2500\u2500 \ud83d\udcc2 security-keys \u251c\u2500\u2500 \ud83d\udcc2 tools-eskibana \u2514\u2500\u2500 \ud83d\udcc2 tools-prometheus Configuration files are organized by environments (e.g. dev, stg, prd), and service type, which we call layers (identities, organizations, storage, etc) to keep any changes made to them separate. Within each of those layers folders you should find the Terraform files that are used to define all the resources that belong to such account environment and specific layer. Project file structure An extended project file structure could be found here While some other basic concepts and naming conventions in the context of Leverage like \"project\" and \"layer\" here Figure: AWS Organization multi-account architecture diagram. (Source: binbash Leverage, \"Leverage Reference Architecture components\", binbash Leverage Doc, accessed August 4th 2021). NOTE: As a convention folders with the -- suffix reflect that the resources are not currently created in AWS, basically they've been destroyed or not yet exist. Such layer separation is meant to avoid situations in which a single folder contains a lot of resources. That is important to avoid because at some point, running leverage terraform plan / apply starts taking too long and that becomes a problem. This organization also provides a layout that is easier to navigate and discover. You simply start with the accounts at the top level and then you get to explore the resource categories within each account.","title":"Project Structure"},{"location":"developer/architecture/dir-structure/#project-structure","text":"","title":"Project Structure"},{"location":"developer/architecture/dir-structure/#filesfolders-organization","text":"The following block provides a brief explanation of the chosen files/folders layout, under every account ( management , shared , security , etc) folder you will see a service layer structure similar to the following: MyExample project file structure ... \u251c\u2500\u2500 \ud83d\udcc2 apps-devstg \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars | \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 backups | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-certificates | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 cdn-s3-frontend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 databases-aurora | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 databases-mysql | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 databases-pgsql | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks-demoapps | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-certs | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-firewall | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 storage | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 tools-cloud-nuke | \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2514\u2500\u2500 \ud83d\udcc2 security-keys \u251c\u2500\u2500 \ud83d\udcc2 apps-prd \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 backups | \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u251c\u2500\u2500 \ud83d\udcc2 cdn-s3-frontend | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks | \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc2 security-certs | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2514\u2500\u2500 \ud83d\udcc2 security-keys \u251c\u2500\u2500 \ud83d\udcc4 build.env \u251c\u2500\u2500 \ud83d\udcc4 build.py \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2514\u2500\u2500 \ud83d\udcc4 common.tfvars \u251c\u2500\u2500 \ud83d\udcc2 management \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-identities | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 cost-mgmt | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 organizations | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 sso \u2502 \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 backups | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-keys \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u2514\u2500\u2500 \ud83d\udcc2 security-monitoring \u251c\u2500\u2500 \ud83d\udcc2 network \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u2502 \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 network-firewall | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-keys | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 transit-gateway \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u251c\u2500\u2500 \ud83d\udcc2 network-firewall | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u251c\u2500\u2500 \ud83d\udcc2 security-keys | \u2514\u2500\u2500 \ud83d\udcc2 transit-gateway \u251c\u2500\u2500 \ud83d\udcc2 security \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u2502 \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 firewall-manager | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 security-keys | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 security-monitoring \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 | \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u2514\u2500\u2500 \ud83d\udcc2 security-monitoring \u2514\u2500\u2500 \ud83d\udcc2 shared \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u251c\u2500\u2500 \ud83d\udcc2 global | \u251c\u2500\u2500 \ud83d\udcc2 base-dns | \u2514\u2500\u2500 \ud83d\udcc2 base-identities \u251c\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 backups | \u251c\u2500\u2500 \ud83d\udcc2 base-network | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u251c\u2500\u2500 \ud83d\udcc2 container-registry | \u251c\u2500\u2500 \ud83d\udcc2 ec2-fleet | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks-demoapps | \u251c\u2500\u2500 \ud83d\udcc2 k8s-eks-prd | \u251c\u2500\u2500 \ud83d\udcc2 notifications | \u251c\u2500\u2500 \ud83d\udcc2 security-audit | \u251c\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc2 security-compliance | \u251c\u2500\u2500 \ud83d\udcc2 storage | \u251c\u2500\u2500 \ud83d\udcc2 tools-cloud-scheduler-stop-start | \u251c\u2500\u2500 \ud83d\udcc2 tools-eskibana | \u251c\u2500\u2500 \ud83d\udcc2 tools-github-selfhosted-runners | \u251c\u2500\u2500 \ud83d\udcc2 tools-jenkins | \u251c\u2500\u2500 \ud83d\udcc2 tools-managedeskibana | \u251c\u2500\u2500 \ud83d\udcc2 tools-prometheus | \u251c\u2500\u2500 \ud83d\udcc2 tools-vault | \u251c\u2500\u2500 \ud83d\udcc2 tools-vpn-server | \u2514\u2500\u2500 \ud83d\udcc2 tools-webhooks \u2514\u2500\u2500 \ud83d\udcc2 us-east-2 \u251c\u2500\u2500 \ud83d\udcc2 base-network \u251c\u2500\u2500 \ud83d\udcc2 container-registry \u251c\u2500\u2500 \ud83d\udcc2 security-compliance \u251c\u2500\u2500 \ud83d\udcc2 security-keys \u251c\u2500\u2500 \ud83d\udcc2 tools-eskibana \u2514\u2500\u2500 \ud83d\udcc2 tools-prometheus Configuration files are organized by environments (e.g. dev, stg, prd), and service type, which we call layers (identities, organizations, storage, etc) to keep any changes made to them separate. Within each of those layers folders you should find the Terraform files that are used to define all the resources that belong to such account environment and specific layer. Project file structure An extended project file structure could be found here While some other basic concepts and naming conventions in the context of Leverage like \"project\" and \"layer\" here Figure: AWS Organization multi-account architecture diagram. (Source: binbash Leverage, \"Leverage Reference Architecture components\", binbash Leverage Doc, accessed August 4th 2021). NOTE: As a convention folders with the -- suffix reflect that the resources are not currently created in AWS, basically they've been destroyed or not yet exist. Such layer separation is meant to avoid situations in which a single folder contains a lot of resources. That is important to avoid because at some point, running leverage terraform plan / apply starts taking too long and that becomes a problem. This organization also provides a layout that is easier to navigate and discover. You simply start with the accounts at the top level and then you get to explore the resource categories within each account.","title":"Files/Folders Organization"},{"location":"developer/architecture/features/","text":"High Level Design \u00b6 Features \u00b6 Sl. Features Actors Remarks 1 Assurance provider onboarding Assurance Provider 2 OSS producer and consumer onboarding Consumer 3 Marketplace operator onboarding Marketplace Operator 4 Marketplace subscription Assurance Provider, Consumer 5 Add / modify / delete OSS projects into the projects of interest for assurance provider Assurance Provider While adding or modifying a OSS asset, the OASP should make sure that the attested assessment reports are made available 6 Add / modify / delete OSS models into the models of interest for assurance provider Assurance Provider While adding or modifying a OSS asset, the OASP should make sure that the attested assessment reports are made available 7 Search for an OSS project Consumer 8 Search for an OSS model Consumer 9 Place an order for a particular assurance service on an existing OSS artifact Consumer The assessment artifcats are made available at this time. Where do we bring in the payment? 10 Place a request for the assurance service on a new OSS artifact Consumer Should we split this into OSS model and projects? 11 Risk Assessment report 12 Bug bounty program 13 Remediate a particular bug 14 Bid for the new OSS project or model onboarding request Assurance Provider OSSVerse User Interface Views \u00b6 Public View \u00b6 OSSVerse Homepage (ossverse.com) Logo placeholder Top menu navigation Menus : Platform, Solutions, Resources, Company, Sign In, Get Started, Book a demo Marketing banner placeholder. Also has buttons for Get Started and Explore Marketplace Featured Offerings section Featured Assurance Provider section with buttons to navigate to each of their offerings Bottom banner with links to website navigation Copyright text Explore marketplace Navigate to the search page from the home page. Shows the complete list of OSS Software / Model list page with search and filter Each item to be represented as tiles. Each tile should have the OSS artifact name, artifact type indicator (ML model / Project), first few lines of the description, OSSVerse recommended OASP name providing this offering. The user click on a tile will redirects to Login or Sign Up as a business user Producer / Business \u00b6 OSS artifact details page Opens up post login and when the user click on an OSS Project / Model tile from the explore marketplace view. The details page should contain the below key elements. Name of the artifact Detailed description Price of assessment service (This includes 1 OASP doing assessment and 2 OASPs doing a validation). The business user is going to get an assessment report which is attested from OASP1 and validated by 2 other OASPs as end delivery Default selection of marketplace recommend OASP for assessment and 2 OASPs for validation. (User will have the option to change the OASPs. The pricing will change accordingly) Button to place an order. Once order is placed, the page redirects to the order tracking screen. Order details and tracking page User intuitive representation of the progress of a single order from start to delivery. Show the transaction id, OASPs delivering the service, order details, payment information, billing information, expected time of delivery Provision to edit billing information Provision to cancel the order if the OASP is yet to accept it Link to download the deliverables. Attested and certified assessment report can be downloaded if the delivery is complete. The same page can be used in the OASP view to see the order details before accepting or rejecting it. My orders Shows the complete list of orders placed by this organization. On click of an item, page redirects to \u201cOrder details and tracking page\u201d. Open Source Assurance Provider View \u00b6 Dashboard Gives a holistic view of the assurance provider business. Count of assigned orders for assessment. It\u2019s a hyperlink to the pending assessment order list. Count of assigned orders for validation. It\u2019s a hyperlink to the pending validation order list. Count of open challenges OASP can bid for. It\u2019s a hyperlink to the open challenges page. Statistics section about the overall business, revenue etc Count of Open Source Projects this OASP is offering services. It\u2019s a hyperlink to the OSS projects list page Count of Open Source Models this OASP is offering services. It\u2019s a hyperlink to the OSS models list page Assessment orders page Lists all assessment orders assigned to this OASP. Both OSS projects and models assessments are listed here. This should have search and filter feature. The list can be represented as a table On click of a row redirects the page to \u201cOrder details and tracking page\u201d that is similar to the \u201cBusiness/Producer\u201d user view. Risk Assessment Order details and tracking \u201cOrder details and tracking page\u201d that is similar to the \u201cBusiness/Producer\u201d user view Accept and reject button. In case of rejection, there will be a reject reason text box appears before submit. Provision to mark the order as complete. User should be able to upload the deliverables at this stage. Validation orders page Lists all validation orders assigned to this OASP The list can be represented as a table On click of a row redirects the page to \u201cOrder details and tracking page\u201d that is similar to the \u201cBusiness/Producer\u201d user view. Only addition is the Accept and reject button. In case of rejection, there will be a reject reason text box appears before submit. Open challenges page Lists all the open on-demand challenges published in the marketplace The list can be represented as a table On click of a row redirects the page to \u201cChallenge details\u201d page Challenge details page Challenge details should be there. This is similar to TopCoder challenge details page. Provision to bid for the challenge with pricing, delivery timelines, deliverables etc Open source projects listing page Table representation of all projects this OASP supports. Search and filter should be there On click of a row redirects the page to OSS project details page. Button to list new OSS project to marketplace Open source project details page Project name Description GitHub URL Modify button click makes this page editable Modify offerings checkboxes Risk Assessment Third-party validation of risk assessment Vulnerability remediation TAVOSS version (TAVOSS: Trusted and Verified Open Source Software) Feature enhancement List new OSS project to marketplace Project name Description GitHub URL Checkboxes to select the services that can be offered on the model Risk Assessment Third-party validation of risk assessment Vulnerability remediation TAVOSS version (TAVOSS: Trusted and Verified Open Source Software) Feature enhancement Open source models listing page Table representation of all ML models this OASP supports. Search and filter should be there On click of a row redirects the page to ml model details page. Button to list new ML model to marketplace ML model details page Model name Description GitHub / HuggingFace URL Modify button click makes the page editable Modify offerings checkboxes Risk Assessment Third-party validation of risk assessment List new ML model to marketplace Model name Description GitHub / HuggingFace URL Checkboxes to select the services that can be offered on the model Risk Assessment Third-party validation of risk assessment Marketplace Operator \u00b6 Dashboard This is a typical business analytics dashboard Pending order count across OASPs Count of open challenges and number of bids Completed order count during a time window Overall revenue during a time window Count of active OASPs Geography wise automotive company view who are placing orders Marketplace Configurations User should be able to modify configurations for the below Assessment pricing and validation percentages for OASP1, OASP2 and OASP3 Configuration to enable or disable OASP recommendations for the assessment of OSS artifacts Business, OASP and OSSVerse operator user management","title":"Features"},{"location":"developer/architecture/features/#high-level-design","text":"","title":"High Level Design"},{"location":"developer/architecture/features/#features","text":"Sl. Features Actors Remarks 1 Assurance provider onboarding Assurance Provider 2 OSS producer and consumer onboarding Consumer 3 Marketplace operator onboarding Marketplace Operator 4 Marketplace subscription Assurance Provider, Consumer 5 Add / modify / delete OSS projects into the projects of interest for assurance provider Assurance Provider While adding or modifying a OSS asset, the OASP should make sure that the attested assessment reports are made available 6 Add / modify / delete OSS models into the models of interest for assurance provider Assurance Provider While adding or modifying a OSS asset, the OASP should make sure that the attested assessment reports are made available 7 Search for an OSS project Consumer 8 Search for an OSS model Consumer 9 Place an order for a particular assurance service on an existing OSS artifact Consumer The assessment artifcats are made available at this time. Where do we bring in the payment? 10 Place a request for the assurance service on a new OSS artifact Consumer Should we split this into OSS model and projects? 11 Risk Assessment report 12 Bug bounty program 13 Remediate a particular bug 14 Bid for the new OSS project or model onboarding request Assurance Provider","title":"Features"},{"location":"developer/architecture/features/#ossverse-user-interface-views","text":"","title":"OSSVerse User Interface Views"},{"location":"developer/architecture/features/#public-view","text":"OSSVerse Homepage (ossverse.com) Logo placeholder Top menu navigation Menus : Platform, Solutions, Resources, Company, Sign In, Get Started, Book a demo Marketing banner placeholder. Also has buttons for Get Started and Explore Marketplace Featured Offerings section Featured Assurance Provider section with buttons to navigate to each of their offerings Bottom banner with links to website navigation Copyright text Explore marketplace Navigate to the search page from the home page. Shows the complete list of OSS Software / Model list page with search and filter Each item to be represented as tiles. Each tile should have the OSS artifact name, artifact type indicator (ML model / Project), first few lines of the description, OSSVerse recommended OASP name providing this offering. The user click on a tile will redirects to Login or Sign Up as a business user","title":"Public View"},{"location":"developer/architecture/features/#producer-business","text":"OSS artifact details page Opens up post login and when the user click on an OSS Project / Model tile from the explore marketplace view. The details page should contain the below key elements. Name of the artifact Detailed description Price of assessment service (This includes 1 OASP doing assessment and 2 OASPs doing a validation). The business user is going to get an assessment report which is attested from OASP1 and validated by 2 other OASPs as end delivery Default selection of marketplace recommend OASP for assessment and 2 OASPs for validation. (User will have the option to change the OASPs. The pricing will change accordingly) Button to place an order. Once order is placed, the page redirects to the order tracking screen. Order details and tracking page User intuitive representation of the progress of a single order from start to delivery. Show the transaction id, OASPs delivering the service, order details, payment information, billing information, expected time of delivery Provision to edit billing information Provision to cancel the order if the OASP is yet to accept it Link to download the deliverables. Attested and certified assessment report can be downloaded if the delivery is complete. The same page can be used in the OASP view to see the order details before accepting or rejecting it. My orders Shows the complete list of orders placed by this organization. On click of an item, page redirects to \u201cOrder details and tracking page\u201d.","title":"Producer / Business"},{"location":"developer/architecture/features/#open-source-assurance-provider-view","text":"Dashboard Gives a holistic view of the assurance provider business. Count of assigned orders for assessment. It\u2019s a hyperlink to the pending assessment order list. Count of assigned orders for validation. It\u2019s a hyperlink to the pending validation order list. Count of open challenges OASP can bid for. It\u2019s a hyperlink to the open challenges page. Statistics section about the overall business, revenue etc Count of Open Source Projects this OASP is offering services. It\u2019s a hyperlink to the OSS projects list page Count of Open Source Models this OASP is offering services. It\u2019s a hyperlink to the OSS models list page Assessment orders page Lists all assessment orders assigned to this OASP. Both OSS projects and models assessments are listed here. This should have search and filter feature. The list can be represented as a table On click of a row redirects the page to \u201cOrder details and tracking page\u201d that is similar to the \u201cBusiness/Producer\u201d user view. Risk Assessment Order details and tracking \u201cOrder details and tracking page\u201d that is similar to the \u201cBusiness/Producer\u201d user view Accept and reject button. In case of rejection, there will be a reject reason text box appears before submit. Provision to mark the order as complete. User should be able to upload the deliverables at this stage. Validation orders page Lists all validation orders assigned to this OASP The list can be represented as a table On click of a row redirects the page to \u201cOrder details and tracking page\u201d that is similar to the \u201cBusiness/Producer\u201d user view. Only addition is the Accept and reject button. In case of rejection, there will be a reject reason text box appears before submit. Open challenges page Lists all the open on-demand challenges published in the marketplace The list can be represented as a table On click of a row redirects the page to \u201cChallenge details\u201d page Challenge details page Challenge details should be there. This is similar to TopCoder challenge details page. Provision to bid for the challenge with pricing, delivery timelines, deliverables etc Open source projects listing page Table representation of all projects this OASP supports. Search and filter should be there On click of a row redirects the page to OSS project details page. Button to list new OSS project to marketplace Open source project details page Project name Description GitHub URL Modify button click makes this page editable Modify offerings checkboxes Risk Assessment Third-party validation of risk assessment Vulnerability remediation TAVOSS version (TAVOSS: Trusted and Verified Open Source Software) Feature enhancement List new OSS project to marketplace Project name Description GitHub URL Checkboxes to select the services that can be offered on the model Risk Assessment Third-party validation of risk assessment Vulnerability remediation TAVOSS version (TAVOSS: Trusted and Verified Open Source Software) Feature enhancement Open source models listing page Table representation of all ML models this OASP supports. Search and filter should be there On click of a row redirects the page to ml model details page. Button to list new ML model to marketplace ML model details page Model name Description GitHub / HuggingFace URL Modify button click makes the page editable Modify offerings checkboxes Risk Assessment Third-party validation of risk assessment List new ML model to marketplace Model name Description GitHub / HuggingFace URL Checkboxes to select the services that can be offered on the model Risk Assessment Third-party validation of risk assessment","title":"Open Source Assurance Provider View"},{"location":"developer/architecture/features/#marketplace-operator","text":"Dashboard This is a typical business analytics dashboard Pending order count across OASPs Count of open challenges and number of bids Completed order count during a time window Overall revenue during a time window Count of active OASPs Geography wise automotive company view who are placing orders Marketplace Configurations User should be able to modify configurations for the below Assessment pricing and validation percentages for OASP1, OASP2 and OASP3 Configuration to enable or disable OASP recommendations for the assessment of OSS artifacts Business, OASP and OSSVerse operator user management","title":"Marketplace Operator"},{"location":"developer/architecture/overview/","text":"AWS Reference Architecture \u00b6 Overview \u00b6 The AWS Reference Architecture was created on a set of opinionated definitions and conventions on: how to organize files/folders , where to store configuration files , how to handle credentials , how to set up and manage state , which commands and workflows to run in order to perform different tasks, and more. Key Concept Although the Reference Architecture for AWS was initially designed to be compatible with web, mobile and microservices application stacks, it can also accommodate other types of workloads such as machine learning, blockchain, media, and more. It was designed with modularity in mind. A multi-accounts approach is leveraged in order to improve security isolation and resources separation. Furthermore each account infrastructure is divided in smaller units that we call layers . Each layer contains all the required resources and definitions for a specific service or feature to function. Key Concept The design is strongly based on the AWS Well Architected Framework . Each individual configuration of the Reference Architecture is referred to as a project . A Leverage project is comprised of all the relevant accounts and layers. Core Strengths \u00b6 Faster updates (new features and bug fixes). Better code quality and modules maturity (proven and tested). Supported by binbash, and public modules even by 1000's of top talented Open Source community contributors. Increase development cost savings. Clients keep full rights to all commercial, modification, distribution, and private use of the code (No Lock-In) through forks inside their own projects' repositories (open-source and commercially reusable via license MIT and Apache 2.0 . A More Visual Example \u00b6 The following diagram shows the type of AWS multi-account setup you can achieve by using this Reference Architecture: Figure: AWS Organization multi-account reference architecture diagram. (Source: binbash Leverage, \"Leverage Reference Architecture components\", binbash Leverage Doc, accessed August 4th 2021). Read more Don't get locked up into avoiding lock-in AWS Managed Services","title":"AWS Reference Architecture"},{"location":"developer/architecture/overview/#aws-reference-architecture","text":"","title":"AWS Reference Architecture"},{"location":"developer/architecture/overview/#overview","text":"The AWS Reference Architecture was created on a set of opinionated definitions and conventions on: how to organize files/folders , where to store configuration files , how to handle credentials , how to set up and manage state , which commands and workflows to run in order to perform different tasks, and more. Key Concept Although the Reference Architecture for AWS was initially designed to be compatible with web, mobile and microservices application stacks, it can also accommodate other types of workloads such as machine learning, blockchain, media, and more. It was designed with modularity in mind. A multi-accounts approach is leveraged in order to improve security isolation and resources separation. Furthermore each account infrastructure is divided in smaller units that we call layers . Each layer contains all the required resources and definitions for a specific service or feature to function. Key Concept The design is strongly based on the AWS Well Architected Framework . Each individual configuration of the Reference Architecture is referred to as a project . A Leverage project is comprised of all the relevant accounts and layers.","title":"Overview"},{"location":"developer/architecture/overview/#core-strengths","text":"Faster updates (new features and bug fixes). Better code quality and modules maturity (proven and tested). Supported by binbash, and public modules even by 1000's of top talented Open Source community contributors. Increase development cost savings. Clients keep full rights to all commercial, modification, distribution, and private use of the code (No Lock-In) through forks inside their own projects' repositories (open-source and commercially reusable via license MIT and Apache 2.0 .","title":"Core Strengths"},{"location":"developer/architecture/overview/#a-more-visual-example","text":"The following diagram shows the type of AWS multi-account setup you can achieve by using this Reference Architecture: Figure: AWS Organization multi-account reference architecture diagram. (Source: binbash Leverage, \"Leverage Reference Architecture components\", binbash Leverage Doc, accessed August 4th 2021). Read more Don't get locked up into avoiding lock-in AWS Managed Services","title":"A More Visual Example"},{"location":"developer/architecture/target-architecture/","text":"OSSVerse \u00b6 1. Business Context \u00b6 OSSVerse is an open source Marketplace. It is conceptualized as an eco system project comprising of multiple platforms. It is an adaptation of ONDC and Beckn protocol for open source software service delivery . OSSVerse leverages BeSecure(BeS) for delivering open source software security assurance services . OSSVerse aims to establish an open network of OASPs for businesses that will offer trustworthy and reliable open source software assurance services. 1.2 Business Architecture \u00b6 1.2.1 Marketplace Services 1.2.1.1 Onboarding services : ### Account Creation: OASP Creating an OSSVerse Account & OASP profile for OASP The Account creation service will register OASP with OSSVerse Registry and have APIs to update the OASP profile. An OASP profile will have informed related to payments and BAP Seller App end points (e.g instance of BAP compatible adaptation of a BeSLab ) Business Creating an OSSVerse Account for a Business 1.2.1.2 Search: Business a) A Business who is a consumer of Open source Searching for OASP , open source component and open source support services like 1. Open source assessment & Attestation services (Subscription based) 2. Open source remediation Services (On Demand) 3. Open source feature enhancement Services (On Demand) b ) A Business who is a Producer or Distributor of Open source Searching for an OASP for an identified open source component for certification ( On Demand ) OASP Services \u00b6 1. Assurance & Assessment Service: Open-source software undergoes rigorous evaluation for security, compliance, and quality, culminating in comprehensive reports and proof of attestation to ensure trustworthiness. 1.1 Assurance Levels & Services Based on the depth and scope of the assessment required, different assurance levels are offered, each encompassing specific checks and activities: Assurance Level Description Key Emphasis/Requirement Example Checks/Activities 0 No Assurance No formal assessment - Project information gathering 1 Basic Assurance Basic automated security checks, with proof of attestation - SAST (static code analysis) 2 Standard Assurance Everything in Basic (including proof of attestation) + Dependency & license compliance checks - Dependency vulnerability checks - License compliance audit - SAST 3 Comprehensive Assurance Everything in Standard (including proof of attestation) + Dynamic and manual security review - Dynamic application security testing (if applicable) - Expert code review (optional) - Fuzzing 4 Domain-Specific Assurance Everything in Comprehensive (including proof of attestation) + Industry-specific safety and security checks - Adherence to e.g. ISO 26262 and other domain specific standards. - Hardware-in-the-loop (HIL) testing. - Fuzzing and symbolic execution. 1.2 Open-Source Software (OSS) Assurance Service Checklist This checklist outlines the key steps involved in assessing the security, quality, and compliance of open-source software, especially in the automotive domain. 1.2.1 Standard Checks Assurance Check Category Subcategory (Intent) BeS Env Name Example Playbook Name Open-Source Tools SAST (Static Analysis) Security Vulnerability Scan besman-code_analysis-env.sh besman-sast-sonarqube-playbook.sh , besman-sast-codeql-playbook.sh SonarQube (Community Edition), PVS-Studio, CodeQL, Semgrep, Bandit, ESLint, Pylint DAST (Dynamic Analysis) Security Vulnerability Scan besman-dynamic_analysis-env.sh besman-dast-zap-playbook.sh OWASP ZAP, Arachni, Nikto, w3af, Burp Suite Dependency Analysis Dependency Vulnerability Check besman-dependency_analysis-env.sh besman-dependency_check-playbook.sh OWASP Dependency-Check, Retire.js, Synk, WhiteSource Licensing & Compliance License Compliance Audit besman-legal_compliance-env.sh besman-license_compliance-fossology-playbook.sh FOSSology, LicenseFinder, ScanCode, Toolkit Code Quality Analysis Code Quality Assessment besman-code_analysis-env.sh besman-code_quality-sonarqube-playbook.sh SonarQube (Community Edition), PMD, Checkstyle, FindBugs 1.2.2 Automotive-Specific Checks Assurance Check Category Subcategory (Intent) BeS Env Name Example Playbook Name Open-Source Tools Safety Assurance Functional Safety Assessment besman-safety_critical-env.sh besman-safety_assurance-latte-playbook.sh Frama-C, CBMC, SPIN, LATTE Security for Autonomous Driving Adversarial ML & Security Assessment besman-security_assessment_env.sh besman-autonomous_driving_security-carla_art-playbook.sh CARLA Adversarial Robustness Toolbox (ART), OpenSCENARIO Real-Time Performance Real-Time Performance Analysis besman-safety_critical-env.sh besman-real_time_performance-cyclictest-playbook.sh cyclictest Data Privacy & Security Data Privacy Assessment besman-security_assessment_env.sh besman-data_privacy-openssl-playbook.sh OpenSSL, GPG Interoperability & Compatibility Protocol Conformance Testing besman-network_simulation_env.sh besman-protocol_conformance-cantools-playbook.sh Cantools, OpenXC Hardware Integration HIL Testing besman-hardware_integration-env.sh besman-hardware_integration-renode-playbook.sh Renode, QEMU Additional Checks Deep Vulnerability Scan besman-symbolic_execution_fuzzing-env.sh besman-fuzzing-afl-playbook.sh KLEE (Symbolic Execution), AFL (Fuzzing), Flawfinder, cppcheck 1.3 Proof of Attestation Assurance Check Category BeS Env Name Example Playbook Name Open-Source Tools Attestation besman-secure_artifact_signing-env.sh besman-artifact_signing-playbook.sh Sigstore, in-toto framework (OpenSSF), The Update Framework (TUF) Note: Playbook names are provided in a generic way to emphasize their reusability across different projects. The actual playbook implementation would need to handle project-specific configurations and inputs. Validation & Verification Service : (validated & verify if attestation has been done in line with the acceptable OSSVerse OASP Assessment Standard) Check BeS Environments (Exists/Linting) Check Playbooks w.rt each Check part of Assessment claimed. OSAR(Open source assessment/attestation Report) updated with Proof of Verfication & Validation as evidence to be shared (This could be inline with an accept standard like Attestation standard like CycloneDX Attestation standard) Remediation Service : Pentesting Service : Feature Addition : TAVOSS Version & Certification Service : (Certification with or without/Distribution?/hosting?) Business Support Services Technical Architecture \u00b6 Communication between BeSlab & OSSVerse \u00b6 OSSVerse as A BAP/BG/Open Network Registry \u00b6 OASP as BPP \u00b6 Extending BeSLab as a BPP BeS : Establish provenance & presentation of Proofs \u00b6 BeS : OSAR \u00b6","title":"Overview"},{"location":"developer/architecture/target-architecture/#ossverse","text":"","title":"OSSVerse"},{"location":"developer/architecture/target-architecture/#1-business-context","text":"OSSVerse is an open source Marketplace. It is conceptualized as an eco system project comprising of multiple platforms. It is an adaptation of ONDC and Beckn protocol for open source software service delivery . OSSVerse leverages BeSecure(BeS) for delivering open source software security assurance services . OSSVerse aims to establish an open network of OASPs for businesses that will offer trustworthy and reliable open source software assurance services.","title":"1. Business Context"},{"location":"developer/architecture/target-architecture/#12-business-architecture","text":"1.2.1 Marketplace Services 1.2.1.1 Onboarding services : ### Account Creation: OASP Creating an OSSVerse Account & OASP profile for OASP The Account creation service will register OASP with OSSVerse Registry and have APIs to update the OASP profile. An OASP profile will have informed related to payments and BAP Seller App end points (e.g instance of BAP compatible adaptation of a BeSLab ) Business Creating an OSSVerse Account for a Business 1.2.1.2 Search: Business a) A Business who is a consumer of Open source Searching for OASP , open source component and open source support services like 1. Open source assessment & Attestation services (Subscription based) 2. Open source remediation Services (On Demand) 3. Open source feature enhancement Services (On Demand) b ) A Business who is a Producer or Distributor of Open source Searching for an OASP for an identified open source component for certification ( On Demand )","title":"1.2 Business Architecture"},{"location":"developer/architecture/target-architecture/#oasp-services","text":"1. Assurance & Assessment Service: Open-source software undergoes rigorous evaluation for security, compliance, and quality, culminating in comprehensive reports and proof of attestation to ensure trustworthiness. 1.1 Assurance Levels & Services Based on the depth and scope of the assessment required, different assurance levels are offered, each encompassing specific checks and activities: Assurance Level Description Key Emphasis/Requirement Example Checks/Activities 0 No Assurance No formal assessment - Project information gathering 1 Basic Assurance Basic automated security checks, with proof of attestation - SAST (static code analysis) 2 Standard Assurance Everything in Basic (including proof of attestation) + Dependency & license compliance checks - Dependency vulnerability checks - License compliance audit - SAST 3 Comprehensive Assurance Everything in Standard (including proof of attestation) + Dynamic and manual security review - Dynamic application security testing (if applicable) - Expert code review (optional) - Fuzzing 4 Domain-Specific Assurance Everything in Comprehensive (including proof of attestation) + Industry-specific safety and security checks - Adherence to e.g. ISO 26262 and other domain specific standards. - Hardware-in-the-loop (HIL) testing. - Fuzzing and symbolic execution. 1.2 Open-Source Software (OSS) Assurance Service Checklist This checklist outlines the key steps involved in assessing the security, quality, and compliance of open-source software, especially in the automotive domain. 1.2.1 Standard Checks Assurance Check Category Subcategory (Intent) BeS Env Name Example Playbook Name Open-Source Tools SAST (Static Analysis) Security Vulnerability Scan besman-code_analysis-env.sh besman-sast-sonarqube-playbook.sh , besman-sast-codeql-playbook.sh SonarQube (Community Edition), PVS-Studio, CodeQL, Semgrep, Bandit, ESLint, Pylint DAST (Dynamic Analysis) Security Vulnerability Scan besman-dynamic_analysis-env.sh besman-dast-zap-playbook.sh OWASP ZAP, Arachni, Nikto, w3af, Burp Suite Dependency Analysis Dependency Vulnerability Check besman-dependency_analysis-env.sh besman-dependency_check-playbook.sh OWASP Dependency-Check, Retire.js, Synk, WhiteSource Licensing & Compliance License Compliance Audit besman-legal_compliance-env.sh besman-license_compliance-fossology-playbook.sh FOSSology, LicenseFinder, ScanCode, Toolkit Code Quality Analysis Code Quality Assessment besman-code_analysis-env.sh besman-code_quality-sonarqube-playbook.sh SonarQube (Community Edition), PMD, Checkstyle, FindBugs 1.2.2 Automotive-Specific Checks Assurance Check Category Subcategory (Intent) BeS Env Name Example Playbook Name Open-Source Tools Safety Assurance Functional Safety Assessment besman-safety_critical-env.sh besman-safety_assurance-latte-playbook.sh Frama-C, CBMC, SPIN, LATTE Security for Autonomous Driving Adversarial ML & Security Assessment besman-security_assessment_env.sh besman-autonomous_driving_security-carla_art-playbook.sh CARLA Adversarial Robustness Toolbox (ART), OpenSCENARIO Real-Time Performance Real-Time Performance Analysis besman-safety_critical-env.sh besman-real_time_performance-cyclictest-playbook.sh cyclictest Data Privacy & Security Data Privacy Assessment besman-security_assessment_env.sh besman-data_privacy-openssl-playbook.sh OpenSSL, GPG Interoperability & Compatibility Protocol Conformance Testing besman-network_simulation_env.sh besman-protocol_conformance-cantools-playbook.sh Cantools, OpenXC Hardware Integration HIL Testing besman-hardware_integration-env.sh besman-hardware_integration-renode-playbook.sh Renode, QEMU Additional Checks Deep Vulnerability Scan besman-symbolic_execution_fuzzing-env.sh besman-fuzzing-afl-playbook.sh KLEE (Symbolic Execution), AFL (Fuzzing), Flawfinder, cppcheck 1.3 Proof of Attestation Assurance Check Category BeS Env Name Example Playbook Name Open-Source Tools Attestation besman-secure_artifact_signing-env.sh besman-artifact_signing-playbook.sh Sigstore, in-toto framework (OpenSSF), The Update Framework (TUF) Note: Playbook names are provided in a generic way to emphasize their reusability across different projects. The actual playbook implementation would need to handle project-specific configurations and inputs. Validation & Verification Service : (validated & verify if attestation has been done in line with the acceptable OSSVerse OASP Assessment Standard) Check BeS Environments (Exists/Linting) Check Playbooks w.rt each Check part of Assessment claimed. OSAR(Open source assessment/attestation Report) updated with Proof of Verfication & Validation as evidence to be shared (This could be inline with an accept standard like Attestation standard like CycloneDX Attestation standard) Remediation Service : Pentesting Service : Feature Addition : TAVOSS Version & Certification Service : (Certification with or without/Distribution?/hosting?) Business Support Services","title":"OASP Services"},{"location":"developer/architecture/target-architecture/#technical-architecture","text":"","title":"Technical Architecture"},{"location":"developer/architecture/target-architecture/#communication-between-beslab-ossverse","text":"","title":"Communication between BeSlab &amp; OSSVerse"},{"location":"developer/architecture/target-architecture/#ossverse-as-a-bapbgopen-network-registry","text":"","title":"OSSVerse as A BAP/BG/Open Network Registry"},{"location":"developer/architecture/target-architecture/#oasp-as-bpp","text":"Extending BeSLab as a BPP","title":"OASP as BPP"},{"location":"developer/architecture/target-architecture/#bes-establish-provenance-presentation-of-proofs","text":"","title":"BeS : Establish provenance &amp; presentation of Proofs"},{"location":"developer/architecture/target-architecture/#bes-osar","text":"","title":"BeS : OSAR"},{"location":"developer/ossverse-in-a-box/credentials/","text":"Credentials \u00b6 Overview \u00b6 Currently the following two methods are supported: AWS IAM: this is essentially using on-disk, permanent programmatic credentials that are tied to a given IAM User. This method can optionally support MFA which is highly recommended since using permanent credentials is discouraged, so at least with MFA you can counter-balance that. Keep reading... AWS IAM Identity Center (formerly known as AWS SSO): this one is more recent and it's the method recommeded by AWS since it uses roles (managed by AWS) which in turn enforce the usage of temporary credentials. Keep reading... Next Steps \u00b6 If you are planning to choose SSO (highly recommended), check out this section . If you are instead interested in using IAM + MFA, refer to this other section instead.","title":"Credentials"},{"location":"developer/ossverse-in-a-box/credentials/#credentials","text":"","title":"Credentials"},{"location":"developer/ossverse-in-a-box/credentials/#overview","text":"Currently the following two methods are supported: AWS IAM: this is essentially using on-disk, permanent programmatic credentials that are tied to a given IAM User. This method can optionally support MFA which is highly recommended since using permanent credentials is discouraged, so at least with MFA you can counter-balance that. Keep reading... AWS IAM Identity Center (formerly known as AWS SSO): this one is more recent and it's the method recommeded by AWS since it uses roles (managed by AWS) which in turn enforce the usage of temporary credentials. Keep reading...","title":"Overview"},{"location":"developer/ossverse-in-a-box/credentials/#next-steps","text":"If you are planning to choose SSO (highly recommended), check out this section . If you are instead interested in using IAM + MFA, refer to this other section instead.","title":"Next Steps"},{"location":"developer/ossverse-in-a-box/references/","text":"References \u00b6 The following are official AWS documentations, blog posts and whitepapers we have considered while building our Reference Solutions Architecture: CloudTrail for AWS Organizations Reserved Instances - Multi Account AWS Multiple Account Security Strategy AWS Multiple Account Billing Strategy AWS Secure Account Setup Authentication and Access Control for AWS Organizations AWS Regions VPC Peering Route53 DNS VPC Associations AWS Well Architected Framework AWS Tagging strategies Inviting an AWS Account to Join Your Organization","title":"References"},{"location":"developer/ossverse-in-a-box/references/#references","text":"The following are official AWS documentations, blog posts and whitepapers we have considered while building our Reference Solutions Architecture: CloudTrail for AWS Organizations Reserved Instances - Multi Account AWS Multiple Account Security Strategy AWS Multiple Account Billing Strategy AWS Secure Account Setup Authentication and Access Control for AWS Organizations AWS Regions VPC Peering Route53 DNS VPC Associations AWS Well Architected Framework AWS Tagging strategies Inviting an AWS Account to Join Your Organization","title":"References"},{"location":"developer/ossverse-in-a-box/tf-state/","text":"Terraform - S3 & DynamoDB for Remote State Storage & Locking \u00b6 Overview \u00b6 Use this terraform configuration files to create the S3 bucket & DynamoDB table needed to use Terraform Remote State Storage & Locking. What is the Terraform Remote State? Read the official definition by Hashicorp. Figure: Terraform remote state store & locking necessary AWS S3 bucket and DynamoDB table components. (Source: binbash Leverage, \"Terraform Module: Terraform Backend\" , Terraform modules registry, accessed December 3rd 2020). Prerequisites \u00b6 Terraform repo structure + state backend initialization Ensure you have Leverage CLI installed in your system Refer to Configuration Pre-requisites to understand how to set up the configuration files required for this layer. Where you must build your Terraform Reference Architecture account structure Leveraged by the Infrastructure as Code (IaC) Library through the terraform-aws-tfstate-backend module /management/base-tf-backend /security/base-tf-backend /shared/base-tf-backend /network/base-tf-backend /apps-devstg/base-tf-backend /apps-prd/base-tf-backend Set up \u00b6 Steps to initialize your tf-backend At the corresponding account dir, eg: /shared/base-tf-backend then, Run leverage terraform init --skip-validation Run leverage terraform plan , review the output to understand the expected changes Run leverage terraform apply , review the output once more and type yes if you are okay with that This should create a terraform.tfstate file in this directory but we don't want to push that to the repository so let's push the state to the backend we just created Open config.tf and uncomment the following lines: # backend \"s3\" { # key = \"shared/tf-backend/terraform.tfstate\" # } Run leverage terraform init and type yes when Terraform asks if you want to import the state to the S3 backend Done. You can remove terraform.tfstate now (and also terraform.tfstate.backup if available) Expected workflow after set up \u00b6 This video is outdated! Terraform Remote State \u00b6 In the base-tf-backend folder you should find the definition of the infrastructure that needs to be deployed before you can get to work with anything else. IMPORTANT: THIS IS ONLY NEEDED IF THE BACKEND WAS NOT CREATED YET. IF THE BACKEND ALREADY EXISTS YOU JUST USE IT.","title":"Terraform - S3 &amp; DynamoDB for Remote State Storage &amp; Locking"},{"location":"developer/ossverse-in-a-box/tf-state/#terraform-s3-dynamodb-for-remote-state-storage-locking","text":"","title":"Terraform - S3 &amp; DynamoDB for Remote State Storage &amp; Locking"},{"location":"developer/ossverse-in-a-box/tf-state/#overview","text":"Use this terraform configuration files to create the S3 bucket & DynamoDB table needed to use Terraform Remote State Storage & Locking. What is the Terraform Remote State? Read the official definition by Hashicorp. Figure: Terraform remote state store & locking necessary AWS S3 bucket and DynamoDB table components. (Source: binbash Leverage, \"Terraform Module: Terraform Backend\" , Terraform modules registry, accessed December 3rd 2020).","title":"Overview"},{"location":"developer/ossverse-in-a-box/tf-state/#prerequisites","text":"Terraform repo structure + state backend initialization Ensure you have Leverage CLI installed in your system Refer to Configuration Pre-requisites to understand how to set up the configuration files required for this layer. Where you must build your Terraform Reference Architecture account structure Leveraged by the Infrastructure as Code (IaC) Library through the terraform-aws-tfstate-backend module /management/base-tf-backend /security/base-tf-backend /shared/base-tf-backend /network/base-tf-backend /apps-devstg/base-tf-backend /apps-prd/base-tf-backend","title":"Prerequisites"},{"location":"developer/ossverse-in-a-box/tf-state/#set-up","text":"Steps to initialize your tf-backend At the corresponding account dir, eg: /shared/base-tf-backend then, Run leverage terraform init --skip-validation Run leverage terraform plan , review the output to understand the expected changes Run leverage terraform apply , review the output once more and type yes if you are okay with that This should create a terraform.tfstate file in this directory but we don't want to push that to the repository so let's push the state to the backend we just created Open config.tf and uncomment the following lines: # backend \"s3\" { # key = \"shared/tf-backend/terraform.tfstate\" # } Run leverage terraform init and type yes when Terraform asks if you want to import the state to the S3 backend Done. You can remove terraform.tfstate now (and also terraform.tfstate.backup if available)","title":"Set up"},{"location":"developer/ossverse-in-a-box/tf-state/#expected-workflow-after-set-up","text":"This video is outdated!","title":"Expected workflow after set up"},{"location":"developer/ossverse-in-a-box/tf-state/#terraform-remote-state","text":"In the base-tf-backend folder you should find the definition of the infrastructure that needs to be deployed before you can get to work with anything else. IMPORTANT: THIS IS ONLY NEEDED IF THE BACKEND WAS NOT CREATED YET. IF THE BACKEND ALREADY EXISTS YOU JUST USE IT.","title":"Terraform Remote State"},{"location":"developer/ossverse-in-a-box/workflow/","text":"Workflow \u00b6 Overview \u00b6 The sequence of commands that you run to operate on each layer is called the Terraform workflow . In other words, it's what you would typically run in order to create, update, or delete the resources defined in a given layer. The basic workflow \u00b6 Assuming that you have everything configured, the frequent commands you'll need to run are these: # 1. Initialize leverage terraform init # 2. Preview any changes leverage terraform plan # 3. Apply any changes leverage terraform apply The extended workflow \u00b6 Now, the extended workflow is annotated with more explanations and it is intended for users who haven't yet worked with Leverage on a daily basis: Terraform Workflow Make sure you understood the basic concepts: Overview Configuration Directory Structure Remote State Make sure you installed the Leverage CLI . Go to the layer (directory) you need to work with, e.g. shared/global/base-identities/ . Run leverage terraform init -- only the first time you work on this layer, or if you upgraded modules or providers versions, or if you made changes to the Terraform remote backend configuration. Make any changes you need to make. For instance: modify a resource definition, add an output, add a new resource, etc. Run leverage terraform plan to preview any changes. Run leverage terraform apply to give it a final review and to apply any changes. Tip You can use the --layers argument to run Terraform commands on more than one layer. For more information see here Note If desired, at step #5 you could submit a PR, allowing you and the rest of the team to understand and review what changes would be made to your AWS Cloud Architecture components before executing leverage terraform apply ( terraform apply ). This brings the huge benefit of treating changes with a GitOps oriented approach, basically as we should treat any other code & infrastructure change, and integrate it with the rest of our tools and practices like CI/CD, in Running in Automation \u00b6 Figure: Running terraform with AWS in automation (just as reference). Read More Running Terraform in automation","title":"Workflow"},{"location":"developer/ossverse-in-a-box/workflow/#workflow","text":"","title":"Workflow"},{"location":"developer/ossverse-in-a-box/workflow/#overview","text":"The sequence of commands that you run to operate on each layer is called the Terraform workflow . In other words, it's what you would typically run in order to create, update, or delete the resources defined in a given layer.","title":"Overview"},{"location":"developer/ossverse-in-a-box/workflow/#the-basic-workflow","text":"Assuming that you have everything configured, the frequent commands you'll need to run are these: # 1. Initialize leverage terraform init # 2. Preview any changes leverage terraform plan # 3. Apply any changes leverage terraform apply","title":"The basic workflow"},{"location":"developer/ossverse-in-a-box/workflow/#the-extended-workflow","text":"Now, the extended workflow is annotated with more explanations and it is intended for users who haven't yet worked with Leverage on a daily basis: Terraform Workflow Make sure you understood the basic concepts: Overview Configuration Directory Structure Remote State Make sure you installed the Leverage CLI . Go to the layer (directory) you need to work with, e.g. shared/global/base-identities/ . Run leverage terraform init -- only the first time you work on this layer, or if you upgraded modules or providers versions, or if you made changes to the Terraform remote backend configuration. Make any changes you need to make. For instance: modify a resource definition, add an output, add a new resource, etc. Run leverage terraform plan to preview any changes. Run leverage terraform apply to give it a final review and to apply any changes. Tip You can use the --layers argument to run Terraform commands on more than one layer. For more information see here Note If desired, at step #5 you could submit a PR, allowing you and the rest of the team to understand and review what changes would be made to your AWS Cloud Architecture components before executing leverage terraform apply ( terraform apply ). This brings the huge benefit of treating changes with a GitOps oriented approach, basically as we should treat any other code & infrastructure change, and integrate it with the rest of our tools and practices like CI/CD, in","title":"The extended workflow"},{"location":"developer/ossverse-in-a-box/workflow/#running-in-automation","text":"Figure: Running terraform with AWS in automation (just as reference). Read More Running Terraform in automation","title":"Running in Automation"},{"location":"developer/releases/releases-and-versions/","text":"Leverage Product Releases \u00b6 Dear Leveragers, We're constantly kicking with a lot of improvements and some exciting new features Reference Architecture \u00b6 RELEASES Releases | Reference Architecture for AWS Releases | Reference Architecture for HCP Vault Leverage CLI \u00b6 RELEASES Releases | leverage-cli Infrastructure as Code Library \u00b6 RELEASES Releases |Terraform Leverage\u2122 Modules : terraform-aws-waf-owasp terraform-aws-cost-billing-alarm terraform-aws-vpc-flowlogs terraform-aws-cost-budget terraform-aws-tfstate-backend terraform-aws-certbot-lambda terraform-aws-ec2-basic-layout terraform-aws-natgw-notifications terraform-aws-guardduty-multiaccount terraform-aws-network-firewall terraform-aws-backup-notifications terraform-aws-rds-export-to-s3 Releases | Terraform Community Forks Modules : terraform-aws-sso ... Releases | Helm Leverage\u2122 Charts : helm-charts Documentation \u00b6 RELEASES Releases | binbash Leverage\u2122 Documentation","title":"Leverage Product Releases"},{"location":"developer/releases/releases-and-versions/#leverage-product-releases","text":"Dear Leveragers, We're constantly kicking with a lot of improvements and some exciting new features","title":"Leverage Product Releases"},{"location":"developer/releases/releases-and-versions/#reference-architecture","text":"RELEASES Releases | Reference Architecture for AWS Releases | Reference Architecture for HCP Vault","title":"Reference Architecture"},{"location":"developer/releases/releases-and-versions/#leverage-cli","text":"RELEASES Releases | leverage-cli","title":"Leverage CLI"},{"location":"developer/releases/releases-and-versions/#infrastructure-as-code-library","text":"RELEASES Releases |Terraform Leverage\u2122 Modules : terraform-aws-waf-owasp terraform-aws-cost-billing-alarm terraform-aws-vpc-flowlogs terraform-aws-cost-budget terraform-aws-tfstate-backend terraform-aws-certbot-lambda terraform-aws-ec2-basic-layout terraform-aws-natgw-notifications terraform-aws-guardduty-multiaccount terraform-aws-network-firewall terraform-aws-backup-notifications terraform-aws-rds-export-to-s3 Releases | Terraform Community Forks Modules : terraform-aws-sso ... Releases | Helm Leverage\u2122 Charts : helm-charts","title":"Infrastructure as Code Library"},{"location":"developer/releases/releases-and-versions/#documentation","text":"RELEASES Releases | binbash Leverage\u2122 Documentation","title":"Documentation"},{"location":"developer/releases/versions-compatibility-matrix/","text":"Leverage Releases & Versioning \u00b6 binbash Leverage\u2122 and its components intends to be backward compatible, but due to the complex ecosystems of tools we manage this is not always possible. It is always recommended using the latest version of the Leverage CLI with the latest versions of the Reference Architecture for AWS . In case that's not possible we always recommend pinning versions to favor stability and doing controlled updates component by component based on the below presented compatibility matrix table. Compatibility Matrix \u00b6 If you need to know which Leverage CLI versions are compatible with which Leverage Toolbox Docker Images please refer to the Release Notes . Just look for the section called \"Version Compatibility\". Bear in mind though that, at the moment, we do not include a full compatibility table there but at least you should be able to find out what's the Toolbox Image that was used for a given release. If you are looking for the versions of the software included in the Toolbox Docker Image then go instead to the release notes of that repo instead. Release Schedule \u00b6 This project does not follow the Terraform or other release schedule. Leverage aims to provide a reliable deployment and operations experience for the binbash Leverage\u2122 Reference Architecture for AWS , and typically releases about a quarter after the corresponding Terraform release. This time allows for the Terraform project to resolve any issues introduced by the new version and ensures that we can support the latest features. Read more \u00b6 Reference links Consider the following extra links as reference: Hashicorp Terraform releases Amazon EKS Kubernetes release calendar Amazon EKS Kubernetes versions - Amazon EKS","title":"Leverage Releases &amp; Versioning"},{"location":"developer/releases/versions-compatibility-matrix/#leverage-releases-versioning","text":"binbash Leverage\u2122 and its components intends to be backward compatible, but due to the complex ecosystems of tools we manage this is not always possible. It is always recommended using the latest version of the Leverage CLI with the latest versions of the Reference Architecture for AWS . In case that's not possible we always recommend pinning versions to favor stability and doing controlled updates component by component based on the below presented compatibility matrix table.","title":"Leverage Releases &amp; Versioning"},{"location":"developer/releases/versions-compatibility-matrix/#compatibility-matrix","text":"If you need to know which Leverage CLI versions are compatible with which Leverage Toolbox Docker Images please refer to the Release Notes . Just look for the section called \"Version Compatibility\". Bear in mind though that, at the moment, we do not include a full compatibility table there but at least you should be able to find out what's the Toolbox Image that was used for a given release. If you are looking for the versions of the software included in the Toolbox Docker Image then go instead to the release notes of that repo instead.","title":"Compatibility Matrix"},{"location":"developer/releases/versions-compatibility-matrix/#release-schedule","text":"This project does not follow the Terraform or other release schedule. Leverage aims to provide a reliable deployment and operations experience for the binbash Leverage\u2122 Reference Architecture for AWS , and typically releases about a quarter after the corresponding Terraform release. This time allows for the Terraform project to resolve any issues introduced by the new version and ensures that we can support the latest features.","title":"Release Schedule"},{"location":"developer/releases/versions-compatibility-matrix/#read-more","text":"Reference links Consider the following extra links as reference: Hashicorp Terraform releases Amazon EKS Kubernetes release calendar Amazon EKS Kubernetes versions - Amazon EKS","title":"Read more"},{"location":"developer/troubleshooting/","text":"Troubleshooting \u00b6 Repositories \u00b6 General Issues Credential Issues","title":"Troubleshooting"},{"location":"developer/troubleshooting/#troubleshooting","text":"","title":"Troubleshooting"},{"location":"developer/troubleshooting/#repositories","text":"General Issues Credential Issues","title":"Repositories"},{"location":"developer/troubleshooting/credentials/","text":"Troubleshooting credentials issues \u00b6 Make sure you read general troubleshooting page before trying out anything else. Are you using IAM or SSO? \u00b6 Leverage supports two methods for getting AWS credentials: IAM and SSO. We are progressively favoring SSO over IAM, only using the latter as a fallback option. SSO is enabled through the common.tfvars file on this line: sso_enabled = true If that is set to true, then you are using SSO, otherwise it's IAM. Why should I care whether I am using IAM or SSO? \u00b6 Well, because even though both methods will try to get temporary AWS credentials, each method will use a different way to do that. In fact, Leverage relies on the AWS CLI to get the credentials and each method requires completely different commands to achieve that. Do you have MFA enabled? \u00b6 MFA is optionally used via the IAM method. It can be enabled/disabled in the build.env file. Keep in mind that MFA should only be used with the IAM method, not with SSO. Identify which credentials are failing \u00b6 Since Leverage actually relies on Terraform and, since most of the definitions are AWS resources, it is likely that you are having issues with the Terraform AWS provider, in other words, you might be struggling with AWS credentials. Now, bear in mind that Leverage can also be used with other providers such as Gitlab, Github, Hashicorp Cloud Platform, or even SSH via Ansible; so the point here is to understand what credentials are not working for you in order to focus the troubleshooting on the right suspect. Determine the AWS profile you are using \u00b6 When you are facing AWS credentials issues it's important to understand what is the AWS profile that might be causing the issue. Enabling verbose mode should help with that. The suspect profile is likely to show right above the error line and, once you have identified that, you can skip to the next section. If the above doesn't make the error evident yet, perhaps you can explore the following questions: Is it a problem with the Terraform remote state backend? The profile used for that is typically defined in the backend.tfvars file, e.g. this one , or this other one . Is it a problem with another profile used by the layer? Keep in mind that layers can have multiple profile definitions in order to be able to access resources in different accounts. For instance, this is a simple provider definition that uses a single profile , but here's a more complex definition with multiple provider blocks . Can the problematic profile be found in the AWS config file? Or is the profile entry in the AWS config file properly defined? Read the next sections for more details on that. Check the profiles in your AWS config file \u00b6 Once you know what AWS profile is giving you headaches, you can open the AWS config file, typically under ~/.aws/[project_name_here]/config , to look for and inspect that profile definition. Things to look out for: Is there a profile entry in that file that matches the suspect profile? Are there repeated profile entries? Does the profile entry include all necessary fields (e.g. region, role_arn, source_profile; mfa_serial if MFA is enabled)? Keep in mind that profiles change depending on if you are using SSO or IAM for getting credentials so please refer to the corresponding section below in this page to find specific details about your case. Configure the AWS CLI for Leverage \u00b6 These instructions can be used when you need to test your profiles with the AWS CLI, either to verify the profiles are properly set up or to validate the right permissions were granted. Since Leverage stores the AWS config and credentials file under a non-default path, when using the AWS CLI you'll need to point it to the right locations: export AWS_CONFIG_FILE=~/.aws/[project_name_here]/config export AWS_SHARED_CREDENTIALS_FILE=~/.aws/[project_name_here]/credentials Get shell access to the Leverage Toolbox Docker Image Another alternative, if you can't or don't want to install the AWS CLI on your machine, is to use the one included in the Leverage Toolbox Docker image. You can access it by running leverage tf shell Test the failing profile with the AWS CLI \u00b6 Once you have narrowed down your investigation to a profile what you can do is test it. For instance, let's assume that the suspect profile is le-shared-devops . You can run this command: aws sts get-caller-identity --profile le-shared-devops in order to mimic the way that AWS credentials are generated in order to be used by Terraform, so if that command succeeds then that's a good sign. Note: if you use the AWS CLI installed in your host machine, you will need to configure the environment variables in the section \"Configure the AWS CLI for Leverage\" below. AWS CLI Error Messages The AWS CLI has been making great improvements to its error messages over time so it is important to pay attention to its output as it can reveal profiles that have been misconfigured with the wrong roles or missing entries. Regenerating the AWS config or credentials files \u00b6 If you think your AWS config file has misconfigured or missing profile entries (which could happen due to manual editing of that file, or when AWS accounts have been added or remove) you can try regenerating it via Leverage CLI. But before you do that make sure you know which authentication method you are using: SSO or IAM. When using IAM, regenerating your AWS config file can be achieved through the leverage credentials command. Check the command documentation here . When using SSO, the command you need to run is leverage aws configure sso . Refer to that command's documentation for more details. Logging out of your SSO session \u00b6 Seldom times, when using SSO, we have received reports of strange behaviors while trying to run Terraform commands via the Leverage CLI. For instance, users would try to run a leverage tf init command but would get an error saying that their session is expire; so they would try to log in via leverage aws sso login as expected, which would proceed normally so they would try the init command again just to get the same error as before. In these cases, which we are still investigating as they are very hard to reproduce, what has worked for most users is to log out from the SSO session via leverage aws sso logout , even log out from your SSO session through the AWS console running your browser, then try logging back in via leverage aws sso login , and then try the init command again.","title":"Troubleshooting credentials issues"},{"location":"developer/troubleshooting/credentials/#troubleshooting-credentials-issues","text":"Make sure you read general troubleshooting page before trying out anything else.","title":"Troubleshooting credentials issues"},{"location":"developer/troubleshooting/credentials/#are-you-using-iam-or-sso","text":"Leverage supports two methods for getting AWS credentials: IAM and SSO. We are progressively favoring SSO over IAM, only using the latter as a fallback option. SSO is enabled through the common.tfvars file on this line: sso_enabled = true If that is set to true, then you are using SSO, otherwise it's IAM.","title":"Are you using IAM or SSO?"},{"location":"developer/troubleshooting/credentials/#why-should-i-care-whether-i-am-using-iam-or-sso","text":"Well, because even though both methods will try to get temporary AWS credentials, each method will use a different way to do that. In fact, Leverage relies on the AWS CLI to get the credentials and each method requires completely different commands to achieve that.","title":"Why should I care whether I am using IAM or SSO?"},{"location":"developer/troubleshooting/credentials/#do-you-have-mfa-enabled","text":"MFA is optionally used via the IAM method. It can be enabled/disabled in the build.env file. Keep in mind that MFA should only be used with the IAM method, not with SSO.","title":"Do you have MFA enabled?"},{"location":"developer/troubleshooting/credentials/#identify-which-credentials-are-failing","text":"Since Leverage actually relies on Terraform and, since most of the definitions are AWS resources, it is likely that you are having issues with the Terraform AWS provider, in other words, you might be struggling with AWS credentials. Now, bear in mind that Leverage can also be used with other providers such as Gitlab, Github, Hashicorp Cloud Platform, or even SSH via Ansible; so the point here is to understand what credentials are not working for you in order to focus the troubleshooting on the right suspect.","title":"Identify which credentials are failing"},{"location":"developer/troubleshooting/credentials/#determine-the-aws-profile-you-are-using","text":"When you are facing AWS credentials issues it's important to understand what is the AWS profile that might be causing the issue. Enabling verbose mode should help with that. The suspect profile is likely to show right above the error line and, once you have identified that, you can skip to the next section. If the above doesn't make the error evident yet, perhaps you can explore the following questions: Is it a problem with the Terraform remote state backend? The profile used for that is typically defined in the backend.tfvars file, e.g. this one , or this other one . Is it a problem with another profile used by the layer? Keep in mind that layers can have multiple profile definitions in order to be able to access resources in different accounts. For instance, this is a simple provider definition that uses a single profile , but here's a more complex definition with multiple provider blocks . Can the problematic profile be found in the AWS config file? Or is the profile entry in the AWS config file properly defined? Read the next sections for more details on that.","title":"Determine the AWS profile you are using"},{"location":"developer/troubleshooting/credentials/#check-the-profiles-in-your-aws-config-file","text":"Once you know what AWS profile is giving you headaches, you can open the AWS config file, typically under ~/.aws/[project_name_here]/config , to look for and inspect that profile definition. Things to look out for: Is there a profile entry in that file that matches the suspect profile? Are there repeated profile entries? Does the profile entry include all necessary fields (e.g. region, role_arn, source_profile; mfa_serial if MFA is enabled)? Keep in mind that profiles change depending on if you are using SSO or IAM for getting credentials so please refer to the corresponding section below in this page to find specific details about your case.","title":"Check the profiles in your AWS config file"},{"location":"developer/troubleshooting/credentials/#configure-the-aws-cli-for-leverage","text":"These instructions can be used when you need to test your profiles with the AWS CLI, either to verify the profiles are properly set up or to validate the right permissions were granted. Since Leverage stores the AWS config and credentials file under a non-default path, when using the AWS CLI you'll need to point it to the right locations: export AWS_CONFIG_FILE=~/.aws/[project_name_here]/config export AWS_SHARED_CREDENTIALS_FILE=~/.aws/[project_name_here]/credentials Get shell access to the Leverage Toolbox Docker Image Another alternative, if you can't or don't want to install the AWS CLI on your machine, is to use the one included in the Leverage Toolbox Docker image. You can access it by running leverage tf shell","title":"Configure the AWS CLI for Leverage"},{"location":"developer/troubleshooting/credentials/#test-the-failing-profile-with-the-aws-cli","text":"Once you have narrowed down your investigation to a profile what you can do is test it. For instance, let's assume that the suspect profile is le-shared-devops . You can run this command: aws sts get-caller-identity --profile le-shared-devops in order to mimic the way that AWS credentials are generated in order to be used by Terraform, so if that command succeeds then that's a good sign. Note: if you use the AWS CLI installed in your host machine, you will need to configure the environment variables in the section \"Configure the AWS CLI for Leverage\" below. AWS CLI Error Messages The AWS CLI has been making great improvements to its error messages over time so it is important to pay attention to its output as it can reveal profiles that have been misconfigured with the wrong roles or missing entries.","title":"Test the failing profile with the AWS CLI"},{"location":"developer/troubleshooting/credentials/#regenerating-the-aws-config-or-credentials-files","text":"If you think your AWS config file has misconfigured or missing profile entries (which could happen due to manual editing of that file, or when AWS accounts have been added or remove) you can try regenerating it via Leverage CLI. But before you do that make sure you know which authentication method you are using: SSO or IAM. When using IAM, regenerating your AWS config file can be achieved through the leverage credentials command. Check the command documentation here . When using SSO, the command you need to run is leverage aws configure sso . Refer to that command's documentation for more details.","title":"Regenerating the AWS config or credentials files"},{"location":"developer/troubleshooting/credentials/#logging-out-of-your-sso-session","text":"Seldom times, when using SSO, we have received reports of strange behaviors while trying to run Terraform commands via the Leverage CLI. For instance, users would try to run a leverage tf init command but would get an error saying that their session is expire; so they would try to log in via leverage aws sso login as expected, which would proceed normally so they would try the init command again just to get the same error as before. In these cases, which we are still investigating as they are very hard to reproduce, what has worked for most users is to log out from the SSO session via leverage aws sso logout , even log out from your SSO session through the AWS console running your browser, then try logging back in via leverage aws sso login , and then try the init command again.","title":"Logging out of your SSO session"},{"location":"developer/troubleshooting/general/","text":"Troubleshooting general issues \u00b6 Gathering more information \u00b6 Trying to get as much information about the issue as possible is key when troubleshooting. If the issue happens while you are working on a layer of the reference architecture and you are using Terraform, you can use the --verbose flag to try to get more information about the underlying issue. For instance, if the error shows up while running a Terraform plan command, you can enable a more verbose output like follows: leverage --verbose tf plan The --verbose flag can also be used when you are working with the Ansible Reference Architecture: leverage --verbose run init Understanding how Leverage gets the AWS credentials for Terraform and other tools \u00b6 Firstly, you need to know that Terraform doesn't support AWS authentication methods that require user interaction. For instance, logging in via SSO or assuming roles that require MFA. That is why Leverage made the following two design decisions in that regard: Configure Terraform to use AWS profiles via Terraform AWS provider and local AWS configuration files . Leverage handles the user interactivity during the authentication phase in order to get the credentials that Terraform needs through AWS profiles. So, Leverage runs simple bash scripts to deal with 2. and then passes the execution flow to Terraform which by then should have the AWS profiles ready-to-use and in the expected path. Where are those AWS profiles stored again? \u00b6 They are stored in 2 files: config and credentials . By default, the AWS CLI will create those files under this path: ~/.aws/ but Leverage uses a slightly different convention, so they should actually be located in this path: ~/.aws/[project_name_here]/ . So, for instance, if your project name is acme , then said files should be found under: ~/.aws/acme/config and ~/.aws/acme/credentials . SSH reiterative confirmation \u00b6 If you get a reiterative dialog for confirmation while running a leverage terraform init : Warning: the ECDSA host key for 'YYY' differs from the key for the IP address 'ZZZ.ZZZ.ZZZ.ZZZ' Offending key for IP in /root/.ssh/known_hosts:xyz Matching host key in /root/.ssh/known_hosts:xyw Are you sure you want to continue connecting (yes/no)? You may have more than 1 key associated to the YYY host. Remove the old or incorrect one, and the dialog should stop. Leverage CLI can't find the Docker daemon \u00b6 The Leverage CLI talks to the Docker API which usually runs as a daemon on your machine. Here's an example of the error: $ leverage tf shell [17:06:13.754] ERROR Docker daemon doesn't seem to be responding. Please check it is up and running correctly before re-running the command. MacOS after Docker Desktop upgrade \u00b6 We've seen this happen after a Docker Desktop upgrade. Defaults are changed and the Docker daemon no longer uses Unix sockets but TCP, or perhaps it does use Unix sockets but under a different path or user. What has worked for us in order to fix the issue is to make sure the following setting is enabled: Note: that setting can be accessed by clicking on the Docker Desktop icon tray, and then clicking on \"Settings...\". Then click on the \"Advanced\" tab to find the checkbox. Linux and Docker in Rootless mode \u00b6 First make sure the user is added to the docker group: $ sudo usermod -aG docker $USER $ newgrp docker If that does not solve it, then the same problem might come from missing env variable DOCKER_HOST . leverage looks for Docker socket at unix:///var/run/docker.sock unless DOCKER_HOST is provided in environment. If you installed Docker in Rootless mode , you need to remember to add DOCKER_HOST in you rc files: export DOCKER_HOST = unix:///run/user/1000/docker.sock or prefix the leverage tool with the env var: $ DOCKER_HOST = unix:///run/user/1000/docker.sock leverage tf shell Leverage CLI fails to mount the SSH directory \u00b6 The Leverage CLI mounts the ~/.ssh directory in order to make the pulling of private Terraform modules work. The error should look similar to the following: [18:26:44.416] ERROR Error creating container: APIError: 400 Client Error for http+docker://localhost/v1.43/containers/create: Bad Request (\"invalid mount config for type \"bind\": stat /host_mnt/private/tmp/com.apple.launchd.CWrsoki5yP/Listeners: operation not supported\") The problem happes because of the file system virtualization that is used by default and can be fixed by choosing the \"osxfs (Legacy)\" option as shown below: Note: that setting can be accessed by clicking on the Docker Desktop icon tray, and then clicking on \"Settings...\". The setting should be in the \"General\" tab. Leverage CLI fails because of missing .gitconfig \u00b6 The Leverage CLI might fail when setting up for the first time if there is no ~/.gitconfig in your home directory or if it is misconfigured. You will see something similar to: [17:13:43.514] ERROR Error creating container: APIError: 400 Client Error for http+docker://localhost/v1.43/containers/create: ... (\"could not find .gitconfig\") In order to fix this, just configure git globally: $ git config --global user.name \"Name Lastname\" $ git config --global user.email \"name.lastname@email.com\"","title":"Troubleshooting general issues"},{"location":"developer/troubleshooting/general/#troubleshooting-general-issues","text":"","title":"Troubleshooting general issues"},{"location":"developer/troubleshooting/general/#gathering-more-information","text":"Trying to get as much information about the issue as possible is key when troubleshooting. If the issue happens while you are working on a layer of the reference architecture and you are using Terraform, you can use the --verbose flag to try to get more information about the underlying issue. For instance, if the error shows up while running a Terraform plan command, you can enable a more verbose output like follows: leverage --verbose tf plan The --verbose flag can also be used when you are working with the Ansible Reference Architecture: leverage --verbose run init","title":"Gathering more information"},{"location":"developer/troubleshooting/general/#understanding-how-leverage-gets-the-aws-credentials-for-terraform-and-other-tools","text":"Firstly, you need to know that Terraform doesn't support AWS authentication methods that require user interaction. For instance, logging in via SSO or assuming roles that require MFA. That is why Leverage made the following two design decisions in that regard: Configure Terraform to use AWS profiles via Terraform AWS provider and local AWS configuration files . Leverage handles the user interactivity during the authentication phase in order to get the credentials that Terraform needs through AWS profiles. So, Leverage runs simple bash scripts to deal with 2. and then passes the execution flow to Terraform which by then should have the AWS profiles ready-to-use and in the expected path.","title":"Understanding how Leverage gets the AWS credentials for Terraform and other tools"},{"location":"developer/troubleshooting/general/#where-are-those-aws-profiles-stored-again","text":"They are stored in 2 files: config and credentials . By default, the AWS CLI will create those files under this path: ~/.aws/ but Leverage uses a slightly different convention, so they should actually be located in this path: ~/.aws/[project_name_here]/ . So, for instance, if your project name is acme , then said files should be found under: ~/.aws/acme/config and ~/.aws/acme/credentials .","title":"Where are those AWS profiles stored again?"},{"location":"developer/troubleshooting/general/#ssh-reiterative-confirmation","text":"If you get a reiterative dialog for confirmation while running a leverage terraform init : Warning: the ECDSA host key for 'YYY' differs from the key for the IP address 'ZZZ.ZZZ.ZZZ.ZZZ' Offending key for IP in /root/.ssh/known_hosts:xyz Matching host key in /root/.ssh/known_hosts:xyw Are you sure you want to continue connecting (yes/no)? You may have more than 1 key associated to the YYY host. Remove the old or incorrect one, and the dialog should stop.","title":"SSH reiterative confirmation"},{"location":"developer/troubleshooting/general/#leverage-cli-cant-find-the-docker-daemon","text":"The Leverage CLI talks to the Docker API which usually runs as a daemon on your machine. Here's an example of the error: $ leverage tf shell [17:06:13.754] ERROR Docker daemon doesn't seem to be responding. Please check it is up and running correctly before re-running the command.","title":"Leverage CLI can't find the Docker daemon"},{"location":"developer/troubleshooting/general/#macos-after-docker-desktop-upgrade","text":"We've seen this happen after a Docker Desktop upgrade. Defaults are changed and the Docker daemon no longer uses Unix sockets but TCP, or perhaps it does use Unix sockets but under a different path or user. What has worked for us in order to fix the issue is to make sure the following setting is enabled: Note: that setting can be accessed by clicking on the Docker Desktop icon tray, and then clicking on \"Settings...\". Then click on the \"Advanced\" tab to find the checkbox.","title":"MacOS after Docker Desktop upgrade"},{"location":"developer/troubleshooting/general/#linux-and-docker-in-rootless-mode","text":"First make sure the user is added to the docker group: $ sudo usermod -aG docker $USER $ newgrp docker If that does not solve it, then the same problem might come from missing env variable DOCKER_HOST . leverage looks for Docker socket at unix:///var/run/docker.sock unless DOCKER_HOST is provided in environment. If you installed Docker in Rootless mode , you need to remember to add DOCKER_HOST in you rc files: export DOCKER_HOST = unix:///run/user/1000/docker.sock or prefix the leverage tool with the env var: $ DOCKER_HOST = unix:///run/user/1000/docker.sock leverage tf shell","title":"Linux and Docker in Rootless mode"},{"location":"developer/troubleshooting/general/#leverage-cli-fails-to-mount-the-ssh-directory","text":"The Leverage CLI mounts the ~/.ssh directory in order to make the pulling of private Terraform modules work. The error should look similar to the following: [18:26:44.416] ERROR Error creating container: APIError: 400 Client Error for http+docker://localhost/v1.43/containers/create: Bad Request (\"invalid mount config for type \"bind\": stat /host_mnt/private/tmp/com.apple.launchd.CWrsoki5yP/Listeners: operation not supported\") The problem happes because of the file system virtualization that is used by default and can be fixed by choosing the \"osxfs (Legacy)\" option as shown below: Note: that setting can be accessed by clicking on the Docker Desktop icon tray, and then clicking on \"Settings...\". The setting should be in the \"General\" tab.","title":"Leverage CLI fails to mount the SSH directory"},{"location":"developer/troubleshooting/general/#leverage-cli-fails-because-of-missing-gitconfig","text":"The Leverage CLI might fail when setting up for the first time if there is no ~/.gitconfig in your home directory or if it is misconfigured. You will see something similar to: [17:13:43.514] ERROR Error creating container: APIError: 400 Client Error for http+docker://localhost/v1.43/containers/create: ... (\"could not find .gitconfig\") In order to fix this, just configure git globally: $ git config --global user.name \"Name Lastname\" $ git config --global user.email \"name.lastname@email.com\"","title":"Leverage CLI fails because of missing .gitconfig"},{"location":"getting-started/overview/","text":"To Be Added","title":"Overview"},{"location":"insiders/","text":"TBD \u00b6","title":"Index"},{"location":"insiders/#tbd","text":"","title":"TBD"},{"location":"resources/","text":"Educational Resources \u00b6 Beckn Protocol Documentation: \u00b6 Core Specification Domain-Specific Adaptations: Explore adaptations for various industries (e.g., education , mobility , energy ). Open Source Security Guides & Best Practices: TODO \u00b6 Open-source security best practices Vulnerability management Compliance frameworks Responsible AI Resources: TODO \u00b6 Fairness Explainability Transparency BeS Lab Documentation: TODO \u00b6 Environment Setup Guides Tool Installation & Usage BeS Playbook Examples Case Studies & Whitepapers: TODO \u00b6 Community Resources \u00b6 Forums & Discussion Boards: (To be created) \u00b6 Events & Webinars: TODO \u00b6 Newsletter or Blog: TODO \u00b6 OSSVerse The broader open-source community BeS Lab Tutorials & Demos: TODO \u00b6 Call to Action \u00b6 Join the OSSVerse Community Today! Start Your Journey Towards Trusted and Verified Open Source Software","title":"Using OSSVerse"},{"location":"resources/#educational-resources","text":"","title":"Educational Resources"},{"location":"resources/#beckn-protocol-documentation","text":"Core Specification Domain-Specific Adaptations: Explore adaptations for various industries (e.g., education , mobility , energy ).","title":"Beckn Protocol Documentation:"},{"location":"resources/#open-source-security-guides-best-practices-todo","text":"Open-source security best practices Vulnerability management Compliance frameworks","title":"Open Source Security Guides &amp; Best Practices: TODO"},{"location":"resources/#responsible-ai-resources-todo","text":"Fairness Explainability Transparency","title":"Responsible AI Resources: TODO"},{"location":"resources/#bes-lab-documentation-todo","text":"Environment Setup Guides Tool Installation & Usage BeS Playbook Examples","title":"BeS Lab Documentation: TODO"},{"location":"resources/#case-studies-whitepapers-todo","text":"","title":"Case Studies &amp; Whitepapers: TODO"},{"location":"resources/#community-resources","text":"","title":"Community Resources"},{"location":"resources/#forums-discussion-boards-to-be-created","text":"","title":"Forums &amp; Discussion Boards: (To be created)"},{"location":"resources/#events-webinars-todo","text":"","title":"Events &amp; Webinars: TODO"},{"location":"resources/#newsletter-or-blog-todo","text":"OSSVerse The broader open-source community BeS Lab","title":"Newsletter or Blog: TODO"},{"location":"resources/#tutorials-demos-todo","text":"","title":"Tutorials &amp; Demos: TODO"},{"location":"resources/#call-to-action","text":"Join the OSSVerse Community Today! Start Your Journey Towards Trusted and Verified Open Source Software","title":"Call to Action"},{"location":"resources/add-aws-accounts/","text":"Add more AWS Accounts \u00b6 Brief \u00b6 You can add new AWS accounts to your Leverage project by following the steps in this page. Important In the examples below, we will be using apps-prd as the account we will be adding and it will be created in the us-east-1 region. Create the new account in your AWS Organization \u00b6 Go to management/global/organizations . Edit the locals.tf file to add the account to the local accounts variable. accounts = { ... ... apps-prd = { email = \"aws+apps-prd@yourcompany.com\", parent_ou = \"apps\" } } Note that the apps organizational unit (OU) is being used as the parent OU of the new account. If you need to use a new OU you can add it to organizational_units variable in the same file. Run the Terraform workflow to apply the new changes. Typically that would be this: leverage terraform init leverage terraform apply Authentication error Note this layer was first applied before using the boostrap user. Now, that we are working with SSO, credentials have changed. So, if this is the first account you add you'll probably get this error applying: \"Error: error configuring S3 Backend: no valid credential sources for S3 Backend found.\" In this case running leverage tf init -reconfigure will fix the issue. Add the new account to the <project>/config/common.tfvars file. The new account ID should have been displayed in the output of the previous step, e.g.: aws_organizations_account.accounts [ \"apps-prd\" ] : Creation complete after 14s [ id = 999999999999 ] Note the id, 999999999999 . ...so please grab it from there and use it to update the file as shown below: accounts = { [ ... ] apps-prd = { email = \"<aws+apps-prd@yourcompany.com>\" , id = \"<add-the-account-id-here>\" } } 5. Since you are using SSO in this project, permissions on the new account must be granted before we can move forward. Add the right permissions to the management/global/sso/account_assignments.tf file. For the example: # ------------------------------------------------------------------------- # apps-prd account # ------------------------------------------------------------------------- { account = var.accounts.apps-prd.id , permission_set_arn = module.permission_sets.permission_sets [ \"Administrator\" ] .arn , permission_set_name = \"Administrator\" , principal_type = local.principal_type_group principal_name = local.groups [ \"administrators\" ] .name } , { account = var.accounts.apps-prd.id , permission_set_arn = module.permission_sets.permission_sets [ \"DevOps\" ] .arn , permission_set_name = \"DevOps\" , principal_type = local.principal_type_group principal_name = local.groups [ \"devops\" ] .name } , Note your needs can vary, these permissions are just an example, please be careful with what you are granting here. Apply these changes: leverage terraform apply And you must update your AWS config file accordingly by running this: leverage aws configure sso Good! Now you are ready to create the initial directory structure for the new account. The next section will guide through those steps. Create and deploy the layers for the new account \u00b6 In this example we will create the apps-prd account structure by using the shared as a template. Create the initial directory structure for the new account \u00b6 Ensure you are at the root of this repository Now create the directory structure for the new account: mkdir -p apps-prd/ { global,us-east-1 } Set up the config files: Create the config files for this account: cp -r shared/config apps-prd/config Open apps-prd/config/backend.tfvars and replace any occurrences of shared with apps-prd . Do the same with apps-prd/config/account.tfvars Create the Terraform Backend layer \u00b6 Copy the layer from an existing one: cp -r shared/us-east-1/base-tf-backend apps-prd/us-east-1/base-tf-backend Info If the source layer was already initialized you should delete the previous Terraform setup using sudo rm -rf .terraform* in the target layer's directory, e.g. rm -rf apps-prd/us-east-1/base-tf-backend/.terraform* Go to the apps-prd/us-east-1/base-tf-backend directory, open the config.tf file and comment the S3 backend block. E.g.: #backend \"s3\" { # key = \"shared/tf-backend/terraform.tfstate\" #} We need to do this for the first apply of this layer. Now run the Terraform workflow to initialize and apply this layer. The flag --skip-validation is needed here since the bucket does not yet exist. leverage terraform init --skip-validation leverage terraform apply Open the config.tf file again uncommenting the block commented before and replacing shared with apps-prd . E.g.: backend \"s3\" { key = \"apps-prd/tf-backend/terraform.tfstate\" } To finish with the backend layer, re-init to move the tfstate to the new location. Run: leverage terraform init Terraform will detect that you are trying to move from a local to a remote state and will ask for confirmation. Initializing the backend... Acquiring state lock. This may take a few moments... Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"local\" backend to the newly configured \"s3\" backend. No existing state was found in the newly configured \"s3\" backend. Do you want to copy this state to the new \"s3\" backend? Enter \"yes\" to copy and \"no\" to start with an empty state. Enter a value: Enter yes and hit enter. Create the security-base layer \u00b6 Copy the layer from an existing one: From the repository root run: cp -r shared/us-east-1/security-base apps-prd/us-east-1/security-base Info If the source layer was already initialized you should delete the previous Terraform setup using sudo rm -rf .terraform* in the target layer's directory, e.g. rm -rf apps-prd/us-east-1/security-base/.terraform* Go to the apps-prd/us-east-1/security-base directory and open the config.tf file replacing any occurrences of shared with apps-prd E.g. this line should be: backend \"s3\" { key = \"apps-prd/security-base/terraform.tfstate\" } Init and apply the layer leverage tf init leverage tf apply Create the network layer \u00b6 Copy the layer from an existing one: From the root of the repository run this: cp -r shared/us-east-1/base-network apps-prd/us-east-1/base-network Info If the source layer was already initialized you should delete the previous Terraform setup using sudo rm -rf .terraform* in the target layer's directory, e.g. rm -rf apps-prd/us-east-1/base-network/.terraform* Go to the apps-prd/us-east-1/base-network directory and open the config.tf file replacing any occurrences of shared with apps-prd . E.g. this line should be: backend \"s3\" { key = \"apps-prd/network/terraform.tfstate\" } Open the file locals.tf and set the new account's CIDRs. vpc_cidr_block = \"172.19.0.0/20\" azs = [ \"${var.region}a\", \"${var.region}b\", #\"${var.region}c\", #\"${var.region}d\", ] private_subnets_cidr = [\"172.19.0.0/21\"] private_subnets = [ \"172.19.0.0/23\", \"172.19.2.0/23\", #\"172.19.4.0/23\", #\"172.19.6.0/23\", ] public_subnets_cidr = [\"172.19.8.0/21\"] public_subnets = [ \"172.19.8.0/23\", \"172.19.10.0/23\", #\"172.19.12.0/23\", #\"172.19.14.0/23\", ] Note here only two AZs are enabled, if needed uncomment the other ones in the three structures. Do not overlap CIDRs! Be careful when chosing CIDRs. Avoid overlaping CIDRs between accounts. If you need a reference on how to chose the right CIDRs, please see here . Calculate CIDRs To calculate CIDRs you can check this playbook . Init and apply the layer leverage tf init leverage tf apply Create the VPC Peering between the new account and the VPC of the Shared account. Edit file shared/us-east-1/base-network/config.tf and add provider and remote state for the created account. provider \"aws\" { alias = \"apps-prd\" region = var.region profile = \"${var.project}-apps-prd-devops\" } data \"terraform_remote_state\" \"apps-prd-vpcs\" { for_each = { for k, v in local.apps-prd-vpcs : k => v if !v[\"tgw\"] } backend = \"s3\" config = { region = lookup(each.value, \"region\") profile = lookup(each.value, \"profile\") bucket = lookup(each.value, \"bucket\") key = lookup(each.value, \"key\") } } Edit file shared/us-east-1/base-network/locals.tf and under # # Data source definitions # ...add the related structure: # # Data source definitions # apps-prd-vpcs = { apps-prd-base = { region = var.region profile = \"${var.project}-apps-prd-devops\" bucket = \"${var.project}-apps-prd-terraform-backend\" key = \"apps-prd/network/terraform.tfstate\" tgw = false } } Edit file shared/us-east-1/base-network/vpc_peerings.tf (if this is your first added account the file won\u00b4t exist, please crate it) and add the peering definition: # # VPC Peering: AppsPrd VPC => Shared VPC # module \"vpc_peering_apps_prd_to_shared\" { source = \"github.com/binbashar/terraform-aws-vpc-peering.git?ref=v6.0.0\" for_each = { for k, v in local.apps-prd-vpcs : k => v if !v[\"tgw\"] } providers = { aws.this = aws aws.peer = aws.apps-prd } this_vpc_id = module.vpc.vpc_id peer_vpc_id = data.terraform_remote_state.apps-prd-vpcs[each.key].outputs.vpc_id this_rts_ids = concat(module.vpc.private_route_table_ids, module.vpc.public_route_table_ids) peer_rts_ids = concat( data.terraform_remote_state.apps-prd-vpcs[each.key].outputs.public_route_table_ids, data.terraform_remote_state.apps-prd-vpcs[each.key].outputs.private_route_table_ids ) auto_accept_peering = true tags = merge(local.tags, { \"Name\" = \"${each.key}-to-shared\", \"PeeringRequester\" = each.key, \"PeeringAccepter\" = \"shared\" }) } Apply the changes (be sure to CD into shared/us-east-1/base-network layer for doing this): leverage terraform init leverage terraform apply Done! \u00b6 That should be it. At this point you should have the following: A brand new AWS account in your AWS organization. Working configuration files for both existing layers and any new layer you add in the future. A remote Terraform State Backend for this new account. Roles and policies (SSO) that are necessary to access the new account. The base networking resources ready to host your compute services. The VPC peerings between the new account and shared Next steps \u00b6 Now you have a new account created, so what else? To keep creating infra on top of this binbash Leverage Landing Zone with this new account added, please check: Check common use cases in Playbooks Review the binbash Leverage architecture Go for EKS !","title":"Add more AWS Accounts"},{"location":"resources/add-aws-accounts/#add-more-aws-accounts","text":"","title":"Add more AWS Accounts"},{"location":"resources/add-aws-accounts/#brief","text":"You can add new AWS accounts to your Leverage project by following the steps in this page. Important In the examples below, we will be using apps-prd as the account we will be adding and it will be created in the us-east-1 region.","title":"Brief"},{"location":"resources/add-aws-accounts/#create-the-new-account-in-your-aws-organization","text":"Go to management/global/organizations . Edit the locals.tf file to add the account to the local accounts variable. accounts = { ... ... apps-prd = { email = \"aws+apps-prd@yourcompany.com\", parent_ou = \"apps\" } } Note that the apps organizational unit (OU) is being used as the parent OU of the new account. If you need to use a new OU you can add it to organizational_units variable in the same file. Run the Terraform workflow to apply the new changes. Typically that would be this: leverage terraform init leverage terraform apply Authentication error Note this layer was first applied before using the boostrap user. Now, that we are working with SSO, credentials have changed. So, if this is the first account you add you'll probably get this error applying: \"Error: error configuring S3 Backend: no valid credential sources for S3 Backend found.\" In this case running leverage tf init -reconfigure will fix the issue. Add the new account to the <project>/config/common.tfvars file. The new account ID should have been displayed in the output of the previous step, e.g.: aws_organizations_account.accounts [ \"apps-prd\" ] : Creation complete after 14s [ id = 999999999999 ] Note the id, 999999999999 . ...so please grab it from there and use it to update the file as shown below: accounts = { [ ... ] apps-prd = { email = \"<aws+apps-prd@yourcompany.com>\" , id = \"<add-the-account-id-here>\" } } 5. Since you are using SSO in this project, permissions on the new account must be granted before we can move forward. Add the right permissions to the management/global/sso/account_assignments.tf file. For the example: # ------------------------------------------------------------------------- # apps-prd account # ------------------------------------------------------------------------- { account = var.accounts.apps-prd.id , permission_set_arn = module.permission_sets.permission_sets [ \"Administrator\" ] .arn , permission_set_name = \"Administrator\" , principal_type = local.principal_type_group principal_name = local.groups [ \"administrators\" ] .name } , { account = var.accounts.apps-prd.id , permission_set_arn = module.permission_sets.permission_sets [ \"DevOps\" ] .arn , permission_set_name = \"DevOps\" , principal_type = local.principal_type_group principal_name = local.groups [ \"devops\" ] .name } , Note your needs can vary, these permissions are just an example, please be careful with what you are granting here. Apply these changes: leverage terraform apply And you must update your AWS config file accordingly by running this: leverage aws configure sso Good! Now you are ready to create the initial directory structure for the new account. The next section will guide through those steps.","title":"Create the new account in your AWS Organization"},{"location":"resources/add-aws-accounts/#create-and-deploy-the-layers-for-the-new-account","text":"In this example we will create the apps-prd account structure by using the shared as a template.","title":"Create and deploy the layers for the new account"},{"location":"resources/add-aws-accounts/#create-the-initial-directory-structure-for-the-new-account","text":"Ensure you are at the root of this repository Now create the directory structure for the new account: mkdir -p apps-prd/ { global,us-east-1 } Set up the config files: Create the config files for this account: cp -r shared/config apps-prd/config Open apps-prd/config/backend.tfvars and replace any occurrences of shared with apps-prd . Do the same with apps-prd/config/account.tfvars","title":"Create the initial directory structure for the new account"},{"location":"resources/add-aws-accounts/#create-the-terraform-backend-layer","text":"Copy the layer from an existing one: cp -r shared/us-east-1/base-tf-backend apps-prd/us-east-1/base-tf-backend Info If the source layer was already initialized you should delete the previous Terraform setup using sudo rm -rf .terraform* in the target layer's directory, e.g. rm -rf apps-prd/us-east-1/base-tf-backend/.terraform* Go to the apps-prd/us-east-1/base-tf-backend directory, open the config.tf file and comment the S3 backend block. E.g.: #backend \"s3\" { # key = \"shared/tf-backend/terraform.tfstate\" #} We need to do this for the first apply of this layer. Now run the Terraform workflow to initialize and apply this layer. The flag --skip-validation is needed here since the bucket does not yet exist. leverage terraform init --skip-validation leverage terraform apply Open the config.tf file again uncommenting the block commented before and replacing shared with apps-prd . E.g.: backend \"s3\" { key = \"apps-prd/tf-backend/terraform.tfstate\" } To finish with the backend layer, re-init to move the tfstate to the new location. Run: leverage terraform init Terraform will detect that you are trying to move from a local to a remote state and will ask for confirmation. Initializing the backend... Acquiring state lock. This may take a few moments... Do you want to copy existing state to the new backend? Pre-existing state was found while migrating the previous \"local\" backend to the newly configured \"s3\" backend. No existing state was found in the newly configured \"s3\" backend. Do you want to copy this state to the new \"s3\" backend? Enter \"yes\" to copy and \"no\" to start with an empty state. Enter a value: Enter yes and hit enter.","title":"Create the Terraform Backend layer"},{"location":"resources/add-aws-accounts/#create-the-security-base-layer","text":"Copy the layer from an existing one: From the repository root run: cp -r shared/us-east-1/security-base apps-prd/us-east-1/security-base Info If the source layer was already initialized you should delete the previous Terraform setup using sudo rm -rf .terraform* in the target layer's directory, e.g. rm -rf apps-prd/us-east-1/security-base/.terraform* Go to the apps-prd/us-east-1/security-base directory and open the config.tf file replacing any occurrences of shared with apps-prd E.g. this line should be: backend \"s3\" { key = \"apps-prd/security-base/terraform.tfstate\" } Init and apply the layer leverage tf init leverage tf apply","title":"Create the security-base layer"},{"location":"resources/add-aws-accounts/#create-the-network-layer","text":"Copy the layer from an existing one: From the root of the repository run this: cp -r shared/us-east-1/base-network apps-prd/us-east-1/base-network Info If the source layer was already initialized you should delete the previous Terraform setup using sudo rm -rf .terraform* in the target layer's directory, e.g. rm -rf apps-prd/us-east-1/base-network/.terraform* Go to the apps-prd/us-east-1/base-network directory and open the config.tf file replacing any occurrences of shared with apps-prd . E.g. this line should be: backend \"s3\" { key = \"apps-prd/network/terraform.tfstate\" } Open the file locals.tf and set the new account's CIDRs. vpc_cidr_block = \"172.19.0.0/20\" azs = [ \"${var.region}a\", \"${var.region}b\", #\"${var.region}c\", #\"${var.region}d\", ] private_subnets_cidr = [\"172.19.0.0/21\"] private_subnets = [ \"172.19.0.0/23\", \"172.19.2.0/23\", #\"172.19.4.0/23\", #\"172.19.6.0/23\", ] public_subnets_cidr = [\"172.19.8.0/21\"] public_subnets = [ \"172.19.8.0/23\", \"172.19.10.0/23\", #\"172.19.12.0/23\", #\"172.19.14.0/23\", ] Note here only two AZs are enabled, if needed uncomment the other ones in the three structures. Do not overlap CIDRs! Be careful when chosing CIDRs. Avoid overlaping CIDRs between accounts. If you need a reference on how to chose the right CIDRs, please see here . Calculate CIDRs To calculate CIDRs you can check this playbook . Init and apply the layer leverage tf init leverage tf apply Create the VPC Peering between the new account and the VPC of the Shared account. Edit file shared/us-east-1/base-network/config.tf and add provider and remote state for the created account. provider \"aws\" { alias = \"apps-prd\" region = var.region profile = \"${var.project}-apps-prd-devops\" } data \"terraform_remote_state\" \"apps-prd-vpcs\" { for_each = { for k, v in local.apps-prd-vpcs : k => v if !v[\"tgw\"] } backend = \"s3\" config = { region = lookup(each.value, \"region\") profile = lookup(each.value, \"profile\") bucket = lookup(each.value, \"bucket\") key = lookup(each.value, \"key\") } } Edit file shared/us-east-1/base-network/locals.tf and under # # Data source definitions # ...add the related structure: # # Data source definitions # apps-prd-vpcs = { apps-prd-base = { region = var.region profile = \"${var.project}-apps-prd-devops\" bucket = \"${var.project}-apps-prd-terraform-backend\" key = \"apps-prd/network/terraform.tfstate\" tgw = false } } Edit file shared/us-east-1/base-network/vpc_peerings.tf (if this is your first added account the file won\u00b4t exist, please crate it) and add the peering definition: # # VPC Peering: AppsPrd VPC => Shared VPC # module \"vpc_peering_apps_prd_to_shared\" { source = \"github.com/binbashar/terraform-aws-vpc-peering.git?ref=v6.0.0\" for_each = { for k, v in local.apps-prd-vpcs : k => v if !v[\"tgw\"] } providers = { aws.this = aws aws.peer = aws.apps-prd } this_vpc_id = module.vpc.vpc_id peer_vpc_id = data.terraform_remote_state.apps-prd-vpcs[each.key].outputs.vpc_id this_rts_ids = concat(module.vpc.private_route_table_ids, module.vpc.public_route_table_ids) peer_rts_ids = concat( data.terraform_remote_state.apps-prd-vpcs[each.key].outputs.public_route_table_ids, data.terraform_remote_state.apps-prd-vpcs[each.key].outputs.private_route_table_ids ) auto_accept_peering = true tags = merge(local.tags, { \"Name\" = \"${each.key}-to-shared\", \"PeeringRequester\" = each.key, \"PeeringAccepter\" = \"shared\" }) } Apply the changes (be sure to CD into shared/us-east-1/base-network layer for doing this): leverage terraform init leverage terraform apply","title":"Create the network layer"},{"location":"resources/add-aws-accounts/#done","text":"That should be it. At this point you should have the following: A brand new AWS account in your AWS organization. Working configuration files for both existing layers and any new layer you add in the future. A remote Terraform State Backend for this new account. Roles and policies (SSO) that are necessary to access the new account. The base networking resources ready to host your compute services. The VPC peerings between the new account and shared","title":"Done!"},{"location":"resources/add-aws-accounts/#next-steps","text":"Now you have a new account created, so what else? To keep creating infra on top of this binbash Leverage Landing Zone with this new account added, please check: Check common use cases in Playbooks Review the binbash Leverage architecture Go for EKS !","title":"Next steps"},{"location":"resources/aws-account-setup/","text":"Creating your AWS Management account \u00b6 Create the first AWS account \u00b6 First and foremost you'll need to create an AWS account for your project. Attention Note this will be your management account and has to be called <project-name>-management . E.g. if your project is called binbash then your account should be binbash-management . Follow the instructions here . This will be the management account for your AWS Organization and the email address you use for signing up will be the root user of this account -- you can see this user represented in the architecture diagram . Since the root user is the main access point to your account it is strongly recommended that you keep its credentials (email, password) safe by following AWS best practices . Tip To protect your management account, [enabling Multi Factor Authentication](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html#id_root-user_manage_mfa) is **highly** encouraged. Also, reviewing the [account's billing setup](https://console.aws.amazon.com/billing/home?#/account) is always a good idea before proceeding. For more details on setting up your AWS account: Organization account setup guide Create a bootstrap user with temporary administrator permissions \u00b6 Leverage needs a user with temporary administrator permissions in order to deploy the initial resources that will form the foundations you will then use to keep building on. That initial deployment is called the bootstrap process and thus the user required for that is called \"the bootstrap user\". To create that user, navigate to the IAM page and create a user named mgmt-org-admin following steps 2 and 3 of this leverage doc . Info Bear in mind that the page for creating users may change from time to time but the key set tings for configuring the bootstrap user are the following : * It must be an IAM user ( we won 't be using IAM Identity Center for this) * Password can be auto-generated * It requires admin privileges which you can achieve by directly attaching the `AdministratorAccess` policy to it * There' s no need to add the user to any group as it is only a temporary user Usually the last step of the user creation should present you the following information: Console sign-in URL User name Console password Make a note of all of these and keep them in a safe place as you will need them in the following steps. Info If you are only getting the bootstrap user credentials for someone else in your team or in Binbash 's team, then please share that using a secure way (e.g. password management service, GPG keys, etc). Info If user was set up with the option \"Force to change password on first login\" , you should log into the console to do so . Next steps \u00b6 You have successfully created and configured the AWS account for your Leverage project. From now on, almost all interactions with the AWS environment (with few notable exceptions) will be performed via Leverage. Next, you will setup all required dependencies to work on a Leverage project in your local machine.","title":"Creating your AWS Management account"},{"location":"resources/aws-account-setup/#creating-your-aws-management-account","text":"","title":"Creating your AWS Management account"},{"location":"resources/aws-account-setup/#create-the-first-aws-account","text":"First and foremost you'll need to create an AWS account for your project. Attention Note this will be your management account and has to be called <project-name>-management . E.g. if your project is called binbash then your account should be binbash-management . Follow the instructions here . This will be the management account for your AWS Organization and the email address you use for signing up will be the root user of this account -- you can see this user represented in the architecture diagram . Since the root user is the main access point to your account it is strongly recommended that you keep its credentials (email, password) safe by following AWS best practices . Tip To protect your management account, [enabling Multi Factor Authentication](https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html#id_root-user_manage_mfa) is **highly** encouraged. Also, reviewing the [account's billing setup](https://console.aws.amazon.com/billing/home?#/account) is always a good idea before proceeding. For more details on setting up your AWS account: Organization account setup guide","title":"Create the first AWS account"},{"location":"resources/aws-account-setup/#create-a-bootstrap-user-with-temporary-administrator-permissions","text":"Leverage needs a user with temporary administrator permissions in order to deploy the initial resources that will form the foundations you will then use to keep building on. That initial deployment is called the bootstrap process and thus the user required for that is called \"the bootstrap user\". To create that user, navigate to the IAM page and create a user named mgmt-org-admin following steps 2 and 3 of this leverage doc . Info Bear in mind that the page for creating users may change from time to time but the key set tings for configuring the bootstrap user are the following : * It must be an IAM user ( we won 't be using IAM Identity Center for this) * Password can be auto-generated * It requires admin privileges which you can achieve by directly attaching the `AdministratorAccess` policy to it * There' s no need to add the user to any group as it is only a temporary user Usually the last step of the user creation should present you the following information: Console sign-in URL User name Console password Make a note of all of these and keep them in a safe place as you will need them in the following steps. Info If you are only getting the bootstrap user credentials for someone else in your team or in Binbash 's team, then please share that using a secure way (e.g. password management service, GPG keys, etc). Info If user was set up with the option \"Force to change password on first login\" , you should log into the console to do so .","title":"Create a bootstrap user with temporary administrator permissions"},{"location":"resources/aws-account-setup/#next-steps","text":"You have successfully created and configured the AWS account for your Leverage project. From now on, almost all interactions with the AWS environment (with few notable exceptions) will be performed via Leverage. Next, you will setup all required dependencies to work on a Leverage project in your local machine.","title":"Next steps"},{"location":"resources/enabling-sso/","text":"Configure SSO settings \u00b6 Enable SSO \u00b6 Let's start by configuring SSO settings. Open this file: <your_project>/config/common.tfvars and update the following lines: sso_enabled = false sso_start_url = \"https://bbleverage.awsapps.com/start\" Change sso_enabled to true as follows to enable SSO support: sso_enabled = true Now you need to set the sso_start_url with the right URL. To find that, navigate here: https://us-east-1.console.aws.amazon.com/singlesignon/home -- you should be already logged in to the Management account for this to work. You should see a \"Settings summary\" panel on the right of the screen that shows the \"AWS access portal URL\". Copy that and use it to replace the value in the sso_start_url entry. Below is an example just for reference: sso_start_url = \"https://d-xyz01234567.awsapps.com/start\" Customize the AWS access portal URL The 'AWS access portal URL' can be customized to use a more friendly name. Check the official documentation for that. Further info on configuring SSO There is more information on how to configure SSO here . Update backend profiles in the management account \u00b6 It's time to set the right profile names in the backend configuration files. Open this file: management/config/backend.tfvars and change the profile value from this: profile = \"me-bootstrap\" To this: profile = \"me-management-oaar\" Please note that in the examples above my short project name is me which is used as a prefix and it's the part that doesn't get replaced. Activate your SSO user and set up your password \u00b6 The SSO users you created when you provisioned the SSO layer need to go through an email activation procedure. The user is the one you set in the project.yaml file at the beginning, in this snippet: users : - first_name : the-name last_name : the-last-name email : user@domain.co groups : - administrators - devops To activate the user find the instructions here . Once SSO user's have been activated, they will need to get their initial password so they are able to log in. Check out the steps for that here . Basically: Log into your sso_start_url address Ingress your username (the user email) Under Password, choose Forgot password. Type in the code shown in the screen A reset password email will be sent Follow the link and reset your password Now, in the same URL as before, log in with the new credentials You will be prompted to create an MFA, just do it. Configure the CLI for SSO \u00b6 Almost there. Let's try the SSO integration now. Configure your SSO profiles \u00b6 Since this is your first time using that you will need to configure it by running this: leverage aws configure sso Follow the wizard to get your AWS config file created for you. There is more info about that here . Verify on a layer in the management account \u00b6 To ensure that worked, let's run a few commands to verify: We'll use sso for the purpose of this example Move to the management/global/sso layer Run: leverage tf plan You should get this error: \"Error: error configuring S3 Backend: no valid credential sources for S3 Backend found.\" This happens because so far you have been running Terraform with a different AWS profile (the bootstrap one). Luckily the fix is simple, just run this: leverage tf init -reconfigure . Terraform should reconfigure the AWS profile in the .terraform/terraform.tfstate file. Now try running that leverage tf plan command again This time it should succeed, you should see the message: No changes. Your infrastructure matches the configuration. Note if you still have the same error, try clearing credentials with: leverage aws sso logout && leverage aws sso login Next steps \u00b6 You successfully enabled SSO. Next, you will orchestrate the remaining accounts, security and shared .","title":"Configure SSO settings"},{"location":"resources/enabling-sso/#configure-sso-settings","text":"","title":"Configure SSO settings"},{"location":"resources/enabling-sso/#enable-sso","text":"Let's start by configuring SSO settings. Open this file: <your_project>/config/common.tfvars and update the following lines: sso_enabled = false sso_start_url = \"https://bbleverage.awsapps.com/start\" Change sso_enabled to true as follows to enable SSO support: sso_enabled = true Now you need to set the sso_start_url with the right URL. To find that, navigate here: https://us-east-1.console.aws.amazon.com/singlesignon/home -- you should be already logged in to the Management account for this to work. You should see a \"Settings summary\" panel on the right of the screen that shows the \"AWS access portal URL\". Copy that and use it to replace the value in the sso_start_url entry. Below is an example just for reference: sso_start_url = \"https://d-xyz01234567.awsapps.com/start\" Customize the AWS access portal URL The 'AWS access portal URL' can be customized to use a more friendly name. Check the official documentation for that. Further info on configuring SSO There is more information on how to configure SSO here .","title":"Enable SSO"},{"location":"resources/enabling-sso/#update-backend-profiles-in-the-management-account","text":"It's time to set the right profile names in the backend configuration files. Open this file: management/config/backend.tfvars and change the profile value from this: profile = \"me-bootstrap\" To this: profile = \"me-management-oaar\" Please note that in the examples above my short project name is me which is used as a prefix and it's the part that doesn't get replaced.","title":"Update backend profiles in the management account"},{"location":"resources/enabling-sso/#activate-your-sso-user-and-set-up-your-password","text":"The SSO users you created when you provisioned the SSO layer need to go through an email activation procedure. The user is the one you set in the project.yaml file at the beginning, in this snippet: users : - first_name : the-name last_name : the-last-name email : user@domain.co groups : - administrators - devops To activate the user find the instructions here . Once SSO user's have been activated, they will need to get their initial password so they are able to log in. Check out the steps for that here . Basically: Log into your sso_start_url address Ingress your username (the user email) Under Password, choose Forgot password. Type in the code shown in the screen A reset password email will be sent Follow the link and reset your password Now, in the same URL as before, log in with the new credentials You will be prompted to create an MFA, just do it.","title":"Activate your SSO user and set up your password"},{"location":"resources/enabling-sso/#configure-the-cli-for-sso","text":"Almost there. Let's try the SSO integration now.","title":"Configure the CLI for SSO"},{"location":"resources/enabling-sso/#configure-your-sso-profiles","text":"Since this is your first time using that you will need to configure it by running this: leverage aws configure sso Follow the wizard to get your AWS config file created for you. There is more info about that here .","title":"Configure your SSO profiles"},{"location":"resources/enabling-sso/#verify-on-a-layer-in-the-management-account","text":"To ensure that worked, let's run a few commands to verify: We'll use sso for the purpose of this example Move to the management/global/sso layer Run: leverage tf plan You should get this error: \"Error: error configuring S3 Backend: no valid credential sources for S3 Backend found.\" This happens because so far you have been running Terraform with a different AWS profile (the bootstrap one). Luckily the fix is simple, just run this: leverage tf init -reconfigure . Terraform should reconfigure the AWS profile in the .terraform/terraform.tfstate file. Now try running that leverage tf plan command again This time it should succeed, you should see the message: No changes. Your infrastructure matches the configuration. Note if you still have the same error, try clearing credentials with: leverage aws sso logout && leverage aws sso login","title":"Verify on a layer in the management account"},{"location":"resources/enabling-sso/#next-steps","text":"You successfully enabled SSO. Next, you will orchestrate the remaining accounts, security and shared .","title":"Next steps"},{"location":"resources/leverage-project-setup/","text":"Create a Leverage project \u00b6 A Leverage project starts with a simple project definition file that you modify to suit your needs. That file is then used to render the initial directory layout which, at the end of this guide, will be your reference architecture. Follow the sections below to begin with that. The account's name will be given by your project's name followed by -management , since Leverage uses a suffix naming system to differentiate between the multiple accounts of a project. For this guide we'll stick to calling the project MyExample and so, the account name will be myexample-management . Along the same line, we'll use the example.com domain for the email address used to register the account. Adding a -aws suffix to the project's name to indicate that this email address is related to the project's AWS account, we end up with a registration email that looks like myexample-aws@example.com . Email addresses for AWS accounts. Each AWS account requires having a unique email address associated to it . The Leverage Reference Architecture for AWS makes use of multiple accounts to better manage the infrastructure , as such , you will need different addresses for each one . Creating a new email account for each AWS is not a really viable solution to this problem , a better approach is to take advantage of mail services that support aliases . For information regarding how this works : [: books : Email setup for your AWS account .]( / user - guide / ref - architecture - aws / features / organization / configuration / # pre - requisites ) Create the project directory \u00b6 Each Leverage project lives in its own working directory. Create a directory for your project as follows: mkdir myexample cd myexample Initialize the project \u00b6 Create the project definition file by running the following command: $ leverage project init [ 18 :53:24.407 ] INFO Project template found. Updating. [ 18 :53:25.105 ] INFO Finished updating template. [ 18 :53:25.107 ] INFO Initializing git repository in project directory. [ 18 :53:25.139 ] INFO No project configuration file found. Dropping configuration template project.yaml. [ 18 :53:25.143 ] INFO Project initialization finished. The command above should create the project definition file ( project.yaml ) and should initialize a git repository in the current working directory. This is important because Leverage projects by-design rely on specific git conventions and also because it is assumed that you will want to keep your infrastructure code versioned. Modify the project definition file \u00b6 Open the project.yaml file and fill in the required information. Typically the placeholder values between < and > symbols are the ones you would want to edit however you are welcome to adjust any other values to suit your needs. For instance, the following is a snippet of the project.yaml file in which the values for project_name and short_name have been set to example and ex respectively: project_name: example short_name: ex primary_region: us-east-1 secondary_region: us-west-2 ... The project_name field only accepts lowercase alphanumeric characters and allows hyphens('-'). For instance, valid names could be 'example' or 'leveragedemo' or 'example-demo' The short_name field only accepts 2 to 4 lowercase alpha characters. For instance, valid names could be 'exam or 'leve or 'ex We typically use as 1ry us-east-1 and 2ry us-west-2 as our default regions for the majority of our projects. However, please note that these regions may not be the most fitting choice for your specific use case. For detailed guidance, we recommend following these provided guidelines . Another example is below. Note that the management , security , and shared accounts have been updated with slightly different email addresses (actually aws+security@example.com and aws+shared@example.com are email aliases of aws@example.com which is a convenient trick in some cases): ... organization: accounts: - name: management email: aws@example.com - name: security email: aws+security@example.com - name: shared email: aws+shared@example.com ... Finally, here's another example snippet that shows how you can define users and assign them to groups: ... users: - first_name: Jane last_name: Doe email: jane.doe@example.com groups: - administrators - devops - first_name: Foo last_name: Bar email: foo.bar@example.com groups: - devops ... Note these users will be the ones used later for SSO access. The project definition file includes other entries but the ones shown above are the most frequently updated. Configure \"bootstrap\" credentials \u00b6 To be able to interact with your AWS environment you first need to configure the credentials to enable AWS CLI to do so. Provide the keys obtained in the previous account creation step to the command by any of the available means. Manually File selection Provide file in command leverage credentials configure --type BOOTSTRAP [09:37:17.530] INFO Loading configuration file. [09:37:18.477] INFO Loading project environment configuration file. [09:37:20.426] INFO Configuring bootstrap credentials. > Select the means by which you'll provide the programmatic keys: Manually > Key: AKIAU1OF18IXH2EXAMPLE > Secret: **************************************** [09:37:51.638] INFO Bootstrap credentials configured in: /home/user/.aws/me/credentials [09:37:53.497] INFO Fetching management account id. [09:37:53.792] INFO Updating project configuration file. [09:37:55.344] INFO Skipping assumable roles configuration. leverage credentials configure --type BOOTSTRAP [09:37:17.530] INFO Loading configuration file. [09:37:18.477] INFO Loading project environment configuration file. [09:37:20.426] INFO Configuring bootstrap credentials. > Select the means by which you'll provide the programmatic keys: Path to an access keys file obtained from AWS > Path to access keys file: ../bootstrap_accessKeys.csv [09:37:51.638] INFO Bootstrap credentials configured in: /home/user/.aws/me/credentials [09:37:53.497] INFO Fetching management account id. [09:37:53.792] INFO Updating project configuration file. [09:37:55.344] INFO Skipping assumable roles configuration. leverage credentials configure --type BOOTSTRAP --credentials-file ../bootstrap_accessKeys.csv [09:37:17.530] INFO Loading configuration file. [09:37:18.477] INFO Loading project environment configuration file. [09:37:20.426] INFO Configuring bootstrap credentials. [09:37:51.638] INFO Bootstrap credentials configured in: /home/user/.aws/me/credentials [09:37:53.497] INFO Fetching management account id. [09:37:53.792] INFO Updating project configuration file. [09:37:55.344] INFO Skipping assumable roles configuration. More information on credentials configure During the credentials setup, the AWS account id is filled in for us in the project configuration file. ... organization : accounts : - name : management email : myexample-aws@example.com id : '000123456789' ... Create the configured project \u00b6 Now you will finally create all the infrastructure definition in the project. leverage project create [09:40:54.934] INFO Loading configuration file. [09:40:54.950] INFO Creating project directory structure. [09:40:54.957] INFO Finished creating directory structure. [09:40:54.958] INFO Setting up common base files. [09:40:54.964] INFO Account: Setting up management . [09:40:54.965] INFO Layer: Setting up config . [09:40:54.968] INFO Layer: Setting up base-tf-backend . [09:40:54.969] INFO Layer: Setting up base-identities . [09:40:54.984] INFO Layer: Setting up organizations . [09:40:54.989] INFO Layer: Setting up security-base . [09:40:54.990] INFO Account: Setting up security . [09:40:54.991] INFO Layer: Setting up config . [09:40:54.994] INFO Layer: Setting up base-tf-backend . [09:40:54.995] INFO Layer: Setting up base-identities . [09:40:55.001] INFO Layer: Setting up security-base . [09:40:55.002] INFO Account: Setting up shared . [09:40:55.003] INFO Layer: Setting up config . [09:40:55.006] INFO Layer: Setting up base-tf-backend . [09:40:55.007] INFO Layer: Setting up base-identities . [09:40:55.008] INFO Layer: Setting up security-base . [09:40:55.009] INFO Layer: Setting up base-network . [09:40:55.013] INFO Project configuration finished. INFO Reformatting terraform configuration to the standard style. [09:40:55.743] INFO Finished setting up project. More information on project create In this step, the directory structure for the project and all definition files are created using the information from the project.yaml file and checked for correct formatting. You will end up with something that looks like this: MyExample project file structure \ud83d\udcc2 myexample \u251c\u2500\u2500 \ud83d\udcc4 build.env \u251c\u2500\u2500 \ud83d\udcc4 project.yaml \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2514\u2500\u2500 \ud83d\udcc4 common.tfvars \u251c\u2500\u2500 \ud83d\udcc2 management \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars | \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 organizations | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 accounts.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 delegated_administrator.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 organizational_units.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 organization.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 policies_scp.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 policy_scp_attachments.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 service_linked_roles.tf | \u2502 \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 groups.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 keys | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 outputs.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 roles.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 users.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 main.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2514\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u251c\u2500\u2500 \ud83d\udcc2 security \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | | \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 groups_policies.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 groups.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 keys | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 outputs.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 role_policies.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 roles.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 users.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 main.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2514\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u251c\u2500\u2500 \ud83d\udcc4 iam_access_analyzer.tf | \u251c\u2500\u2500 \ud83d\udcc4 locals.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2514\u2500\u2500 \ud83d\udcc2 shared \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u251c\u2500\u2500 \ud83d\udcc2 global | \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u251c\u2500\u2500 \ud83d\udcc4 policies.tf | \u251c\u2500\u2500 \ud83d\udcc4 roles.tf | \u251c\u2500\u2500 \ud83d\udcc4 service_linked_roles.tf | \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 \u251c\u2500\u2500 \ud83d\udcc2 base-network \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 network.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 network_vpc_flow_logs.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 outputs.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 main.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2514\u2500\u2500 \ud83d\udcc2 security-base \u251c\u2500\u2500 \ud83d\udcc4 account.tf \u251c\u2500\u2500 \ud83d\udcc4 config.tf \u2514\u2500\u2500 \ud83d\udcc4 variables.tf As you can see, it is a structure comprised of directories for each account containing all the definitions for each of the accounts respective layers. The layers themselves are also grouped based on the region in which they are deployed. The regions are configured through the project.yaml file. In the case of the Leverage landing zone, most layers are deployed in the primary region, so you can see the definition of these layers in a us-east-1 directory, as per the example configuration. Some layers are not bound to a region because their definition is mainly comprised of resources for services that are global in nature, like IAM or Organizations. These kind of layers are kept in a global directory. Next steps \u00b6 You have now created the definition of all the infrastructure for your project and configured the credentials need to deploy such infrastructure in the AWS environment. Next, you will orchestrate the first and main account of the project, the management account.","title":"Create a Leverage project"},{"location":"resources/leverage-project-setup/#create-a-leverage-project","text":"A Leverage project starts with a simple project definition file that you modify to suit your needs. That file is then used to render the initial directory layout which, at the end of this guide, will be your reference architecture. Follow the sections below to begin with that. The account's name will be given by your project's name followed by -management , since Leverage uses a suffix naming system to differentiate between the multiple accounts of a project. For this guide we'll stick to calling the project MyExample and so, the account name will be myexample-management . Along the same line, we'll use the example.com domain for the email address used to register the account. Adding a -aws suffix to the project's name to indicate that this email address is related to the project's AWS account, we end up with a registration email that looks like myexample-aws@example.com . Email addresses for AWS accounts. Each AWS account requires having a unique email address associated to it . The Leverage Reference Architecture for AWS makes use of multiple accounts to better manage the infrastructure , as such , you will need different addresses for each one . Creating a new email account for each AWS is not a really viable solution to this problem , a better approach is to take advantage of mail services that support aliases . For information regarding how this works : [: books : Email setup for your AWS account .]( / user - guide / ref - architecture - aws / features / organization / configuration / # pre - requisites )","title":"Create a Leverage project"},{"location":"resources/leverage-project-setup/#create-the-project-directory","text":"Each Leverage project lives in its own working directory. Create a directory for your project as follows: mkdir myexample cd myexample","title":"Create the project directory"},{"location":"resources/leverage-project-setup/#initialize-the-project","text":"Create the project definition file by running the following command: $ leverage project init [ 18 :53:24.407 ] INFO Project template found. Updating. [ 18 :53:25.105 ] INFO Finished updating template. [ 18 :53:25.107 ] INFO Initializing git repository in project directory. [ 18 :53:25.139 ] INFO No project configuration file found. Dropping configuration template project.yaml. [ 18 :53:25.143 ] INFO Project initialization finished. The command above should create the project definition file ( project.yaml ) and should initialize a git repository in the current working directory. This is important because Leverage projects by-design rely on specific git conventions and also because it is assumed that you will want to keep your infrastructure code versioned.","title":"Initialize the project"},{"location":"resources/leverage-project-setup/#modify-the-project-definition-file","text":"Open the project.yaml file and fill in the required information. Typically the placeholder values between < and > symbols are the ones you would want to edit however you are welcome to adjust any other values to suit your needs. For instance, the following is a snippet of the project.yaml file in which the values for project_name and short_name have been set to example and ex respectively: project_name: example short_name: ex primary_region: us-east-1 secondary_region: us-west-2 ... The project_name field only accepts lowercase alphanumeric characters and allows hyphens('-'). For instance, valid names could be 'example' or 'leveragedemo' or 'example-demo' The short_name field only accepts 2 to 4 lowercase alpha characters. For instance, valid names could be 'exam or 'leve or 'ex We typically use as 1ry us-east-1 and 2ry us-west-2 as our default regions for the majority of our projects. However, please note that these regions may not be the most fitting choice for your specific use case. For detailed guidance, we recommend following these provided guidelines . Another example is below. Note that the management , security , and shared accounts have been updated with slightly different email addresses (actually aws+security@example.com and aws+shared@example.com are email aliases of aws@example.com which is a convenient trick in some cases): ... organization: accounts: - name: management email: aws@example.com - name: security email: aws+security@example.com - name: shared email: aws+shared@example.com ... Finally, here's another example snippet that shows how you can define users and assign them to groups: ... users: - first_name: Jane last_name: Doe email: jane.doe@example.com groups: - administrators - devops - first_name: Foo last_name: Bar email: foo.bar@example.com groups: - devops ... Note these users will be the ones used later for SSO access. The project definition file includes other entries but the ones shown above are the most frequently updated.","title":"Modify the project definition file"},{"location":"resources/leverage-project-setup/#configure-bootstrap-credentials","text":"To be able to interact with your AWS environment you first need to configure the credentials to enable AWS CLI to do so. Provide the keys obtained in the previous account creation step to the command by any of the available means. Manually File selection Provide file in command leverage credentials configure --type BOOTSTRAP [09:37:17.530] INFO Loading configuration file. [09:37:18.477] INFO Loading project environment configuration file. [09:37:20.426] INFO Configuring bootstrap credentials. > Select the means by which you'll provide the programmatic keys: Manually > Key: AKIAU1OF18IXH2EXAMPLE > Secret: **************************************** [09:37:51.638] INFO Bootstrap credentials configured in: /home/user/.aws/me/credentials [09:37:53.497] INFO Fetching management account id. [09:37:53.792] INFO Updating project configuration file. [09:37:55.344] INFO Skipping assumable roles configuration. leverage credentials configure --type BOOTSTRAP [09:37:17.530] INFO Loading configuration file. [09:37:18.477] INFO Loading project environment configuration file. [09:37:20.426] INFO Configuring bootstrap credentials. > Select the means by which you'll provide the programmatic keys: Path to an access keys file obtained from AWS > Path to access keys file: ../bootstrap_accessKeys.csv [09:37:51.638] INFO Bootstrap credentials configured in: /home/user/.aws/me/credentials [09:37:53.497] INFO Fetching management account id. [09:37:53.792] INFO Updating project configuration file. [09:37:55.344] INFO Skipping assumable roles configuration. leverage credentials configure --type BOOTSTRAP --credentials-file ../bootstrap_accessKeys.csv [09:37:17.530] INFO Loading configuration file. [09:37:18.477] INFO Loading project environment configuration file. [09:37:20.426] INFO Configuring bootstrap credentials. [09:37:51.638] INFO Bootstrap credentials configured in: /home/user/.aws/me/credentials [09:37:53.497] INFO Fetching management account id. [09:37:53.792] INFO Updating project configuration file. [09:37:55.344] INFO Skipping assumable roles configuration. More information on credentials configure During the credentials setup, the AWS account id is filled in for us in the project configuration file. ... organization : accounts : - name : management email : myexample-aws@example.com id : '000123456789' ...","title":"Configure \"bootstrap\" credentials"},{"location":"resources/leverage-project-setup/#create-the-configured-project","text":"Now you will finally create all the infrastructure definition in the project. leverage project create [09:40:54.934] INFO Loading configuration file. [09:40:54.950] INFO Creating project directory structure. [09:40:54.957] INFO Finished creating directory structure. [09:40:54.958] INFO Setting up common base files. [09:40:54.964] INFO Account: Setting up management . [09:40:54.965] INFO Layer: Setting up config . [09:40:54.968] INFO Layer: Setting up base-tf-backend . [09:40:54.969] INFO Layer: Setting up base-identities . [09:40:54.984] INFO Layer: Setting up organizations . [09:40:54.989] INFO Layer: Setting up security-base . [09:40:54.990] INFO Account: Setting up security . [09:40:54.991] INFO Layer: Setting up config . [09:40:54.994] INFO Layer: Setting up base-tf-backend . [09:40:54.995] INFO Layer: Setting up base-identities . [09:40:55.001] INFO Layer: Setting up security-base . [09:40:55.002] INFO Account: Setting up shared . [09:40:55.003] INFO Layer: Setting up config . [09:40:55.006] INFO Layer: Setting up base-tf-backend . [09:40:55.007] INFO Layer: Setting up base-identities . [09:40:55.008] INFO Layer: Setting up security-base . [09:40:55.009] INFO Layer: Setting up base-network . [09:40:55.013] INFO Project configuration finished. INFO Reformatting terraform configuration to the standard style. [09:40:55.743] INFO Finished setting up project. More information on project create In this step, the directory structure for the project and all definition files are created using the information from the project.yaml file and checked for correct formatting. You will end up with something that looks like this: MyExample project file structure \ud83d\udcc2 myexample \u251c\u2500\u2500 \ud83d\udcc4 build.env \u251c\u2500\u2500 \ud83d\udcc4 project.yaml \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2514\u2500\u2500 \ud83d\udcc4 common.tfvars \u251c\u2500\u2500 \ud83d\udcc2 management \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars | \u251c\u2500\u2500 \ud83d\udcc2 global | \u2502 \u251c\u2500\u2500 \ud83d\udcc2 organizations | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 accounts.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 delegated_administrator.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 organizational_units.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 organization.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 policies_scp.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 policy_scp_attachments.tf | \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 service_linked_roles.tf | \u2502 \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 groups.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 keys | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 outputs.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 roles.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 users.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 main.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2514\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u251c\u2500\u2500 \ud83d\udcc2 security \u2502 \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u2502 \u251c\u2500\u2500 \ud83d\udcc2 global | | \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 groups_policies.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 groups.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 keys | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 outputs.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 role_policies.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 roles.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 users.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 | \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u2502 \u251c\u2500\u2500 \ud83d\udcc4 main.tf | \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf | \u2514\u2500\u2500 \ud83d\udcc2 security-base | \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u251c\u2500\u2500 \ud83d\udcc4 iam_access_analyzer.tf | \u251c\u2500\u2500 \ud83d\udcc4 locals.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2514\u2500\u2500 \ud83d\udcc2 shared \u251c\u2500\u2500 \ud83d\udcc2 config \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tfvars \u2502 \u2514\u2500\u2500 \ud83d\udcc4 backend.tfvars \u251c\u2500\u2500 \ud83d\udcc2 global | \u2514\u2500\u2500 \ud83d\udcc2 base-identities | \u251c\u2500\u2500 \ud83d\udcc4 account.tf | \u251c\u2500\u2500 \ud83d\udcc4 config.tf | \u251c\u2500\u2500 \ud83d\udcc4 locals.tf | \u251c\u2500\u2500 \ud83d\udcc4 policies.tf | \u251c\u2500\u2500 \ud83d\udcc4 roles.tf | \u251c\u2500\u2500 \ud83d\udcc4 service_linked_roles.tf | \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2514\u2500\u2500 \ud83d\udcc2 us-east-1 \u251c\u2500\u2500 \ud83d\udcc2 base-network \u2502 \u251c\u2500\u2500 \ud83d\udcc4 account.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 network.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 network_vpc_flow_logs.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 outputs.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u251c\u2500\u2500 \ud83d\udcc2 base-tf-backend \u2502 \u251c\u2500\u2500 \ud83d\udcc4 config.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 locals.tf \u2502 \u251c\u2500\u2500 \ud83d\udcc4 main.tf \u2502 \u2514\u2500\u2500 \ud83d\udcc4 variables.tf \u2514\u2500\u2500 \ud83d\udcc2 security-base \u251c\u2500\u2500 \ud83d\udcc4 account.tf \u251c\u2500\u2500 \ud83d\udcc4 config.tf \u2514\u2500\u2500 \ud83d\udcc4 variables.tf As you can see, it is a structure comprised of directories for each account containing all the definitions for each of the accounts respective layers. The layers themselves are also grouped based on the region in which they are deployed. The regions are configured through the project.yaml file. In the case of the Leverage landing zone, most layers are deployed in the primary region, so you can see the definition of these layers in a us-east-1 directory, as per the example configuration. Some layers are not bound to a region because their definition is mainly comprised of resources for services that are global in nature, like IAM or Organizations. These kind of layers are kept in a global directory.","title":"Create the configured project"},{"location":"resources/leverage-project-setup/#next-steps","text":"You have now created the definition of all the infrastructure for your project and configured the credentials need to deploy such infrastructure in the AWS environment. Next, you will orchestrate the first and main account of the project, the management account.","title":"Next steps"},{"location":"resources/local-setup/","text":"Install Leverage CLI \u00b6 Leverage-based projects are better managed via the Leverage CLI which is a companion tool that simplifies your daily interactions with Leverage. This page will guide you through the installation steps. Prerequisites \u00b6 In order to install the CLI you should have the following installed in your system: Git Python 3 version 3.8 and up Docker Install Leverage CLI \u00b6 Leverage CLI is distributed as a python package that you can install it via pip as follows: pip install leverage For further details on installing Leverage CLI: Install Leverage CLI Verify your Leverage CLI installation \u00b6 Verify that your Leverage CLI installation was successful by running the following command: $ leverage --version leverage, version 1 .9.2 It is generally recommended to install the latest stable version of the CLI Enable tab completion \u00b6 If you use Bash, Zsh or Fish, you can enable shell completion for Leverage commands. Bash Zsh Fish Add to ~/.bashrc : eval \" $( _LEVERAGE_COMPLETE = bash_source leverage ) \" Add to ~/.zshrc : eval \" $( _LEVERAGE_COMPLETE = zsh_source leverage ) \" Add to ~/.config/fish/completions/leverage.fish : eval ( env _LEVERAGE_COMPLETE = fish_source leverage ) Now you need to restart your shell. Next steps \u00b6 Now you have your system completely configured to work on a Leverage project. Next, you will setup and create your Leverage project.","title":"Install Leverage CLI"},{"location":"resources/local-setup/#install-leverage-cli","text":"Leverage-based projects are better managed via the Leverage CLI which is a companion tool that simplifies your daily interactions with Leverage. This page will guide you through the installation steps.","title":"Install Leverage CLI"},{"location":"resources/local-setup/#prerequisites","text":"In order to install the CLI you should have the following installed in your system: Git Python 3 version 3.8 and up Docker","title":"Prerequisites"},{"location":"resources/local-setup/#install-leverage-cli_1","text":"Leverage CLI is distributed as a python package that you can install it via pip as follows: pip install leverage For further details on installing Leverage CLI: Install Leverage CLI","title":"Install Leverage CLI"},{"location":"resources/local-setup/#verify-your-leverage-cli-installation","text":"Verify that your Leverage CLI installation was successful by running the following command: $ leverage --version leverage, version 1 .9.2 It is generally recommended to install the latest stable version of the CLI","title":"Verify your Leverage CLI installation"},{"location":"resources/local-setup/#enable-tab-completion","text":"If you use Bash, Zsh or Fish, you can enable shell completion for Leverage commands. Bash Zsh Fish Add to ~/.bashrc : eval \" $( _LEVERAGE_COMPLETE = bash_source leverage ) \" Add to ~/.zshrc : eval \" $( _LEVERAGE_COMPLETE = zsh_source leverage ) \" Add to ~/.config/fish/completions/leverage.fish : eval ( env _LEVERAGE_COMPLETE = fish_source leverage ) Now you need to restart your shell.","title":"Enable tab completion"},{"location":"resources/local-setup/#next-steps","text":"Now you have your system completely configured to work on a Leverage project. Next, you will setup and create your Leverage project.","title":"Next steps"},{"location":"resources/management-account/","text":"Configure the Management account \u00b6 Finally we reach the point in which you'll get to actually create the infrastructure in our AWS environment. Some accounts and layers rely on other accounts or layers to be deployed first, which creates dependencies between them and establishes an order in which all layers should be deployed. We will go through these dependencies in order. The management account is used to configure and access all the accounts in the AWS Organization. Consolidated Billing and Cost Management are also enforced though this account. Costs associated with this solution By default this AWS Reference Architecture configuration should not incur in any costs. Deploy the Management account's layers \u00b6 To begin, place yourself in the management account directory. cd management Terraform backend layer \u00b6 Move into the us-east-1/base-tf-backend directory and run: leverage terraform init --skip-validation leverage terraform apply All apply commands will prompt for confirmation, answer yes when this happens. More information on terraform init and terraform apply Now, the infrastructure for the Terraform state management is created. The next step is to push the local .tfstate to the bucket. To do this, uncomment the backend section for the terraform configuration in management/base-tf-backend/config.tf backend \"s3\" { key = \"management/tf-backend/terraform.tfstate\" } And run once more: leverage terraform init When prompted, answer yes . Now you can safely remove the terraform.tfstate and terraform.tfstate.backup files created during the apply step. Terraform backend More information regarding what is the Terraform backend and Terraform state management: Terraform backend How to manage Terraform state Organizations layer \u00b6 Next, in the same fashion as in the previous layer, move into the global/organizations directory and run: leverage terraform init leverage terraform apply The AWS account that you created manually is the management account itself, so to prevent Terraform from trying to create it and error out, this account definition is commented by default in the code. Now you need to make the Terraform state aware of the link between the two. To do that, uncomment the management organizations account resource in accounts.tf resource \"aws_organizations_account\" \"management\" { name = \"${var.project_long}-management\" email = local.management_account.email } Grab the management account id that previously was automatically filled in for us in the project.yaml file ... organization : accounts : - name : management email : myexample-aws@example.com id : '000123456789' ... And run: leverage terraform import aws_organizations_account.management 000123456789 More information on terraform import Getting errors with zsh? Zsh users may need to prepend noglob to the import command for it to be recognized correctly, as an alternative, square brackets can be escaped as \\[\\] Security layer \u00b6 Change directory to us-east-1/security-base and run this: leverage terraform init leverage terraform apply Update the bootstrap credentials \u00b6 Now that the management account has been deployed, and more specifically, all Organizations accounts have been created (in the organizations layer ) you need to update the credentials for the bootstrap process before proceeding to deploy any of the remaining accounts. This will fetch the organizations structure from the AWS environment and create individual profiles associated with each account for the AWS CLI to use. So, run: $ leverage credentials configure --type BOOTSTRAP --skip-access-keys-setup [ 09 :08:44.762 ] INFO Loading configuration file. [ 09 :08:44.785 ] Loading project environment configuration file. [ 09 :08:44.791 ] Loading Terraform common configuration. [ 09 :08:53.247 ] Configuring assumable roles. [ 09 :08:53.248 ] Fetching organization accounts. [ 09 :08:55.193 ] Backing up account profiles file. [ 09 :08:55.761 ] Configuring profile me-management-oaar [ 09 :08:59.977 ] Configuring profile me-security-oaar [ 09 :09:04.081 ] Configuring profile me-shared-oaar [ 09 :09:08.305 ] Account profiles configured in : /home/user/.aws/me/config [ 09 :09:08.307 ] INFO Updating project ' s Terraform common configuration. More information on credentials configure SSO layer \u00b6 Before working on the SSO layer you have to navigate to the AWS IAM Identity Center page , set the region to the primary region you've chosen and enable Single Sign-On (SSO) by clicking on the Enable button. Now back to the terminal. The SSO layer is deployed in two steps. First, switch to the global/sso directory and run the following: leverage terraform init leverage terraform apply Secondly, open the account_assignments.tf file and uncomment the entire section that starts with this line: # module \"account_assignments\" { # source = \"github.com/binbashar/terraform-aws-sso.git//modules/account-assignments?ref=0.7.1\" [REDACTED] # ] # } After that, run these commands: leverage terraform init leverage terraform apply Next steps \u00b6 You have successfully orchestrated the management account for your project and configured the credentials for the next steps. Now, let's enable SSO for the rest of the process.","title":"Configure the Management account"},{"location":"resources/management-account/#configure-the-management-account","text":"Finally we reach the point in which you'll get to actually create the infrastructure in our AWS environment. Some accounts and layers rely on other accounts or layers to be deployed first, which creates dependencies between them and establishes an order in which all layers should be deployed. We will go through these dependencies in order. The management account is used to configure and access all the accounts in the AWS Organization. Consolidated Billing and Cost Management are also enforced though this account. Costs associated with this solution By default this AWS Reference Architecture configuration should not incur in any costs.","title":"Configure the Management account"},{"location":"resources/management-account/#deploy-the-management-accounts-layers","text":"To begin, place yourself in the management account directory. cd management","title":"Deploy the Management account's layers"},{"location":"resources/management-account/#terraform-backend-layer","text":"Move into the us-east-1/base-tf-backend directory and run: leverage terraform init --skip-validation leverage terraform apply All apply commands will prompt for confirmation, answer yes when this happens. More information on terraform init and terraform apply Now, the infrastructure for the Terraform state management is created. The next step is to push the local .tfstate to the bucket. To do this, uncomment the backend section for the terraform configuration in management/base-tf-backend/config.tf backend \"s3\" { key = \"management/tf-backend/terraform.tfstate\" } And run once more: leverage terraform init When prompted, answer yes . Now you can safely remove the terraform.tfstate and terraform.tfstate.backup files created during the apply step. Terraform backend More information regarding what is the Terraform backend and Terraform state management: Terraform backend How to manage Terraform state","title":"Terraform backend layer"},{"location":"resources/management-account/#organizations-layer","text":"Next, in the same fashion as in the previous layer, move into the global/organizations directory and run: leverage terraform init leverage terraform apply The AWS account that you created manually is the management account itself, so to prevent Terraform from trying to create it and error out, this account definition is commented by default in the code. Now you need to make the Terraform state aware of the link between the two. To do that, uncomment the management organizations account resource in accounts.tf resource \"aws_organizations_account\" \"management\" { name = \"${var.project_long}-management\" email = local.management_account.email } Grab the management account id that previously was automatically filled in for us in the project.yaml file ... organization : accounts : - name : management email : myexample-aws@example.com id : '000123456789' ... And run: leverage terraform import aws_organizations_account.management 000123456789 More information on terraform import Getting errors with zsh? Zsh users may need to prepend noglob to the import command for it to be recognized correctly, as an alternative, square brackets can be escaped as \\[\\]","title":"Organizations layer"},{"location":"resources/management-account/#security-layer","text":"Change directory to us-east-1/security-base and run this: leverage terraform init leverage terraform apply","title":"Security layer"},{"location":"resources/management-account/#update-the-bootstrap-credentials","text":"Now that the management account has been deployed, and more specifically, all Organizations accounts have been created (in the organizations layer ) you need to update the credentials for the bootstrap process before proceeding to deploy any of the remaining accounts. This will fetch the organizations structure from the AWS environment and create individual profiles associated with each account for the AWS CLI to use. So, run: $ leverage credentials configure --type BOOTSTRAP --skip-access-keys-setup [ 09 :08:44.762 ] INFO Loading configuration file. [ 09 :08:44.785 ] Loading project environment configuration file. [ 09 :08:44.791 ] Loading Terraform common configuration. [ 09 :08:53.247 ] Configuring assumable roles. [ 09 :08:53.248 ] Fetching organization accounts. [ 09 :08:55.193 ] Backing up account profiles file. [ 09 :08:55.761 ] Configuring profile me-management-oaar [ 09 :08:59.977 ] Configuring profile me-security-oaar [ 09 :09:04.081 ] Configuring profile me-shared-oaar [ 09 :09:08.305 ] Account profiles configured in : /home/user/.aws/me/config [ 09 :09:08.307 ] INFO Updating project ' s Terraform common configuration. More information on credentials configure","title":"Update the bootstrap credentials"},{"location":"resources/management-account/#sso-layer","text":"Before working on the SSO layer you have to navigate to the AWS IAM Identity Center page , set the region to the primary region you've chosen and enable Single Sign-On (SSO) by clicking on the Enable button. Now back to the terminal. The SSO layer is deployed in two steps. First, switch to the global/sso directory and run the following: leverage terraform init leverage terraform apply Secondly, open the account_assignments.tf file and uncomment the entire section that starts with this line: # module \"account_assignments\" { # source = \"github.com/binbashar/terraform-aws-sso.git//modules/account-assignments?ref=0.7.1\" [REDACTED] # ] # } After that, run these commands: leverage terraform init leverage terraform apply","title":"SSO layer"},{"location":"resources/management-account/#next-steps","text":"You have successfully orchestrated the management account for your project and configured the credentials for the next steps. Now, let's enable SSO for the rest of the process.","title":"Next steps"},{"location":"resources/post-deployment/","text":"Post-deployment steps \u00b6 At this point the landing zone should be ready. The bootstrap user can now be deleted. Delete the bootstrap user \u00b6 Log into your sso_start_url address with your SSO user Select the management account and log into the Management console Go to IAM Delete the user mgmt-org-admin Adding SSO users and groups \u00b6 To add users or groups, please see SSO Managing Users document . Next steps \u00b6 Now you not only have a fully functional landing zone configuration deployed, but also are able to interact with it using your own AWS SSO credentials. For more detailed information on the binbash Leverage Landing Zone , visit the links below. How it works User guide","title":"Post-deployment steps"},{"location":"resources/post-deployment/#post-deployment-steps","text":"At this point the landing zone should be ready. The bootstrap user can now be deleted.","title":"Post-deployment steps"},{"location":"resources/post-deployment/#delete-the-bootstrap-user","text":"Log into your sso_start_url address with your SSO user Select the management account and log into the Management console Go to IAM Delete the user mgmt-org-admin","title":"Delete the bootstrap user"},{"location":"resources/post-deployment/#adding-sso-users-and-groups","text":"To add users or groups, please see SSO Managing Users document .","title":"Adding SSO users and groups"},{"location":"resources/post-deployment/#next-steps","text":"Now you not only have a fully functional landing zone configuration deployed, but also are able to interact with it using your own AWS SSO credentials. For more detailed information on the binbash Leverage Landing Zone , visit the links below. How it works User guide","title":"Next steps"},{"location":"resources/security-and-shared-accounts/","text":"Configure the Security and Shared accounts \u00b6 You should by now be more familiar with the steps required to create and configure the Management account. Now you need to do pretty much the same with two more accounts: Security and Shared. Follow the sections in this page to get started! What are these accounts used for? The Security account is intended for operating security services (e.g. GuardDuty, AWS Security Hub, AWS Audit Manager, Amazon Detective, Amazon Inspector, and AWS Config), monitoring AWS accounts, and automating security alerting and response. The Shared Services account supports the services that multiple applications and teams use to deliver their outcomes. Some examples include VPN servers, monitoring systems, and centralized logs management services. Deploy the Security account's layers \u00b6 The next account to orchestrate is the security account. This account is intended for centralized user management via a IAM roles based cross organization authentication approach. This means that most of the users for your organization will be defined in this account and those users will access the different accounts through this one. First, go to the security directory. cd security Set profile \u00b6 Since we are using SSO, check in security/config/backend.tfvars file the profile is set to: profile = \"me-security-devops\" If it is not, please modify it. Note we are using a sample short project name me , use the one you have set. Terraform backend layer \u00b6 Move into the us-east-1/base-tf-backend directory and run: leverage terraform init --skip-validation leverage terraform apply More information on terraform init and terraform apply Now, to push the local .tfstate to the bucket, uncomment the backend section for the terraform configuration in security/base-tf-backend/config.tf backend \"s3\" { key = \"security/tf-backend/terraform.tfstate\" } And run again: leverage terraform init When prompted, answer yes . Now you can safely remove the terraform.tfstate and terraform.tfstate.backup files created during the apply step. Security layer \u00b6 The last layer for the security account is the security layer. Move into the us-east-1/security-base directory and run: leverage terraform init leverage terraform apply Deploy the Shared account's layers \u00b6 The last account in this deployment is the shared account. Again, this account is intended for managing the infrastructure of shared services and resources such as directory services, DNS, VPN, monitoring tools or centralized logging solutions. Place yourself in the shared directory. cd shared Set profile \u00b6 Since we are using SSO, check in shared/config/backend.tfvars file the profile is set to: profile = \"me-shared-devops\" If it is not, please modify it. Note we are using a sample short project name me , use the one you have set. Terraform backend layer \u00b6 Move into the us-east-1/base-tf-backend directory and run: leverage terraform init --skip-validation leverage terraform apply More information on terraform init and terraform apply Now, to push the local .tfstate to the bucket, uncomment the backend section for the terraform configuration in shared/base-tf-backend/config.tf backend \"s3\" { key = \"shared/tf-backend/terraform.tfstate\" } And run a second time: leverage terraform init When prompted, answer yes . Now you can safely remove the terraform.tfstate and terraform.tfstate.backup files created during the apply step. Security layer \u00b6 Next, move into the us-east-1/security-base directory: leverage terraform init leverage terraform apply Network layer \u00b6 The last layer should be the network layer, so switch to that us-east-1/base-network and run: leverage terraform init leverage terraform apply Next steps \u00b6 You have now a fully deployed landing zone configuration for the Leverage Reference Architecture for AWS, with its three accounts management, security and shared ready to be used. Next, you are going to tackle de last steps.","title":"Configure the Security and Shared accounts"},{"location":"resources/security-and-shared-accounts/#configure-the-security-and-shared-accounts","text":"You should by now be more familiar with the steps required to create and configure the Management account. Now you need to do pretty much the same with two more accounts: Security and Shared. Follow the sections in this page to get started! What are these accounts used for? The Security account is intended for operating security services (e.g. GuardDuty, AWS Security Hub, AWS Audit Manager, Amazon Detective, Amazon Inspector, and AWS Config), monitoring AWS accounts, and automating security alerting and response. The Shared Services account supports the services that multiple applications and teams use to deliver their outcomes. Some examples include VPN servers, monitoring systems, and centralized logs management services.","title":"Configure the Security and Shared accounts"},{"location":"resources/security-and-shared-accounts/#deploy-the-security-accounts-layers","text":"The next account to orchestrate is the security account. This account is intended for centralized user management via a IAM roles based cross organization authentication approach. This means that most of the users for your organization will be defined in this account and those users will access the different accounts through this one. First, go to the security directory. cd security","title":"Deploy the Security account's layers"},{"location":"resources/security-and-shared-accounts/#set-profile","text":"Since we are using SSO, check in security/config/backend.tfvars file the profile is set to: profile = \"me-security-devops\" If it is not, please modify it. Note we are using a sample short project name me , use the one you have set.","title":"Set profile"},{"location":"resources/security-and-shared-accounts/#terraform-backend-layer","text":"Move into the us-east-1/base-tf-backend directory and run: leverage terraform init --skip-validation leverage terraform apply More information on terraform init and terraform apply Now, to push the local .tfstate to the bucket, uncomment the backend section for the terraform configuration in security/base-tf-backend/config.tf backend \"s3\" { key = \"security/tf-backend/terraform.tfstate\" } And run again: leverage terraform init When prompted, answer yes . Now you can safely remove the terraform.tfstate and terraform.tfstate.backup files created during the apply step.","title":"Terraform backend layer"},{"location":"resources/security-and-shared-accounts/#security-layer","text":"The last layer for the security account is the security layer. Move into the us-east-1/security-base directory and run: leverage terraform init leverage terraform apply","title":"Security layer"},{"location":"resources/security-and-shared-accounts/#deploy-the-shared-accounts-layers","text":"The last account in this deployment is the shared account. Again, this account is intended for managing the infrastructure of shared services and resources such as directory services, DNS, VPN, monitoring tools or centralized logging solutions. Place yourself in the shared directory. cd shared","title":"Deploy the Shared account's layers"},{"location":"resources/security-and-shared-accounts/#set-profile_1","text":"Since we are using SSO, check in shared/config/backend.tfvars file the profile is set to: profile = \"me-shared-devops\" If it is not, please modify it. Note we are using a sample short project name me , use the one you have set.","title":"Set profile"},{"location":"resources/security-and-shared-accounts/#terraform-backend-layer_1","text":"Move into the us-east-1/base-tf-backend directory and run: leverage terraform init --skip-validation leverage terraform apply More information on terraform init and terraform apply Now, to push the local .tfstate to the bucket, uncomment the backend section for the terraform configuration in shared/base-tf-backend/config.tf backend \"s3\" { key = \"shared/tf-backend/terraform.tfstate\" } And run a second time: leverage terraform init When prompted, answer yes . Now you can safely remove the terraform.tfstate and terraform.tfstate.backup files created during the apply step.","title":"Terraform backend layer"},{"location":"resources/security-and-shared-accounts/#security-layer_1","text":"Next, move into the us-east-1/security-base directory: leverage terraform init leverage terraform apply","title":"Security layer"},{"location":"resources/security-and-shared-accounts/#network-layer","text":"The last layer should be the network layer, so switch to that us-east-1/base-network and run: leverage terraform init leverage terraform apply","title":"Network layer"},{"location":"resources/security-and-shared-accounts/#next-steps","text":"You have now a fully deployed landing zone configuration for the Leverage Reference Architecture for AWS, with its three accounts management, security and shared ready to be used. Next, you are going to tackle de last steps.","title":"Next steps"}]}